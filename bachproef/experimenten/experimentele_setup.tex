\section{Experimentele setup}

\subsection{Deep learning frameworks en tools}

Vandaag de dag bestaan er verschillende deep learning frameworks die de ontwikkeling en implementatie van neurale netwerken sterk vergemakkelijken. Enkele van de meest gebruikte zijn TensorFlow, PyTorch, JAX en Keras. TensorFlow en PyTorch zijn de twee dominante deep learning frameworks in het veld van artificiële intelligentie. Keras is een speciale uitzondering: het is namelijk een bibliotheek die compatibiliteit biedt tussen deze verschillende frameworks.

\subsubsection{TensorFlow}

TensorFlow (geïntroduceerd door Google in 2015) werd ontworpen met het oog op grootschalige productieomgevingen. Het framework biedt uitgebreide ondersteuning voor het deployen van modellen op verschillende platformen, zoals mobiele apparaten en webapplicaties, en beschikt over geavanceerde tools zoals TensorBoard voor visualisatie en TensorFlow Serving voor schaalbare modelinzet. TensorFlow’s aanpak met statische computationele grafen (in vroege versies) maakte het echter aanvankelijk minder intuïtief voor onderzoek en experimentatie, hoewel TensorFlow 2.x dit deels heeft verbeterd door de introductie van \emph{eager execution}. TensorFlow biedt ook veruit de beste integratie met \glspl{tpu}. \autocite{Pang_2019}

\subsubsection{PyTorch}

PyTorch (ontwikkeld door \gls{fair} en uitgebracht in 2016) heeft daarentegen vanaf het begin sterk ingezet op gebruiksvriendelijkheid en flexibiliteit. PyTorch maakt gebruik van dynamische computationele grafen, wat betekent dat het netwerk direct kan worden aangepast tijdens de uitvoering. Dit maakt het debuggen eenvoudiger en laat onderzoekers sneller experimenteren met nieuwe architecturen en technieken. Bovendien sluit de programmeerstijl van PyTorch dichter aan bij standaard Python, wat de leercurve verlaagt en de ontwikkelsnelheid verhoogt. In de onderzoekswereld heeft PyTorch hierdoor snel populariteit gewonnen, en veel state-of-the-art modellen en papers publiceren tegenwoordig hun codebase standaard in PyTorch. \autocite{Imambi_2021} \\

Voor dit project is uiteindelijk gekozen voor PyTorch vanwege de flexibiliteit en transparantie die het biedt tijdens het ontwikkelen en experimenteren met objectdetectiemodellen. PyTorch biedt namelijk uitstekende integraties met moderne detectiebibliotheken zoals TorchVision, wat de implementatie en evaluatie van complexe modellen versnelt. Daarnaast is het makkelijk om gepretrainde detectiemodellen te importeren, waardoor in dit onderzoek gefocust kan worden op de nieuwe ontwikkelingen zonder de focus te moeten verschuiven naar complexe backbones. Ook de installatie, configuratie en setup van PyTorch zijn veel eenvoudiger dan bij TensorFlow. Deze eigenschappen maken PyTorch tot de meest geschikte keuze om de doelstellingen van dit onderzoek efficiënt en effectief te realiseren.

\subsection{Hardware en compute resources}

Het trainen van deze objectdetectiemodellen is een intensief proces dat zowel op het vlak van data als computationele resources aanzienlijke vereisten stelt. Zoals uitgelegd in \ref{subsec:definitie-en-gebruik-op-sonarafbeeldingen} combineert objectdetectie classificatie en lokalisatie: het model moet niet alleen herkennen wat er in een afbeelding aanwezig is, maar ook waar het zich bevindt via het voorspellen van \glspl{bounding_box}. Tijdens het trainingsproces worden geannoteerde datasets gebruikt waarbij objecten met klasse-labels en coördinaten zijn aangeduid, zoals in \gls{coco} of Pascal \gls{voc}. Het model leert via een samengestelde \gls{loss_functie}, die typisch zowel classificatiefouten als fouten in de \glspl{bounding_box} omvat. Door middel van \gls{backpropagation} worden de gewichten van het neurale netwerk aangepast om deze gecombineerde \gls{loss_functie} te minimaliseren. Omdat objectdetectie vaak gebaseerd is op diepe \glspl{cnn}, vereist het trainingsproces krachtige hardware. \\

\subsubsection{CPU / GPU / TPU}

Omwille van deze vereisten, worden neurale netwerken vrijwel nooit getraind op \glspl{cpu}. Een \gls{cpu} is namelijk zeer goed in het uitvoeren van \emph{general-purpose} taken (besturingssysteem draaien, tekstverwerking, spreadsheets \dots) op een sequentiële manier. Om hiervoor geoptimaliseerd te zijn, bevat een \gls{cpu} relatief weinig cores. Dit varieert meestal tussen de 2 en de 64. Echter draaien deze cores op een hoge tot zeer hoge snelheid, meestal tussen de 2 en de 5 GHz. Het grote nadeel van \glspl{cpu} is dat ze niet geschikt zijn voor taken die sterk geparallelliseerd worden uitgevoerd, zoals het trainen van een neuraal netwerk. Deze taak vergt namelijk een grote hoeveelheid aan matrix- en tensoroperaties, hetgeen efficiënt kan geparallelliseerd worden. \\

Voor deze use-case wordt dus meestal gebruik gemaakt van één of meerdere \glspl{gpu}. Deze zijn oorspronkelijk bedoeld om beelden te renderen om ze op een monitor weer te geven. Aangezien dit ook een taak is die in hoge mate geparallelliseerd kan worden, is een \gls{gpu} ontworpen met (tien)duizenden kleinere cores in plaats van enkele grote cores. Deze cores hebben dan wel weer een lagere snelheid dan de \gls{cpu}-cores. Doorheen de jaren worden \glspl{gpu} steeds meer gebruikt om parallelliseerbare taken uit te voeren. Naast de training van neurale netwerken gaat dit bijvoorbeeld ook om gaming, 3D-rendering en het minen van cryptomunten zoals Bitcoin. \autocite{Anish_Dev_2014} \\

Echter is een \gls{gpu} niet speciaal geoptimaliseerd om neurale netwerken te trainen. Dit probleem wordt opgelost door de \gls{tpu}. Dit is een \gls{asic} die speciaal ontworpen is door Google voor het trainen van neurale netwerken. De \gls{tpu} is geoptimaliseerd om matrixvermenigvuldigingen en andere tensoroperaties uit te voeren. \autocite{Jouppi_2017} Het nadeel van de \gls{tpu} is dat de compatibiliteit met third-party frameworks zoals PyTorch beperkt is. \glspl{tpu} werken het beste met TensorFlow, het deep learning-framework van Google zelf. \autocite{Wang_2019} \\

Afhankelijk van modelcomplexiteit, datasetgrootte, batchgrootte en de gebruikte optimalisatie-instellingen kan trainingsduur variëren van enkele uren tot meerdere dagen. Daarom wordt in veel gevallen -- en ook in dit onderzoek -- gebruik gemaakt van vooraf getrainde modellen (pretrained backbones) als uitgangspunt, wat de trainingstijd aanzienlijk kan verkorten en -- vooral bij kleinere datasets -- betere prestaties oplevert. \\

\subsubsection{Toegang tot resources}

Er zijn enkele mogelijkheden om toegang te krijgen tot \glspl{gpu} voor het trainen van deep learning-modellen. Zo is het mogelijk om via \href{https://colab.research.google.com/}{Google Colab} gratis toegang te krijgen tot een NVIDIA Tesla T4 \gls{gpu} met 16 GB aan videogeheugen en 2560 CUDA-cores. \autocite{TechPowerUp_Tesla-T4} Ondanks dat deze \gls{gpu} voor het eerst op de markt is gebracht in 2018, is ze nog steeds bruikbaar en nuttig voor AI-workloads. Ook biedt Google via Colab een TPUv2 aan met 8 cores. Een nadeel is wel dat deze resources slechts beperkt toegankelijk zijn afhankelijk van de drukte en hoe zwaar de resource wordt belast. Hierdoor is Google Colab minder geschikt om grote, complexe modellen te trainen. Een ander nadeel is de toegang tot de benodigde data. \\

Op Google Colab is er telkens een virtuele schijf voorzien van zo'n 40-60 GB. Echter wordt deze volledig verwijderd op het moment dat de runtime afgesloten wordt. De data wordt met andere woorden niet persistent opgeslagen. Datasets telkens opnieuw downloaden en voorbereiden verspilt onnodig tijd en resources. Ook is het mogelijk om Colab te koppelen aan Google Drive, maar ook daar is de opslagruimte beperkt (15 GB gratis) en de leessnelheid is te traag om er trainingsdata op te slaan. Deze problemen worden grotendeels verholpen wanneer er gekozen wordt om een dedicated \gls{gpu}-server te huren in de cloud, maar dit doet de kosten natuurlijk oplopen, zeker wanneer er een groot en complex model getraind moet worden. \\

In dit onderzoek werd er daarom gekozen om de modellen te trainen op een dedicated trainingsserver van \href{https://www.exail.com/}{Exail Robotics Belgium}. Deze server is uitgerust met een Intel Core i7-11700K processor met 8 cores, 64 GB DDR4 RAM en een NVIDIA RTX A5000 \gls{gpu} met 24 GB aan videogeheugen en 8192 CUDA-cores. \autocite{TechPowerUp_RTX-A5000}

\subsection{Optimalisatietechnieken}

Doorheen de ontwikkeling van de verschillende modellen in dit onderzoek zal er gebruik gemaakt worden van een reeks optimalisatietechnieken om -- onder andere -- de performantie van de modellen te verbeteren, efficiënter met resources om te springen of sneller te trainen. Hieronder volgt een korte uitleg van enkele optimalisatietechnieken toegepast in dit onderzoek.

\subsubsection{Early stopping}

Een belangrijke optimalisatie binnen deep learning is het tegengaan van \gls{overfitting}. Een mogelijke oorzaak hiervan is dat er te lang (lees: voor te veel epochs) getraind wordt voor de beschikbare hoeveelheid data. Het is dus belangrijk dat het model traint voor de juiste hoeveelheid epochs. Te weinig epochs zorgt ervoor dat het model niet zijn maximale performance bereikt, te veel epochs zorgt voor \gls{overfitting}. Gelukkig bestaat er een oplossing om dit grotendeels te automatiseren: \emph{early stopping}. \autocite{Ying_2019} \\

Early stopping is een regularisatietechniek die kan gebruikt worden bij elke iteratieve trainingstechniek (zoals \gls{gradient_descent}). Het zorgt ervoor dat de training automatisch gestopt wordt als de performantie van het model op een validatieset verslechterd. Daarnaast worden ook geen kostbare resources of tijd verspild. \autocite{Prechelt_1998} In tegenstelling tot Keras en TensorFlow, bevat het PyTorch-framework geen geïntegreerde callback voor early stopping. Gelukkig is dit een relatief eenvoudige klasse om zelf te implementeren. \\

Voor een simpele \texttt{EarlyStopping}-klasse zijn er slechts enkele parameters nodig: 

\begin{itemize}
    \item \texttt{patience}: het aantal epochs waar geen verbetering plaatsvindt voordat het trainingsproces gestopt wordt.
    \item \texttt{delta}: de minimumwaarde die als verbetering wordt gezien
    \item \texttt{mode}: doel van het algoritme. Is het de bedoeling dat de metriek geminimaliseerd of gemaximaliseerd wordt?
\end{itemize}

Het eigenlijke \texttt{EarlyStopping} is verassend eenvoudig. Elke epoch wordt de \texttt{\_\_call\_\_}-methode aangeroepen. Afhankelijk van de \texttt{mode} wordt een score berekend. Deze score is gelijk aan de metriek die aan de methode meegegeven wordt als het doel is om deze te maximaliseren. Als deze metriek geminimaliseerd moet worden, is de score de tegengestelde van de metriek. Als er nog geen beste score is, wordt de huidige score als beste opgeslagen en wordt de huidige staat van het model als beste staat opgeslagen. Als de score echter kleiner is dan dan de som van de beste score en de \texttt{delta}, wordt dit gezien als verslechtering. Een counter wordt verhoogd met 1 en als deze counter groter wordt dan de \texttt{patience}, dan wordt de training gestopt. Als geen van deze twee voorwaarden geldt (er is dus wel degelijk een significante verbetering), dan wordt de beste score aangepaste aan de huidige score, de huidige staat van het model als beste staat opgeslagen en de counter gereset. Eens het trainingsproces gestopt is, wordt de beste staat van het model terug ingeladen.

\subsubsection{Mixed precision training}

Bij het trainen van objectdetectiemodellen is rekenkracht vaak een beperkende factor vanwege de hoge complexiteit van de netwerken en de omvangrijke inputdata. De modellen die in dit onderzoek gebruikt zullen worden vereisen aanzienlijke GPU-geheugenruimte en rekentijd, vooral bij gebruik van leertechnieken zoals \gls{ssl} en \gls{self-sl}, doordat ze deels of volledig zonder gelabelde data werken en vertrouwen op complexe pretext-taken of iteratieve labeling. In dit kader biedt \emph{mixed precision training} een belangrijke optimalisatiemogelijkheid. Het zorgt namelijk voor een verminderd geheugengebruik en een sneller trainingsproces, zonder noemenswaardig verlies aan modelnauwkeurigheid. Dit maakt het mogelijk om efficiënter te trainen op dezelfde hardware, grotere batchgroottes te gebruiken en sneller te experimenteren, wat van groot belang is tijdens dit iteratief ontwikkelingsproces. \\

Mixed precision training is een techniek die gebruik maakt van zowel 16-bit (half precision) als 32-bit (single precision) floating point getallen tijdens het trainen van neurale netwerken. In plaats van alle berekeningen in 32-bit uit te voeren -- wat standaard is in de meeste frameworks -- maakt mixed precision gebruik van 16-bit waar mogelijk, zonder daarbij de modelnauwkeurigheid significant te verliezen. Deze aanpak leidt tot versnelde trainingstijden en lager gebruik van \acrshort{gpu}-geheugen. \\

In PyTorch wordt mixed precision training ondersteund via het \gls{amp}-systeem, dat beschikbaar is in de \texttt{torch.amp} module. Met \gls{amp} kan mixed precision eenvoudig in de trainingscode geïntegreerd worden, zonder handmatig de precisie van elke operatie te moeten beheren. Het belangrijkste onderdeel van \gls{amp} is de \emph{\texttt{autocast} context manager}, die automatisch beslist welke operaties in 16-bit uitgevoerd kunnen worden en welke in 32-bit moeten blijven. Daarnaast zorgt de \emph{\texttt{GradScaler} utility} ervoor dat de gradiënten geschaald worden tijdens \gls{backpropagation} om het risico op \gls{buffer_underflow} bij zeer kleine 16-bit waardes te beperken. Hierdoor blijft het trainingsproces stabiel. \\

Het gebruik van \gls{amp} is bijzonder waardevol in resource-intensieve taken zoals in dit onderzoek. Deze \gls{ssl} en \gls{self-sl} objectdetectietaken vragen veel geheugen en rekenkracht. Door mixed precision toe te passen, kunnen grotere batch sizes worden gebruikt en snellere iteraties worden bereikt op dezelfde hardware. Bovendien ondersteunt de meeste moderne \gls{GPU}-hardware van NVIDIA mixed precision training volledig, wat leidt tot aanzienlijke prestatieverbeteringen zonder kwaliteitsverlies.

