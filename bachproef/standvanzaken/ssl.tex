\section{Semi-supervised learning: principes en technieken}

\subsection{Definitie en waarde binnen objectdetectie}

Binnen het domein van machine learning bestaan er verschillende technieken om een model te trainen. Meestal wordt er gesproken van twee grote stromingen: supervised learning en unsupervised learning. Bij supervised learning wordt er gebruik gemaakt van een dataset en een uitkomst (hetgeen het model uiteindelijk moet kunnen voorspellen). Dit kan een label zijn of een bepaalde numerieke waarde. Belangrijk is dat zowel de input als de gewenste output gegeven zijn. Het model leert dus het verband tussen de twee. Bij unsupervised learning zijn er geen verwachte outputs. De volledige dataset wordt door het model gebruikt om patronen in te herkennen. Unsupervised learning wordt daarom ook meestal gebruikt om verkennende data-analyse uit te voeren. Echter hebben beide methoden enkele nadelen. Bij supervised learning is het traag en duur om alle data op een correcte manier te labelen. Unsupervised learning heeft dit probleem niet, maar heeft een beperkt aantal toepassingen en is minder accuraat. Een alternatief is \gls{ssl}, wat een compromis tussen zowel supervised als unsupervised learning is. \autocite{C_A_Padmanabha_Reddy_2018} \\

\gls{ssl} is een subdomein van machine learning waar gebruik gemaakt wordt van zowel gelabelde als ongelabelde data om modellen te trainen. Dit is bijzonder nuttig in situaties waarin het labelen van gegevens duur of tijdrovend is, zoals bij sonardata het geval is. \gls{ssl} bevindt zich tussen supervised learning (waar alle trainingsdata gelabeld zijn) en unsupervised learning (waar geen labels beschikbaar zijn). Door gebruik te maken van een kleine hoeveelheid gelabelde gegevens in combinatie met een grote hoeveelheid ongelabelde gegevens, kan een model beter generaliseren waardoor de prestaties verbeteren met minder menselijke annotatie-inspanning. \autocite{Hady_2013} \\

Zoals eerder vermeld, is \gls{ssl} een subdomein van machine learning, net zoals supervised en unsupervised learning. Het is dus niet één model of één architectuur, maar een waaier van verschillende algoritmen die soms op hele andere manieren werken. Wel hebben ze allemaal gemeen dat ze zowel gelabelde als ongelabelde data gebruiken. \\

Één van de belangrijkste technieken binnen \gls{ssl} is \emph{consistency regularization} (cf. infra), waarbij een model wordt aangemoedigd om consistente voorspellingen te maken voor kleine verstoringen van dezelfde ongelabelde input. \autocite{Fan_2022} Een ander veelgebruikt principe is \emph{pseudo-labeling} (cf. infra), waarbij het model zelf voorspellingen genereert voor ongelabelde gegevens en deze gebruikt als extra trainingsdata. \autocite{Lee_2013} \\

Daarnaast is er graph-based \gls{ssl}. Dit is een techniek die gebruik maakt van grafen (netwerken) om de relaties tussen gelabelde en ongelabelde data te modelleren en labelinformatie effectiever te verspreiden. In plaats van uitsluitend te vertrouwen op individuele gegevenspunten, gebruiken deze methoden de structuur van de dataset om aannames te maken over onbekende labels. Dit is vooral nuttig in situaties waarin de onderliggende data een natuurlijke connectiviteit vertoont, zoals sociale netwerken, biologische netwerken en tekstanalyses. \autocite{Song_2021} \\

Graph-based \gls{ssl}-modellen stellen de dataset voor als een graaf $G = (V, E)$ waarbij $V$ de knopen (datapunten) zijn (zowel gelabelde als ongelabelde gegevens) en $E$ de de gewogen randen (connecties) tussen knopen zijn, die de relatie of gelijkenis tussen de datapunten aangeven. Het basisidee is dat naburige knopen waarschijnlijk tot dezelfde klasse behoren, een principe dat bekend staat als \emph{label propagation}. Labels van bekende knopen (gelabelde data) worden hierbij iteratief verspreid naar naburige knopen op basis van de sterkte van de verbindingen. \autocite{Zhu_2005} \\

\gls{ssl} wordt toegepast in diverse domeinen, zoals beeld- en spraakherkenning, biomedische analyse en autonome systemen. Recente ontwikkelingen in deep learning hebben geleid tot geavanceerde \gls{ssl}-methoden, zoals FixMatch en MixMatch (cf. infra), die de prestaties aanzienlijk verbeteren door sterke data-augmentatie, efficiënter gebruik van ongelabelde data en de combinatie van andere \gls{ssl}-technieken. Onderzoek heeft aangetoond dat \gls{ssl} met slechts 10\% gelabelde data al bijna dezelfde prestaties kan bereiken als volledig gelabelde modellen. \autocite{Lucas_2022}

\subsection{Veelgebruikte SSL-methoden}

\acrfull{ssl} omvat verschillende methoden die gebruik maken van zowel gelabelde als ongelabelde data om de prestaties van machine learning-modellen te verbeteren. Veelgebruikte \gls{ssl}-methoden compenseren de beperkte beschikbaarheid van gelabelde data door patronen en structuren in ongelabelde data te benutten. \gls{ssl} wordt voor alle soorten doeleinden gebruikt: niet alleen in predictieve modellen, waarbij er vaak gebruik gemaakt wordt van pseudo-labeling en consistency regularization, wordt \gls{ssl} ingezet. Ook bij generatieve modellen wordt \gls{ssl} enorm veel gebruikt. Dit gaat dan bijvoorbeeld om \glspl{vae} en \glspl{gan}. Deze worden hierbij ingezet om aanvullende trainingsgegevens te genereren. Door al deze technieken te combineren, kunnen \gls{ssl}-methoden significante verbeteringen bieden voor taken zoals beeldherkenning, spraakverwerking en \gls{nlp}. \autocite{van_Engelen_2019} Hieronder worden enkele van de meest prominente technieken binnen \gls{ssl} besproken.

\subsubsection{Pseudo-labeling}

Pseudo-labeling is een belangrijke techniek binnen \gls{ssl} waarbij een model, getraind op een beperkte hoeveelheid gelabelde data, wordt ingezet om voorspellingen te doen op ongelabelde data. Deze voorspellingen, aangeduid als ``pseudo-labels'', worden vervolgens behandeld als echte labels, waardoor het model verder kan worden verfijnd met een uitgebreidere dataset. Dit proces wordt iteratief herhaald, zodat het model geleidelijk aan zijn prestaties verbetert door zowel de gelabelde als de pseudo-gelabelde data te gebruiken. \autocite{Lee_2013} \\

Het succes van pseudo-labeling is natuurlijk afhankelijk van de nauwkeurigheid van de gegenereerde pseudo-labels. Om de kwaliteit te waarborgen, wordt vaak een drempelwaarde ingesteld voor de voorspellingszekerheid: alleen voorspellingen die boven deze drempel uitkomen, worden als pseudo-labels geaccepteerd. Dit helpt het model om zich te concentreren op voorbeelden waarbij het relatief zeker is van de voorspelling, waardoor het risico op het leren van verkeerde informatie wordt verminderd. \autocite{Kage_2024} \\

Een belangrijk voordeel van pseudo-labeling is dat het effectief gebruikmaakt van grote hoeveelheden ongelabelde data, wat vooral nuttig is in domeinen zoals sonarbeeldvorming, waar het verkrijgen van gelabelde data duur en tijdrovend is. Verder zijn toepassingen van pseudo-labeling te vinden in verschillende gebieden, waaronder beeldherkenning, spraakverwerking en \gls{nlp}. \autocite{Min_2022} Recent onderzoek van \textcite{Ferreira_2023} heeft aangetoond dat pseudo-labeling een zeer goede performantie kan neerzetten, terwijl het de behoefte aan uitgebreide gelabelde datasets vermindert. \\

Desondanks kent pseudo-labeling ook uitdagingen. Als het model in een vroeg stadium onnauwkeurige pseudo-labels genereert, kan dit leiden tot het versterken van fouten, een fenomeen bekend als \emph{confirmation bias}. Om dit te voorkomen, worden technieken zoals \emph{curriculum learning} toegepast, waarbij het model eerst wordt getraind met de meest zekere pseudo-labels en geleidelijk aan minder zekere voorbeelden toevoegt naarmate de training vordert. \autocite{Cascante_Bonilla_2020}

\subsubsection{Consistency Regularization}

Een ander fundamenteel concept binnen \gls{ssl} is consistency regularization. Deze techniek wordt in verschillende algoritmen gebruikt om de betrouwbaarheid van het model te verbeteren. Het doet dit door het model te dwingen consistente voorspellingen te maken voor kleine variaties van dezelfde input. Het idee is gebaseerd op de veronderstelling dat een model robuust moet zijn tegen kleine verstoringen in de input, vooral wanneer de input geen gelabelde gegevens bevat. \\

Eerst wordt een ongelabeld voorbeeld $x_u$ aangepast met kleine verstoringen. Dit kan gaan om data-augmentatie (bv. willekeurige rotaties, verscherping, kleuraanpassingen, \dots), het toevoegen van ruis (bv. Gaussian noise), \dots Dit resulteert in twee versies van dezelfde input: het origineel $x_u$ en de verstoorde versie $x'_u$. Daarna voorspelt het model de kansverdeling van de klassen voor zowel de originele als de verstoorde input. \\

Een consistente voorspelling betekent dat beide kansverdelingen ``dicht'' bij elkaar moeten liggen. Om dit te garanderen wordt een gespecialiseerde \gls{loss_functie} gebruikt, zoals de \gls{kl}-divergentie \autocite{Hall_1987} of de \gls{mse}. Het model wordt getraind om deze \emph{loss} te minimaliseren, zodat het stabiele en robuuste voorspellingen leert maken, zelfs bij verstoringen. \\

Consistency regularization is enorm effectief (en wordt daarom ook veel gebruikt) omdat het ervoor zorgt dat het model gebruik maakt van de onderliggende structuur van ongelabelde data. Hierdoor verbetert de generalisatie, omdat het model minder gevoelig wordt voor kleine ruis en variaties. Daarnaast verhoogt de sample-efficiëntie, waardoor minder gelabelde data nodig is voor goede prestaties. \autocite{Fan_2022}

\subsubsection{MixMatch en FixMatch}

\lipsum[1]