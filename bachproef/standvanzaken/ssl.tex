\section{Semi-supervised learning: principes en technieken}

\subsection{Definitie en waarde binnen objectdetectie}

Binnen het domein van machine learning bestaan er verschillende technieken om een model te trainen. Meestal wordt er gesproken van twee grote stromingen: supervised learning en unsupervised learning. Bij supervised learning wordt er gebruik gemaakt van een dataset en een uitkomst (hetgeen het model uiteindelijk moet kunnen voorspellen). Dit kan een label zijn of een bepaalde numerieke waarde. Belangrijk is dat zowel de input als de gewenste output gegeven zijn. Het model leert dus het verband tussen de twee. Bij unsupervised learning zijn er geen verwachte outputs. De volledige dataset wordt door het model gebruikt om patronen in te herkennen. Unsupervised learning wordt daarom ook meestal gebruikt om verkennende data-analyse uit te voeren. Echter hebben beide methoden enkele nadelen. Bij supervised learning is het traag en duur om alle data op een correcte manier te labelen. Unsupervised learning heeft dit probleem niet, maar heeft een beperkt aantal toepassingen en is minder accuraat. Een alternatief is \gls{ssl}, wat een compromis tussen zowel supervised als unsupervised learning is. \autocite{C_A_Padmanabha_Reddy_2018} \\

\gls{ssl} is een subdomein van machine learning waar gebruik gemaakt wordt van zowel gelabelde als ongelabelde data om modellen te trainen. Dit is bijzonder nuttig in situaties waarin het labelen van gegevens duur of tijdrovend is, zoals bij sonardata het geval is. \gls{ssl} bevindt zich tussen supervised learning (waar alle trainingsdata gelabeld zijn) en unsupervised learning (waar geen labels beschikbaar zijn). Door gebruik te maken van een kleine hoeveelheid gelabelde gegevens in combinatie met een grote hoeveelheid ongelabelde gegevens, kan een model beter generaliseren waardoor de prestaties verbeteren met minder menselijke annotatie-inspanning. \autocite{Hady_2013} \\

Zoals eerder vermeld, is \gls{ssl} een subdomein van machine learning, net zoals supervised en unsupervised learning. Het is dus niet één model of één architectuur, maar een waaier van verschillende algoritmen die soms op hele andere manieren werken. Wel hebben ze allemaal gemeen dat ze zowel gelabelde als ongelabelde data gebruiken. \\

Één van de belangrijkste technieken binnen \gls{ssl} is \emph{consistency regularization} (cf. infra), waarbij een model wordt aangemoedigd om consistente voorspellingen te maken voor kleine verstoringen van dezelfde ongelabelde input. \autocite{Fan_2022} Een ander veelgebruikt principe is \emph{pseudo-labeling} (cf. infra), waarbij het model zelf voorspellingen genereert voor ongelabelde gegevens en deze gebruikt als extra trainingsdata. \autocite{Lee_2013} \\

Daarnaast is er graph-based \gls{ssl}. Dit is een techniek die gebruik maakt van grafen (netwerken) om de relaties tussen gelabelde en ongelabelde data te modelleren en labelinformatie effectiever te verspreiden. In plaats van uitsluitend te vertrouwen op individuele gegevenspunten, gebruiken deze methoden de structuur van de dataset om aannames te maken over onbekende labels. Dit is vooral nuttig in situaties waarin de onderliggende data een natuurlijke connectiviteit vertoont, zoals sociale netwerken, biologische netwerken en tekstanalyses. \autocite{Song_2021} \\

Graph-based \gls{ssl}-modellen stellen de dataset voor als een graaf $G = (V, E)$ waarbij $V$ de knopen (datapunten) zijn (zowel gelabelde als ongelabelde gegevens) en $E$ de de gewogen randen (connecties) tussen knopen zijn, die de relatie of gelijkenis tussen de datapunten aangeven. Het basisidee is dat naburige knopen waarschijnlijk tot dezelfde klasse behoren, een principe dat bekend staat als \emph{label propagation}. Labels van bekende knopen (gelabelde data) worden hierbij iteratief verspreid naar naburige knopen op basis van de sterkte van de verbindingen. \autocite{Zhu_2005} \\

\gls{ssl} wordt toegepast in diverse domeinen, zoals beeld- en spraakherkenning, biomedische analyse en autonome systemen. Recente ontwikkelingen in deep learning hebben geleid tot geavanceerde \gls{ssl}-methoden, zoals FixMatch en MixMatch (cf. infra), die de prestaties aanzienlijk verbeteren door sterke data-augmentatie, efficiënter gebruik van ongelabelde data en de combinatie van andere \gls{ssl}-technieken. Onderzoek heeft aangetoond dat \gls{ssl} met slechts 10\% gelabelde data al bijna dezelfde prestaties kan bereiken als volledig gelabelde modellen. \autocite{Lucas_2022}

\subsection{Veelgebruikte SSL-methoden}

\acrfull{ssl} omvat verschillende methoden die gebruik maken van zowel gelabelde als ongelabelde data om de prestaties van machine learning-modellen te verbeteren. Veelgebruikte \gls{ssl}-methoden compenseren de beperkte beschikbaarheid van gelabelde data door patronen en structuren in ongelabelde data te benutten. \gls{ssl} wordt voor alle soorten doeleinden gebruikt: niet alleen in predictieve modellen, waarbij er vaak gebruik gemaakt wordt van pseudo-labeling en consistency regularization, wordt \gls{ssl} ingezet. Ook bij generatieve modellen wordt \gls{ssl} enorm veel gebruikt. Dit gaat dan bijvoorbeeld om \glspl{vae} en \glspl{gan}. Deze worden hierbij ingezet om aanvullende trainingsgegevens te genereren. Door al deze technieken te combineren, kunnen \gls{ssl}-methoden significante verbeteringen bieden voor taken zoals beeldherkenning, spraakverwerking en \gls{nlp}. \autocite{van_Engelen_2019} Hieronder worden enkele van de meest prominente technieken binnen \gls{ssl} besproken.

\subsubsection{Pseudo-labeling}

Pseudo-labeling is een belangrijke techniek binnen \gls{ssl} waarbij een model, getraind op een beperkte hoeveelheid gelabelde data, wordt ingezet om voorspellingen te doen op ongelabelde data. Deze voorspellingen, aangeduid als ``pseudo-labels'', worden vervolgens behandeld als echte labels, waardoor het model verder kan worden verfijnd met een uitgebreidere dataset. Dit proces wordt iteratief herhaald, zodat het model geleidelijk aan zijn prestaties verbetert door zowel de gelabelde als de pseudo-gelabelde data te gebruiken. \autocite{Lee_2013} \\

Het succes van pseudo-labeling is natuurlijk afhankelijk van de nauwkeurigheid van de gegenereerde pseudo-labels. Om de kwaliteit te waarborgen, wordt vaak een drempelwaarde ingesteld voor de voorspellingszekerheid: alleen voorspellingen die boven deze drempel uitkomen, worden als pseudo-labels geaccepteerd. Dit helpt het model om zich te concentreren op voorbeelden waarbij het relatief zeker is van de voorspelling, waardoor het risico op het leren van verkeerde informatie wordt verminderd. \autocite{Kage_2024} \\

Een belangrijk voordeel van pseudo-labeling is dat het effectief gebruikmaakt van grote hoeveelheden ongelabelde data, wat vooral nuttig is in domeinen zoals sonarbeeldvorming, waar het verkrijgen van gelabelde data duur en tijdrovend is. Verder zijn toepassingen van pseudo-labeling te vinden in verschillende gebieden, waaronder beeldherkenning, spraakverwerking en \gls{nlp}. \autocite{Min_2022} Recent onderzoek van \textcite{Ferreira_2023} heeft aangetoond dat pseudo-labeling een zeer goede performantie kan neerzetten, terwijl het de behoefte aan uitgebreide gelabelde datasets vermindert. \\

Desondanks kent pseudo-labeling ook uitdagingen. Als het model in een vroeg stadium onnauwkeurige pseudo-labels genereert, kan dit leiden tot het versterken van fouten, een fenomeen bekend als \emph{confirmation bias}. Om dit te voorkomen, worden technieken zoals \emph{curriculum learning} toegepast, waarbij het model eerst wordt getraind met de meest zekere pseudo-labels en geleidelijk aan minder zekere voorbeelden toevoegt naarmate de training vordert. \autocite{Cascante_Bonilla_2020}

\subsubsection{Consistency Regularization}

Een ander fundamenteel concept binnen \gls{ssl} is consistency regularization. Deze techniek wordt in verschillende algoritmen gebruikt om de betrouwbaarheid van het model te verbeteren. Het doet dit door het model te dwingen consistente voorspellingen te maken voor kleine variaties van dezelfde input. Het idee is gebaseerd op de veronderstelling dat een model robuust moet zijn tegen kleine verstoringen in de input, vooral wanneer de input geen gelabelde gegevens bevat. \\

Eerst wordt een ongelabeld voorbeeld $x_u$ aangepast met kleine verstoringen. Dit kan gaan om data-augmentatie (bv. willekeurige rotaties, verscherping, kleuraanpassingen, \dots), het toevoegen van ruis (bv. Gaussian noise), \dots Dit resulteert in twee versies van dezelfde input: het origineel $x_u$ en de verstoorde versie $x'_u$. Daarna voorspelt het model de kansverdeling van de klassen voor zowel de originele als de verstoorde input. \\

Een consistente voorspelling betekent dat beide kansverdelingen ``dicht'' bij elkaar moeten liggen. Om dit te garanderen wordt een gespecialiseerde \gls{loss_functie} gebruikt, zoals de \gls{kl}-divergentie \autocite{Hall_1987} of de \gls{mse}. Het model wordt getraind om deze \emph{loss} te minimaliseren, zodat het stabiele en robuuste voorspellingen leert maken, zelfs bij verstoringen. \\

Consistency regularization is enorm effectief (en wordt daarom ook veel gebruikt) omdat het ervoor zorgt dat het model gebruik maakt van de onderliggende structuur van ongelabelde data. Hierdoor verbetert de generalisatie, omdat het model minder gevoelig wordt voor kleine ruis en variaties. Daarnaast verhoogt de sample-efficiëntie, waardoor minder gelabelde data nodig is voor goede prestaties. \autocite{Fan_2022}

\subsubsection{FixMatch}

Beide voorgaande technieken -- zowel pseudo-labeling als consistency regularization -- zijn slechts concepten binnen het domein van \gls{ssl}. Dit wil daarom niet zeggen dat ze niet gebruikt worden in productie, maar vaak zijn ze slechts steunpilaren voor een grotere, complexere en effectievere architecturen. Hieronder wordt FixMatch -- een eerste voorbeeld die relevant is voor dit onderoek -- besproken. \\

FixMatch combineert pseudo-labeling met consistency regularization. Het werd geïntroduceerd door \textcite{Sohn_2020} en heeft bewezen indrukwekkende prestaties te leveren op benchmarks zoals CIFAR-10, waarbij met slechts 250 gelabelde voorbeelden een nauwkeurigheid van 94,93\% werd bereikt. De kernideeën achter FixMatch zijn gebaseerd op het principe dat een model consistente voorspellingen zou moeten maken, zelfs wanneer invoerdata wordt gewijzigd met verschillende vormen van data-augmentatie. FixMatch combineert twee belangrijke technieken binnen \gls{ssl}: pseudo-labeling en sterke versus zwakke augmentatie. \\

Voor een gegeven ongelabeld voorbeeld $x_u$ wordt eerst een zwak geaugmenteerde versie $x_u^w$ gegenereerd met eenvoudige transformaties zoals willekeurige spiegelingen en verschuivingen. Het model maakt een voorspelling $p\left(y|x_u^w\right)$, wat een kansverdeling over de mogelijke klassen oplevert. Als de hoogste voorspelde waarschijnlijkheid boven een drempel $\tau$ ligt wordt dit label als pseudo-label gebruikt. 

$$
\hat{y}_u = \arg \max p\left(y|x_u^w\right) \ \text{indien} \ \max p\left(y|x_u^w\right) > \tau
$$

Dit voorkomt dat onzekere voorspellingen als pseudo-labels worden gebruikt, waardoor ruis in de trainingsdata wordt verminderd. \\

Vervolgens wordt een sterk geaugmenteerde versie $x_u^s$ van dezelfde input $x_u$ gemaakt met agressieve augmentaties, zoals kleurvervormingen en Cutout (willekeurige afdekking van delen van de afbeelding). \autocite{DeVries_2017} Het model wordt getraind om dezelfde voorspelling $\hat{y}_u$ te maken op de sterk geaugmenteerde input $x_u^s$, waardoor het robuust wordt tegen verschillende variaties in de data. \\

Om FixMatch te trainen is er echter een gespecialiseerde \gls{loss_functie} nodig, die een combinatie is voor zowel de gelabelde als de pseudo-gelabelde data:

$$
L = L_s + \lambda_u L_u
$$

waarbij $L_s$ de gewone \gls{loss_functie} is voor de gelabelde data, $L_u$ de \emph{consistency loss} is voor pseudo-gelabelde data, gedefinieerd als de gemiddelde cross-entropy tussen het pseudo-label en de voorspelling op $x_{u'}^s$ en $\lambda_u$ een hyperparameter is die de balans tussen gelabelde en ongelabelde data regelt. \\

De hoge performantie van FixMatch heeft verschillende redenen: In tegenstelling tot traditionele pseudo-labeling, waarbij alle voorspellingen worden gebruikt, accepteert FixMatch alleen pseudo-labels als de modelvoorspelling een zekerheid boven een vooraf ingestelde drempel $\tau$ heeft. Dit voorkomt het versterken van foute pseudo-labels en verbetert de robuustheid van het model. Ook wordt het model gedwongen om dezelfde voorspelling te maken voor een input, ongeacht of deze zwak of sterk geaugmenteerd is. Dit helpt het model om stabiele, generaliseerbare representaties te leren. Daarnaast voorkomt het toepassen van een tweefasige-augmentatiestrategie dat het model te afhankelijk wordt van specifieke kenmerken in de data. Dit helpt overfitting te verminderen.

\subsubsection{MixMatch}

MixMatch integreert ook verschillende \gls{ssl}-technieken, waaronder data-augmentatie, pseudo-labeling en mixup. Het algoritme is een verbetering op de FixMatch-architectuur en werd geïntroduceerd door \textcite{Berthelot_2019}. In benchmarks heeft het hele goede prestaties geleverd op datasets zoals CIFAR-10 en STL-10. \\

Voor elk ongelabeld voorbeeld wordt het model meerdere keren toegepast om voorspellingen te genereren. Het gemiddelde van de voorspellingen wordt gebruikt om ruis te verminderen en betrouwbaardere pseudo-labels te verkrijgen. Dit helpt het model om robuustere pseudo-labels te creëren dan traditionele pseudo-labeling, waar slechts één voorspelling per voorbeeld wordt gebruikt. \\

Om ervoor te zorgen dat de pseudo-labels niet te onzeker zijn, wordt een \emph{softmax-temperature-scaling} toegepast om de kansverdeling scherper te maken. Dit betekent dat het model meer vertrouwen krijgt in de meest waarschijnlijke klassen, waardoor het pseudo-label effectiever wordt.

$$
q(y|x) = \frac{p(y|x)^{\frac{1}{T}}}{\sum_{y'} p(y'|x)^\frac{1}{T}}
$$

waarbij $T$ de temperatuurparameter is die de scherpte van de distributie regelt (kleinere waarden leiden tot scherpere distributies). \\

MixMatch maakt ook gebruik van \emph{mixup}, een data-augmentatietechniek waarbij synthetische trainingssamples worden gemaakt van paren bestaande trainingssamples en hun labels ($(x_1, y_1)$ en $(x_2, y_2)$). Merk op dat zowel de gelabelde als de pseudo-gelabelde samples gebruikt worden. 

\begin{align*}
    \tilde{x} & = \lambda x_1 + (1 - \lambda) x_2 \\
    \tilde{y} & = \lambda y_1 + (1 - \lambda) y_2
\end{align*}

Hierbij is $\lambda$ een gewichtsparameter, die willekeurig gekozen is uit een Beta-verdeling. Deze techniek zorgt voor soepelere beslissingsgrenzen en vermindert overfitting door het model te dwingen om robuuster te generaliseren. \autocite{Zhang_2017} \\

Omdat dezelfde input meerdere keren wordt geaugmenteerd en verwerkt, wordt het model getraind om consistente voorspellingen te maken voor deze variaties. Dit dwingt het model om stabiele representaties te leren, zelfs wanneer kleine veranderingen worden aangebracht in de inputdata. Net zoals FixMatch minimaliseert MixMatch een gecombineerde \gls{loss_functie} die zowel de gelabelde als de pseudo-gelabelde voorbeelden gebruikt (cf. supra). \autocite{Berthelot_2019}