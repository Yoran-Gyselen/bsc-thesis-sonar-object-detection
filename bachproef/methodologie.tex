%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{Methodologie}%
\label{ch:methodologie}

Dit onderzoek volgt een gestructureerde aanpak om semi- en self-supervised learning technieken voor objectdetectie in sonardata te implementeren en te evalueren. In deze methodologie wordt een onderverdeling gemaakt van de verschillende fasen in dit onderzoek. Hierbij wordt de basis gelegd voor de experimenten die zullen worden uitgevoerd in de proof of concept.

\section{Data-acquisitie}

Allereerst moet er een keuze gemaakt worden voor het gebruik van een dataset in het verdere verloop van dit onderzoek. Alle drie de datasets die in \ref{subsec:mogelijke-oplossingen} besproken werden, maken het mogelijk om een objectdetectiemodel mee te trainen. Ze bieden namelijk allemaal annotaties van \glspl{bounding_box} op de bijhorende afbeeldingen aan. Daarnaast zijn deze datasets makkelijk te verwerken, aangezien alle beelden in conventionele afbeeldingsformaten zijn opgeslagen (zoals PNG, JPG, BMP, PGM, \dots). Toch is er -- specifiek voor dit onderzoek -- één dataset die geschikter is dan de anderen. De UATD-dataset is -- misschien ietwat subjectief -- uitgekozen om te gebruiken in de rest van dit onderzoek. Dit komt omdat ze bepaalde aspecten aanbiedt die de andere datasets niet hebben. \\

\gls{sss} for Mine Detection lijkt op het eerste zicht de perfecte dataset voor dit onderzoek. Het probleem is echter dat ze relatief klein is: ze bevat ``slechts'' 1170 afbeeldingen. Op het eerste zicht lijkt dit voldoende. Echter moet deze dataset nog opgesplitst worden in -- ten minste -- een trainingsset en een testset.\footnote{In een optimale situatie zou de data opgesplitst worden in drie sets: een trainingsset, een testset en een validatieset. Dit komt omdat de validatiedataset -- hoewel ze niet gebruikt wordt om het model te trainen -- gebruikt wordt om de paramaters van het model te tunen. Dit kan leiden tot \gls{overfitting}. Het is beter om als testset data te gebruiken dat het model nog niet gezien heeft. \autocite{Goodfellow_2016}} Ook komen er slechts 668 objecten voor in de dataset. Tot overmaat van ramp zijn deze ook zeer slecht verdeeld. 

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{\# objecten / beeld} & \textbf{\# beelden} \\
        \midrule
        13 & 1 \\
        9  & 2 \\
        8  & 4 \\
        7  & 8 \\
        6  & 8 \\
        5  & 13 \\
        4  & 9 \\
        3  & 41 \\
        2  & 59 \\
        1  & 159 \\
        0  & 866 \\
        \bottomrule
    \end{tabular}
    \caption[Aantal objecten per afbeelding in SSS for Mine Data]{\label{tab:objects_per_image_sss} Tabel met verdeling van objecten per afbeelding in de \gls{sss} for Mine Detection-dataset.}
\end{table}

Er is één afbeelding met wel 13 objecten en 866 zonder ook maar één object. Door deze slechte verdeling en de beperkte hoeveelheid data in de dataset is ze dus weinig bruikbaar voor dit onderzoek. \\

Dit is een probleem waar de UXO-dataset absoluut niet mee kampt. Deze heeft dan echter weer andere problemen. De dataset is namelijk volledig samengesteld in een gecontroleerde testopstelling. Ze bevat dus geen \emph{real-world}-data. Dit betekent echter ook dat bepaalde artefacten en afwijkingen typisch aan meren en zeeën niet in deze dataset voorkomen. De beelden zijn zodanig zuiver dat het hoogstwaarschijnlijk mogelijk zou zijn om de \glspl{blindganger} te herkennen door te zoeken naar de groep helderste pixels of met een edge-detection algoritme. \autocite{Torre_1986} \\

Ook de grootte van de dataset is misschien iets te mooi om waar te zijn. De beelden in de dataset zijn namelijk geen onafhankelijke afbeeldingen, maar frames van een continue opname. Dit zorgt ervoor dat er (nagenoeg) geen verschil is tussen afbeelding $n$ en afbeelding $n+1$. Als alle afbeeldingen na elkaar worden afgespeeld, ziet men een opname van een transformatie (rotatie, verschuiving, \dots) van één van de \glspl{blindganger}. Ten slotte staat er telkens maar één object op een afbeelding, wat multiple objectdetectie (meerdere objecten op één afbeelding herkennen) onmogelijk maakt.

\section{Dataverdeling}

In dit onderzoek zullen verschillende modellen getraind worden met verschillende leertechnieken. Daarom is het belangrijk om een duidelijk zicht te krijgen op hoe de gekozen dataset verdeeld moet worden zodat dit efficiënt en effectief kan gebeuren. Zoals vermeld zal er gebruik gemaakt worden van de UATD-dataset. Aangezien deze al opgesplitst is in drie subsets, zal de data als volgt verdeeld worden:

\begin{itemize}
    \item \texttt{UATD\_Training}: trainingsset (7600 samples)
    \item \texttt{UATD\_Test\_1}: testset (800 samples)
    \item \texttt{UATD\_Test\_2}: validatieset (waar nodig/mogelijk) (800 samples)
\end{itemize}

De trainingsset bevat 7600 gelabelde samples. Om de hypotheses in dit onderzoek echter te kunnen testen, zal deze set om bepaalde modellen te trainen verder opgesplitst worden in een gelabelde en een ongelabelde set. Om de resultaten van \gls{ssl} en \gls{self-sl} beter te kunnen evalueren, zal en volledig gesuperviseerd model getraind worden op subsets van de gelabelde data. Meer specifiek gaat dit om: 100\% (7600 samples), 50\% (3800 samples), 10\% (760 samples), 5\% (380 samples) en 1\% (76 samples). In deze fase worden de overige ongelabelde samples gewoonweg niet gebruikt. \\

Omwille van de resource-intensiviteit om deze modellen te trainen en het feit dat dit ``slechts'' een proof of concept is, zullen de \gls{ssl}- en \gls{self-sl}-modellen dit trainingsschema niet volgen. Ze worden getraind op de twee subsets die het interessantst zijn voor dit onderzoek. Een subset van 10\% (760 samples) zal gebruikt worden om de praktische werking in een realistisch scenario aan te tonen. Daarnaast zal een subset van 5\% (380 samples) gebruikt worden om een mogelijk scenario waar data zeer schaars is na te bootsen. De overige samples zullen telkens gebruikt worden als ongelabelde trainingsdata. De modellen worden in se dus telkens op de volledige trainingsset getraind, echter is de leertechniek -- en dus de verhouding gelabeld/ongelabeld -- voor elk model verschillend.

\section{Modelselectie}

\subsection{Supervised: Faster R-CNN}



\subsection{Semi-supervised: FixMatch}

\subsection{Self-supervised: BYOL pre-training}

\section{Resultaten en evaluatie}

Uiteindelijk zullen de verschillende uitgevoerde experimenten worden geëvalueerd en met elkaar vergeleken. Dit zal op verschillende manieren gedaan worden. Specifiek gaat dit om kwantitatieve en kwalitatieve methoden. Kwantitatief zal vooral \acrfull{map} gebruikt worden om objectdetectieprestaties te beoordelen. Echter is dit niet de enige kwantitatieve metriek die er toe doet. Naast de effectieve performantie van het model is het ook belangrijk om rekening te houden met de resources die nodig zijn om een bepaald model te trainen. Ook wordt een vergelijking gemaakt tussen dezelfde modellen die getraind zijn met een verschillende verdeling van gelabelde en ongelabelde data. Dit om de label-efficiëntie (Hoe goed presteert het model met een beperkte hoeveelheid gelabelde data?) te meten. Dit zal cijfermatig uitsluitsel geven over de effectiviteit van semi-supervised en self-supervised learning tegenover supervised learning. \\

Naast de kwantitatieve analyse wordt er ook een kwalitatieve analyse uitgevoerd. De focus ligt hierbij niet zozeer op de cijfers, maar eerder op de effectieve voorspellingen van de modellen. Hierbij zal sterk worden gebruik gemaakt van beeldmateriaal en de voorspellingen gemaakt door de modellen. Hoewel een score heel objectief uitsluitsel kan geven over de performantie van een model, zijn real-world voorbeelden nog steeds waardevol voor de uiteindelijke evaluatie. Een score houdt namelijk niet altijd (genoeg) rekening met dingen die voor de eindgebruiker belangrijk zijn en omgekeerd. \\

Uiteindelijk zullen enkele experts in sonaranalyse de bruikbaarheid van de resultaten beoordelen en aanbevelingen geven voor verdere verbeteringen. Ten slotte zullen de methodologie, resultaten en code worden gedocumenteerd om reproduceerbaarheid te waarborgen.