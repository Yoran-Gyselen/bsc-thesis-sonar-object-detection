%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{Methodologie}%
\label{ch:methodologie}

Dit onderzoek volgt een gestructureerde aanpak om \gls{ssl} en \gls{self-sl} technieken voor objectdetectie in sonardata te implementeren en te evalueren. In deze methodologie wordt een onderverdeling gemaakt van de verschillende fasen in dit onderzoek. Hierbij wordt de basis gelegd voor de experimenten die zullen worden uitgevoerd in de proof of concept.

\section{Data-acquisitie}

Allereerst moet er een keuze gemaakt worden voor het gebruik van een dataset in het verdere verloop van dit onderzoek. Alle drie de datasets die in \ref{subsec:mogelijke-oplossingen} besproken werden, maken het mogelijk om een objectdetectiemodel mee te trainen. Ze bieden namelijk allemaal annotaties van \glspl{bounding_box} op de bijhorende afbeeldingen aan. Daarnaast zijn deze datasets makkelijk te verwerken, aangezien alle beelden in conventionele afbeeldingsformaten zijn opgeslagen (zoals PNG, JPG, BMP, PGM, \dots). Toch is er -- specifiek voor dit onderzoek -- één dataset die geschikter is dan de anderen. De UATD-dataset is -- misschien ietwat subjectief -- uitgekozen om te gebruiken in de rest van dit onderzoek. Dit komt omdat ze bepaalde aspecten aanbiedt die de andere datasets niet hebben. \\

\gls{sss} for Mine Detection lijkt op het eerste zicht de perfecte dataset voor dit onderzoek. Het probleem is echter dat ze relatief klein is: ze bevat ``slechts'' 1170 afbeeldingen. Op het eerste zicht lijkt dit voldoende. Echter moet deze dataset nog opgesplitst worden in -- ten minste -- een trainingsset en een testset.\footnote{In een optimale situatie zou de data opgesplitst worden in drie sets: een trainingsset, een testset en een validatieset. Dit komt omdat de validatiedataset -- hoewel ze niet gebruikt wordt om het model te trainen -- gebruikt wordt om de paramaters van het model te tunen. Dit kan leiden tot \gls{overfitting}. Het is beter om als testset data te gebruiken dat het model nog niet gezien heeft. \autocite{Goodfellow_2016}} Ook komen er slechts 668 objecten voor in de dataset. Tot overmaat van ramp zijn deze ook zeer slecht verdeeld. 

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{\# objecten / beeld} & \textbf{\# beelden} \\
        \midrule
        13 & 1 \\
        9  & 2 \\
        8  & 4 \\
        7  & 8 \\
        6  & 8 \\
        5  & 13 \\
        4  & 9 \\
        3  & 41 \\
        2  & 59 \\
        1  & 159 \\
        0  & 866 \\
        \bottomrule
    \end{tabular}
    \caption[Aantal objecten per afbeelding in SSS for Mine Data]{\label{tab:objects_per_image_sss} Tabel met verdeling van objecten per afbeelding in de \gls{sss} for Mine Detection-dataset.}
\end{table}

Er is één afbeelding met wel 13 objecten en 866 zonder ook maar één object. Door deze slechte verdeling en de beperkte hoeveelheid data in de dataset is ze dus weinig bruikbaar voor dit onderzoek. \\

Dit is een probleem waar de UXO-dataset absoluut niet mee kampt. Deze heeft dan echter weer andere problemen. De dataset is namelijk volledig samengesteld in een gecontroleerde testopstelling. Ze bevat dus geen \emph{real-world}-data. Dit betekent echter ook dat bepaalde artefacten en afwijkingen typisch aan meren en zeeën niet in deze dataset voorkomen. De beelden zijn zodanig zuiver dat het hoogstwaarschijnlijk mogelijk zou zijn om de \glspl{blindganger} te herkennen door te zoeken naar de groep helderste pixels of met een edge-detection algoritme. \autocite{Torre_1986} \\

Ook de grootte van de dataset is misschien iets te mooi om waar te zijn. De beelden in de dataset zijn namelijk geen onafhankelijke afbeeldingen, maar frames van een continue opname. Dit zorgt ervoor dat er (nagenoeg) geen verschil is tussen afbeelding $n$ en afbeelding $n+1$. Als alle afbeeldingen na elkaar worden afgespeeld, ziet men een opname van een transformatie (rotatie, verschuiving, \dots) van één van de \glspl{blindganger}. Ten slotte staat er telkens maar één object op een afbeelding, wat multiple objectdetectie (meerdere objecten op één afbeelding herkennen) onmogelijk maakt.

\section{Dataverdeling}
\label{sec:dataverdeling}

In dit onderzoek zullen verschillende modellen getraind worden met verschillende leertechnieken. Daarom is het belangrijk om een duidelijk zicht te krijgen op hoe de gekozen dataset verdeeld moet worden zodat dit efficiënt en effectief kan gebeuren. Zoals vermeld zal er gebruik gemaakt worden van de UATD-dataset. Aangezien deze al opgesplitst is in drie subsets, zal de data als volgt verdeeld worden:

\begin{itemize}
    \item \texttt{UATD\_Training}: trainingsset (7600 samples)
    \item \texttt{UATD\_Test\_1}: testset (800 samples)
    \item \texttt{UATD\_Test\_2}: validatieset (waar nodig/mogelijk) (800 samples)
\end{itemize}

De trainingsset bevat 7600 gelabelde samples. Om de hypotheses in dit onderzoek echter te kunnen testen, zal deze set om bepaalde modellen te trainen verder opgesplitst worden in een gelabelde en een ongelabelde set. Om de resultaten van \gls{ssl} en \gls{self-sl} beter te kunnen evalueren, zal en volledig gesuperviseerd model getraind worden op subsets van de gelabelde data. Meer specifiek gaat dit om: 100\% (7600 samples), 50\% (3800 samples), 10\% (760 samples), 5\% (380 samples) en 1\% (76 samples). In deze fase worden de overige ongelabelde samples gewoonweg niet gebruikt. \\

Omwille van de resource-intensiviteit om deze modellen te trainen en het feit dat dit ``slechts'' een proof of concept is, zullen de \gls{ssl}- en \gls{self-sl}-modellen dit trainingsschema niet volgen. Ze worden getraind op de twee subsets die het interessantst zijn voor dit onderzoek. Een subset van 10\% (760 samples) zal gebruikt worden om de praktische werking in een realistisch scenario aan te tonen. Daarnaast zal een subset van 5\% (380 samples) gebruikt worden om een mogelijk scenario waar data zeer schaars is na te bootsen. De overige samples zullen telkens gebruikt worden als ongelabelde trainingsdata. De modellen worden in se dus telkens op de volledige trainingsset getraind, echter is de leertechniek -- en dus de verhouding gelabeld/ongelabeld -- voor elk model verschillend.

\section{Modelselectie}
\label{sec:modelselectie}

\subsection{Supervised: Faster R-CNN}

Voor de gesuperviseerde baseline binnen dit onderzoek is gekozen voor Faster \gls{rcnn}. Deze keuze is gemotiveerd door de hoge nauwkeurigheid die het model biedt bij objectdetectie, in het bijzonder in situaties waar precisie belangrijker is dan snelheid. In tegenstelling tot de andere modellen: \gls{yolo} en \gls{ssd}, die geoptimaliseerd zijn voor real-time toepassingen en een lagere latency, biedt Faster \gls{rcnn} doorgaans betere prestaties op het gebied van lokalisatie en classificatie. Deze eigenschappen zijn enorm waardevol in de context van sonardata, waar objecten meestal subtiele kenmerken vertonen en nauwkeurige detectie essentieel is. \\

Sonardata is ruisachtig, bevat lage resolutiebeelden, en vertoont objecten met weinig visuele contrasten. Dit is zeer goed te zien in de dataset die voor dit onderzoek gebruikt wordt. Hierbij is het zelfs moeilijk om als mens zonder getraind oog objecten van elkaar te onderscheiden en te benoemen. In dergelijke gevallen is een model dat profiteert van een two-stage detectieproces, zoals Faster \gls{rcnn}, beter in staat om relevante objecten van achtergrondruis te onderscheiden. Dit two-stage proces -- bestaande uit een \gls{rpn} gevolgd door een classificatie- en regressiehead -- stelt het model in staat om meer contextueel geïnformeerde en preciezere objectvoorspellingen te doen. \\

Bovendien wordt Faster \gls{rcnn} in de literatuur vaak gebruikt als objectdetectiemodel voor vergelijkbaar onderzoek, zo ook in de paper van \textcite{Xie_2022}, waar de dataset die ook voor dit onderzoek gebruikt wordt, praktisch onderzocht wordt. Dit maakt het model niet alleen geschikt voor de sonardataset zelf, maar ook vergelijkbaar met bestaande benchmarks, wat cruciaal is voor het evalueren van de effectiviteit van alternatieve leermethoden. In tegenstelling tot single-shot detectors zoals \gls{ssd} en \gls{yolo}, die sneller zijn maar mogelijk minder goed generaliseren in \gls{ssl} en \gls{self-sl}-settings, biedt Faster \gls{rcnn} een betere balans tussen generaliseerbaarheid en performance, vooral in domeinen met beperkte gelabelde data. \\

Ten slotte is Faster \gls{rcnn} flexibel en goed ondersteund in frameworks zoals PyTorch, waardoor het eenvoudig geïntegreerd kan worden in experimenten met \gls{ssl} en \gls{self-sl}-technieken. Deze praktische overweging speelt eveneens een rol in de keuze voor dit model als fundament binnen dit onderzoek.

\subsection{Semi-supervised: FixMatch}

Voor het \gls{ssl}-model is gekozen voor FixMatch, een methode die sterke prestaties laat zien in settings met beperkte gelabelde data. FixMatch combineert de simpliciteit van pseudo-labeling met de effectiviteit van consistency regularization. Binnen FixMatch worden deze twee benaderingen op een efficiënte manier geïntegreerd. In tegenstelling tot het traditionele pseudo-labeling, waarbij zwak gecontroleerde voorspellingen worden gebruikt als labels voor ongesuperviseerde data, legt FixMatch een threshold op: alleen voorspellingen met hoge waarschijnlijkheid worden gebruikt als pseudo-labels. Hierdoor vermindert het risico op het versterken van foutieve voorspellingen -- een cruciaal voordeel binnen sonardata, waar objecten vaak vaag of slecht afgebakend zijn. \\

Een ander sterk punt van FixMatch is het gebruik van sterke en zwakke augmentaties binnen het consistency framework. Door hetzelfde ongesuperviseerde voorbeeld onder lichte (zwakke) en zware (sterke) augmentaties aan te bieden, en het model te trainen om consistente voorspellingen te doen, wordt het model robuust gemaakt. In de context van sonardata -- waar visuele variatie door ruis of reflecties kan voorkomen -- helpt dit mechanisme het model om betekenisvolle representaties te leren. Dit is effectiever dan eenvoudige consistency regularization zonder augmentatievariatie, dat gevoeliger kan zijn voor overfitting aan irrelevante ruis in de data. \\

Hoewel MixMatch ook een krachtig \gls{ssl}-framework is dat meerdere strategieën (waaronder mixup) combineert, vereist het een relatief complex trainingsproces en intensieve hyperparametertuning. FixMatch daarentegen biedt een eenvoudigere implementatie en minder afhankelijkheid van hyperparameters, wat het aantrekkelijk maakt voor toepassingen in domeinspecifieke data zoals sonar, waar experimenteren met veel configuraties moeilijk is door beperkte data en computationele resources. \\

Samenvattend biedt FixMatch een sterke combinatie van betrouwbaarheid, eenvoud en robuustheid. De methode is bijzonder geschikt voor domeinen met weinig gelabelde data, hoge data-variabiliteit en lage visuele resolutie, zoals bij sonarbeelden. Hierdoor vormt het een degelijke keuze voor dit onderzoek, zowel vanuit technisch perspectief als met het oog op reproduceerbaarheid en vergelijkbaarheid met vergelijkbare literatuur in \gls{ssl}.

\subsection{Self-supervised: BYOL pre-training}

Voor het \gls{self-sl}-leerproces is gekozen voor \gls{byol} als pretrainingmethode, met als doel een krachtige feature extractor (backbone) te trainen zonder gebruik te maken van gelabelde data. Deze backbone wordt vervolgens geïntegreerd in de supervised baseline, Faster \gls{rcnn}, om de prestaties bij objectdetectie in sonardata te verbeteren. \gls{byol} is geselecteerd vanwege zijn robuuste mogelijkheid om representaties aan te leren zonder negatieve paren, wat het bijzonder geschikt maakt voor data met beperkte visuele variatie zoals sonarbeelden. \\

In tegenstelling tot methoden zoals \gls{simclr} en \gls{moco}, die sterk leunen op contrastieve loss en dus negatieve voorbeelden nodig hebben, leert \gls{byol} door het afstemmen van een online netwerk op een target netwerk dat langzaam geüpdatet wordt via momentum. Dit maakt \gls{byol} stabieler en effectiever bij kleinere \glspl{batch_size}, een belangrijk voordeel in scenario's met beperkte computationele resources of datasets die niet rijk zijn aan semantische variatie, zoals bij sonar. Bovendien vermindert het ontbreken van negatieve paren het risico dat semantisch gelijkaardige objecten (zoals verschillende onderwaterstructuren) foutief als verschillend worden gemodelleerd. \\

\gls{simclr}, hoewel krachtig in standaard visuele benchmarks, vereist grote \glspl{batch_size} en geavanceerde augmentatiestrategieën om goed te presteren, wat in de praktijk minder haalbaar is bij domeineigen data zoals sonar. \gls{moco} probeert dit te verhelpen met een memory bank, maar introduceert tegelijkertijd extra complexiteit, berust sterk op hyperparameters en vertrouwt nog steeds op de aanwezigheid van duidelijke negatieve voorbeelden -- iets wat binnen sonardata niet vanzelfsprekend is. In vergelijking daarmee biedt \gls{byol} een eenvoudiger trainingsproces met consistente resultaten, zelfs zonder zware tuning. \\

Tot slot heeft de literatuur aangetoond dat het pretrainen met \gls{byol} en het finetunen voor een downstream taak -- zoals Faster \gls{rcnn} in dit onderzoek -- leidt tot betere generalisatie en snellere convergentie dan alternatieve \gls{self-sl}-methoden. In dit onderzoek speelt dit een cruciale rol, omdat het model zich moet kunnen aanpassen aan objecten die zelden voorkomen, slecht gedefinieerd zijn, of in onbekende vormen voorkomen. \gls{byol} draagt bij aan het leren van semantisch rijke en transfereerbare representaties, die de prestaties van het uiteindelijke detectiemodel zullen verbeteren zonder afhankelijkheid van ``dure'' annotaties.

\section{Resultaten en evaluatie}

Uiteindelijk zullen de verschillende uitgevoerde experimenten worden geëvalueerd en met elkaar vergeleken. Dit zal op verschillende manieren gedaan worden. Specifiek gaat dit om kwantitatieve en kwalitatieve methoden. Kwantitatief zal vooral \acrfull{map} gebruikt worden om objectdetectieprestaties te beoordelen. Echter is dit niet de enige kwantitatieve metriek die er toe doet. Naast de effectieve performantie van het model is het ook belangrijk om rekening te houden met de resources die nodig zijn om een bepaald model te trainen. Ook wordt een vergelijking gemaakt tussen dezelfde modellen die getraind zijn met een verschillende verdeling van gelabelde en ongelabelde data. Dit om de label-efficiëntie (Hoe goed presteert het model met een beperkte hoeveelheid gelabelde data?) te meten. Dit zal cijfermatig uitsluitsel geven over de effectiviteit van semi-supervised en self-supervised learning tegenover supervised learning. \\

Naast de kwantitatieve analyse wordt er ook een kwalitatieve analyse uitgevoerd. De focus ligt hierbij niet zozeer op de cijfers, maar eerder op de effectieve voorspellingen van de modellen. Hierbij zal sterk worden gebruik gemaakt van beeldmateriaal en de voorspellingen gemaakt door de modellen. Hoewel een score heel objectief uitsluitsel kan geven over de performantie van een model, zijn real-world voorbeelden nog steeds waardevol voor de uiteindelijke evaluatie. Een score houdt namelijk niet altijd (genoeg) rekening met dingen die voor de eindgebruiker belangrijk zijn en omgekeerd. \\

Uiteindelijk zullen enkele experts in sonaranalyse de bruikbaarheid van de resultaten beoordelen en aanbevelingen geven voor verdere verbeteringen. Ten slotte zullen de methodologie, resultaten en code worden gedocumenteerd om reproduceerbaarheid te waarborgen.