% Encoding: UTF-8

@Article{Aubard_2024_Datasets,
  author      = {Aubard, Martin and Madureira, Ana and Teixeira, Luís and Pinto, José},
  date        = {2024-12-16},
  title       = {Sonar-based Deep Learning in Underwater Robotics: Overview, Robustness and Challenges},
  doi         = {10.48550/ARXIV.2412.11840},
  eprint      = {2412.11840},
  eprintclass = {cs.RO},
  eprinttype  = {arXiv},
  abstract    = {With the growing interest in underwater exploration and monitoring, Autonomous Underwater Vehicles (AUVs) have become essential. The recent interest in onboard Deep Learning (DL) has advanced real-time environmental interaction capabilities relying on efficient and accurate vision-based DL models. However, the predominant use of sonar in underwater environments, characterized by limited training data and inherent noise, poses challenges to model robustness. This autonomy improvement raises safety concerns for deploying such models during underwater operations, potentially leading to hazardous situations. This paper aims to provide the first comprehensive overview of sonar-based DL under the scope of robustness. It studies sonar-based DL perception task models, such as classification, object detection, segmentation, and SLAM. Furthermore, the paper systematizes sonar-based state-of-the-art datasets, simulators, and robustness methods such as neural network verification, out-of-distribution, and adversarial attacks. This paper highlights the lack of robustness in sonar-based DL research and suggests future research pathways, notably establishing a baseline sonar-based dataset and bridging the simulation-to-reality gap.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Aubard_2024_Datasets - Sonar Based Deep Learning in Underwater Robotics_ Overview, Robustness and Challenges.pdf:PDF:http\://arxiv.org/pdf/2412.11840v1},
  groups      = {Data},
  keywords    = {Robotics (cs.RO), Computer Vision and Pattern Recognition (cs.CV), Signal Processing (eess.SP), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  publisher   = {arXiv},
  year        = {2024},
}

@Article{Pessanha_Santos_2024,
  author       = {Pessanha Santos, Nuno and Moura, Ricardo and Sampaio Torgal, Gonçalo and Lobo, Victor and Neto, Miguel de Castro},
  date         = {2024-04},
  journaltitle = {Data in Brief},
  title        = {Side-scan sonar imaging data of underwater vehicles for mine detection},
  doi          = {10.1016/j.dib.2024.110132},
  issn         = {2352-3409},
  pages        = {110132},
  volume       = {53},
  file         = {:Pessanha_Santos_2024 - Side Scan Sonar Imaging Data of Underwater Vehicles for Mine Detection.pdf:PDF},
  groups       = {Dataset Papers},
  publisher    = {Elsevier BV},
}

@Misc{Dahn_2024_UXO,
  author    = {Dahn, Nikolas and Bande Firvida, Miguel and Sharma, Proneet and Mohrmann, Jochen and Geisler, Oliver and Sanghamreddy, Prithvi Kumar and Marquardt, Kevin and Christensen, Leif},
  date      = {2024},
  title     = {An Acoustic and Optical Dataset for the Perception of Underwater Unexploded Ordnance (UXO)},
  doi       = {10.5281/ZENODO.11068045},
  copyright = {BSD 3-Clause "New" or "Revised" License},
  groups    = {Datasets},
  keywords  = {Unexploded Ordnance, UXO, Environment Pollution, Imaging Sonar},
  publisher = {Zenodo},
}

@Article{Xie_2022,
  author       = {Xie, Kaibing and Yang, Jian and Qiu, Kang},
  date         = {2022-12},
  journaltitle = {Scientific Data},
  title        = {A Dataset with Multibeam Forward-Looking Sonar for Underwater Object Detection},
  doi          = {10.1038/s41597-022-01854-w},
  issn         = {2052-4463},
  number       = {1},
  volume       = {9},
  file         = {:Xie_2022 - A Dataset with Multibeam Forward Looking Sonar for Underwater Object Detection.pdf:PDF:https\://www.nature.com/articles/s41597-022-01854-w.pdf},
  groups       = {Dataset Papers},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Murray_Rust_2008,
  author       = {Murray-Rust, Peter},
  date         = {2008-01},
  journaltitle = {Nature Precedings},
  title        = {Open Data in Science},
  doi          = {10.1038/npre.2008.1526.1},
  issn         = {1756-0357},
  file         = {:Murray_Rust_2008 - Open Data in Science.pdf:PDF:https\://www.nature.com/articles/npre.2008.1526.1.pdf},
  groups       = {Data},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Aubard_2024_ROSAR,
  author      = {Aubard, Martin and Antal, László and Madureira, Ana and Teixeira, Luis F. and Ábrahám, Erika},
  date        = {2024-10-14},
  title       = {ROSAR: An Adversarial Re-Training Framework for Robust Side-Scan Sonar Object Detection},
  doi         = {10.48550/ARXIV.2410.10554},
  eprint      = {2410.10554},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {This paper introduces ROSAR, a novel framework enhancing the robustness of deep learning object detection models tailored for side-scan sonar (SSS) images, generated by autonomous underwater vehicles using sonar sensors. By extending our prior work on knowledge distillation (KD), this framework integrates KD with adversarial retraining to address the dual challenges of model efficiency and robustness against SSS noises. We introduce three novel, publicly available SSS datasets, capturing different sonar setups and noise conditions. We propose and formalize two SSS safety properties and utilize them to generate adversarial datasets for retraining. Through a comparative analysis of projected gradient descent (PGD) and patch-based adversarial attacks, ROSAR demonstrates significant improvements in model robustness and detection accuracy under SSS-specific conditions, enhancing the model's robustness by up to 1.85\%. ROSAR is available at https://github.com/remaro-network/ROSAR-framework.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Aubard_2024_ROSAR - ROSAR_ an Adversarial Re Training Framework for Robust Side Scan Sonar Object Detection.pdf:PDF:http\://arxiv.org/pdf/2410.10554v1},
  groups      = {Dataset Papers},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Robotics (cs.RO), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2024},
}

@Misc{Aubard_2024_SWDD,
  author    = {Aubard, Martin and Antal, László and Madureira, Maria and F. Teixeira, Luis and Ábrahám, Erika},
  date      = {2024},
  title     = {SWDD: Sonar Wall Detection Dataset},
  doi       = {10.5281/ZENODO.10528134},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  publisher = {Zenodo},
}

@Article{Alvarez_Tunon_2024,
  author      = {Álvarez-Tuñón, Olaya and Marnet, Luiza Ribeiro and Antal, László and Aubard, Martin and Costa, Maria and Brodskiy, Yury},
  date        = {2024-01-31},
  title       = {SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization},
  doi         = {10.48550/ARXIV.2401.17907},
  eprint      = {2401.17907},
  eprintclass = {cs.RO},
  eprinttype  = {arXiv},
  abstract    = {This paper presents SubPipe, an underwater dataset for SLAM, object detection, and image segmentation. SubPipe has been recorded using a \gls{LAUV}, operated by OceanScan MST, and carrying a sensor suite including two cameras, a side-scan sonar, and an inertial navigation system, among other sensors. The AUV has been deployed in a pipeline inspection environment with a submarine pipe partially covered by sand. The AUV's pose ground truth is estimated from the navigation sensors. The side-scan sonar and RGB images include object detection and segmentation annotations, respectively. State-of-the-art segmentation, object detection, and SLAM methods are benchmarked on SubPipe to demonstrate the dataset's challenges and opportunities for leveraging computer vision algorithms. To the authors' knowledge, this is the first annotated underwater dataset providing a real pipeline inspection scenario. The dataset and experiments are publicly available online at https://github.com/remaro-network/SubPipe-dataset},
  copyright   = {Creative Commons Attribution Share Alike 4.0 International},
  file        = {:Alvarez_Tunon_2024 - SubPipe_ a Submarine Pipeline Inspection Dataset for Segmentation and Visual Inertial Localization.pdf:PDF:http\://arxiv.org/pdf/2401.17907v2},
  groups      = {Dataset Papers},
  keywords    = {Robotics (cs.RO), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2024},
}

@Misc{Jian_2022,
  author    = {Jian, Yang and Kaibing, Xie},
  date      = {2022},
  title     = {Underwater acoustic target detection (UATD) dataset},
  doi       = {10.6084/M9.FIGSHARE.21331143.V3},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  keywords  = {91103 Ocean Engineering, FOS: Environmental engineering},
  publisher = {figshare},
}

@Article{Bourke_1998,
  author       = {Bourke, Paul},
  date         = {1998},
  journaltitle = {BMP Files. July},
  title        = {Bmp image format},
  volume       = {8},
  file         = {:Bourke_1998 - Bmp Image Format.pdf:PDF},
  groups       = {Data},
}

@Article{Everingham_2009,
  author       = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  date         = {2009-09},
  journaltitle = {International Journal of Computer Vision},
  title        = {The Pascal Visual Object Classes (VOC) Challenge},
  doi          = {10.1007/s11263-009-0275-4},
  issn         = {1573-1405},
  number       = {2},
  pages        = {303--338},
  volume       = {88},
  file         = {:Everingham_2009 - The Pascal Visual Object Classes (VOC) Challenge.pdf:PDF:https\://www.pure.ed.ac.uk/ws/files/7879113/ijcv_voc09.pdf},
  groups       = {Data},
  publisher    = {Springer Science and Business Media LLC},
}

@Misc{Pessanha_Santos_2024_SSSFMD,
  author    = {Pessanha Santos, Nuno and Moura, Ricardo},
  date      = {2024},
  title     = {Side-scan sonar imaging for Mine detection},
  doi       = {10.6084/M9.FIGSHARE.24574879},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  keywords  = {Ocean engineering, Field robotics, Autonomous vehicle systems},
  publisher = {figshare},
}

@Book{Geron_2023,
  author    = {Géron, Aurélien},
  date      = {2023},
  title     = {Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow},
  edition   = {Third edition},
  isbn      = {9781098125974},
  location  = {Beijing},
  note      = {Literaturangaben. - Index},
  pagetotal = {834},
  publisher = {O'Reilly},
  series    = {Data science / machine learning},
  subtitle  = {Concepts, tools, and techniques to build intelligent systems},
  file      = {:Geron_2023 - Hands on Machine Learning with Scikit Learn, Keras, and TensorFlow.pdf:PDF},
  groups    = {Deep Learning},
  ppn_gvk   = {1799537536},
}

@InProceedings{Dahn_2024,
  author    = {Dahn, Nikolas and Firvida, Miguel Bande and Sharma, Proneet and Christensen, Leif and Geisle, Oliver and Mohrmann, Jochen and Frey, Torsten and Kumar Sanghamreddy, Prithvi and Kirchner, Frank},
  booktitle = {OCEANS 2024 - Halifax},
  date      = {2024-09},
  title     = {An Acoustic and Optical Dataset for the Perception of Underwater Unexploded Ordnance (UXO)},
  doi       = {10.1109/oceans55160.2024.10754316},
  pages     = {1--6},
  publisher = {IEEE},
  file      = {:Dahn_2024 - An Acoustic and Optical Dataset for the Perception of Underwater Unexploded Ordnance (UXO).pdf:PDF},
  groups    = {Dataset Papers},
}

@Misc{Alvarez_Tunon_2024_SubPipe,
  author    = {Álvarez-Tuñón, Olaya and Ribeiro Marnet, Luiza and Antal, László and Aubard, Martin and Costa, Maria and Brodskiy, Yury},
  date      = {2024},
  title     = {SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization},
  doi       = {10.5281/ZENODO.10053564},
  language  = {en},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  keywords  = {underwater dataset, pipeline, RGB and grayscale camera, side-scan sonar, SLAM, object detection, semantic segmentation},
  publisher = {Zenodo},
}

@WWW{Poskanzer_2016,
  author = {Jef Poskanzer},
  date   = {2016-10-09},
  title  = {pgm - Netpbm grayscale image format},
  url    = {https://netpbm.sourceforge.net/doc/pgm.html},
  file   = {:Poskanzer_2016 - Pgm Netpbm Grayscale Image Format.pdf:PDF},
  groups = {Specs & Formats},
}

@Book{Goodfellow_2016,
  author    = {Goodfellow, Ian and Bengio Yoshua and Courville, Aaron},
  date      = {2016},
  title     = {Deep learning},
  editor    = {Aaron Courville and Yoshua Bengio},
  isbn      = {9780262337373},
  location  = {Cambridge, Massachusetts},
  note      = {Includes bibliographical references and index},
  pagetotal = {1775},
  publisher = {The MIT Press},
  series    = {Adaptive computation and machine learning},
  file      = {:Goodfellow_2016 - Deep Learning.pdf:PDF},
  groups    = {Deep Learning},
  ppn_gvk   = {1789979978},
}

@Article{Torre_1986,
  author       = {Torre, Vincent and Poggio, Tomaso A.},
  date         = {1986-03},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {On Edge Detection},
  doi          = {10.1109/tpami.1986.4767769},
  issn         = {0162-8828},
  number       = {2},
  pages        = {147--163},
  volume       = {PAMI-8},
  file         = {:Torre_1986 - On Edge Detection.pdf:PDF},
  groups       = {Edge Detection},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Wang_2024,
  author       = {Wang, Wenling and Zhang, Qiaoxin and Qi, Zhisheng and Huang, Mengxing},
  date         = {2024-01},
  journaltitle = {Sensors},
  title        = {CenterNet-Saccade: Enhancing Sonar Object Detection with Lightweight Global Feature Extraction},
  doi          = {10.3390/s24020665},
  issn         = {1424-8220},
  number       = {2},
  pages        = {665},
  volume       = {24},
  file         = {:Wang_2024 - CenterNet Saccade_ Enhancing Sonar Object Detection with Lightweight Global Feature Extraction.pdf:PDF:https\://www.mdpi.com/1424-8220/24/2/665/pdf?version=1705744354},
  groups       = {Object Detection},
  publisher    = {MDPI AG},
}

@Article{Labbe_Morissette_2019,
  author      = {Labbe-Morissette, Guillaume and Gauthier, Sylvain},
  date        = {2019-09-17},
  title       = {A machine vision meta-algorithm for automated recognition of underwater objects using sidescan sonar imagery},
  doi         = {10.48550/ARXIV.1909.07763},
  eprint      = {1909.07763},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {This paper details a new method to recognize and detect underwater objects in real-time sidescan sonar data imagery streams, with case-studies of applications for underwater archeology, and ghost fishing gear retrieval. We first synthesize images from sidescan data, apply geometric and radiometric corrections, then use 2D feature detection algorithms to identify point clouds of descriptive visual microfeatures such as corners and edges in the sonar images. We then apply a clustering algorithm on the feature point clouds to group feature sets into regions of interest, reject false positives, yielding a georeferenced inventory of objects.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Labbe_Morissette_2019 - A Machine Vision Meta Algorithm for Automated Recognition of Underwater Objects Using Sidescan Sonar Imagery.pdf:PDF:http\://arxiv.org/pdf/1909.07763v1},
  groups      = {Object Detection},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  publisher   = {arXiv},
  year        = {2019},
}

@Article{Valdenegro_Toro_2019,
  author      = {Valdenegro-Toro, Matias},
  date        = {2019-07-01},
  title       = {Learning Objectness from Sonar Images for Class-Independent Object Detection},
  doi         = {10.48550/ARXIV.1907.00734},
  eprint      = {1907.00734},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Detecting novel objects without class information is not trivial, as it is difficult to generalize from a small training set. This is an interesting problem for underwater robotics, as modeling marine objects is inherently more difficult in sonar images, and training data might not be available apriori. Detection proposals algorithms can be used for this purpose but usually requires a large amount of output bounding boxes. In this paper we propose the use of a fully convolutional neural network that regresses an objectness value directly from a Forward-Looking sonar image. By ranking objectness, we can produce high recall (96 %) with only 100 proposals per image. In comparison, EdgeBoxes requires 5000 proposals to achieve a slightly better recall of 97 %, while Selective Search requires 2000 proposals to achieve 95 % recall. We also show that our method outperforms a template matching baseline by a considerable margin, and is able to generalize to completely new objects. We expect that this kind of technique can be used in the field to find lost objects under the sea.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Valdenegro_Toro_2019 - Learning Objectness from Sonar Images for Class Independent Object Detection.pdf:PDF:http\://arxiv.org/pdf/1907.00734v1},
  groups      = {Object Detection},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Robotics (cs.RO), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  publisher   = {arXiv},
  year        = {2019},
}

@Article{Valdenegro_Toro_2017,
  author       = {Valdenegro-Toro, Matias},
  date         = {2017-09-08},
  journaltitle = {Proceedings of ANNPR 2016},
  title        = {Objectness Scoring and Detection Proposals in Forward-Looking Sonar Images with Convolutional Neural Networks},
  doi          = {10.1007/978-3-319-46182-3_18},
  eprint       = {1709.02600},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  issn         = {1611-3349},
  pages        = {209--219},
  abstract     = {Forward-looking sonar can capture high resolution images of underwater scenes, but their interpretation is complex. Generic object detection in such images has not been solved, specially in cases of small and unknown objects. In comparison, detection proposal algorithms have produced top performing object detectors in real-world color images. In this work we develop a Convolutional Neural Network that can reliably score objectness of image windows in forward-looking sonar images and by thresholding objectness, we generate detection proposals. In our dataset of marine garbage objects, we obtain 94% recall, generating around 60 proposals per image. The biggest strength of our method is that it can generalize to previously unseen objects. We show this by detecting chain links, walls and a wrench without previous training in such objects. We strongly believe our method can be used for class-independent object detection, with many real-world applications such as chain following and mine detection.},
  booktitle    = {Artificial Neural Networks in Pattern Recognition},
  copyright    = {arXiv.org perpetual, non-exclusive license},
  file         = {:Valdenegro_Toro_2017 - Objectness Scoring and Detection Proposals in Forward Looking Sonar Images with Convolutional Neural Networks.pdf:PDF:http\://arxiv.org/pdf/1709.02600v1},
  groups       = {Object Detection},
  isbn         = {9783319461823},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences},
  publisher    = {Springer International Publishing},
  year         = {2017},
}

@Article{Otsu_1979,
  author       = {Otsu, Nobuyuki},
  date         = {1979-01},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  title        = {A Threshold Selection Method from Gray-Level Histograms},
  doi          = {10.1109/tsmc.1979.4310076},
  issn         = {2168-2909},
  number       = {1},
  pages        = {62--66},
  volume       = {9},
  file         = {:Otsu_1979 - A Threshold Selection Method from Gray Level Histograms.pdf:PDF},
  groups       = {Thresholding},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Priyadharsini_2019,
  author       = {Priyadharsini, R. and Sharmila, T. Sree},
  date         = {2019},
  journaltitle = {Procedia Computer Science},
  title        = {Object Detection In Underwater Acoustic Images Using Edge Based Segmentation Method},
  doi          = {10.1016/j.procs.2020.01.015},
  issn         = {1877-0509},
  pages        = {759--765},
  volume       = {165},
  file         = {:Priyadharsini_2019 - Object Detection in Underwater Acoustic Images Using Edge Based Segmentation Method.pdf:PDF},
  groups       = {Edge Detection},
  publisher    = {Elsevier BV},
}

@Article{Awalludin_2022,
  author       = {Awalludin, Ezmahamrul Afreen and Arsad, Tengku Noorfarahana T. and Yussof, Wan Nural Jawahir Hj Wan and Bachok, Zainudin and Hitam, Muhammad Suzuri},
  date         = {2022-03},
  journaltitle = {Journal of Telecommunictions and Information Technology},
  title        = {A Comparative Study of Various Edge Detection Techniques for Underwater Images},
  doi          = {10.26636/jtit.2022.155921},
  issn         = {1899-8852},
  number       = {2022},
  pages        = {23--33},
  volume       = {1},
  file         = {:Awalludin_2022 - A Comparative Study of Various Edge Detection Techniques for Underwater Images.pdf:PDF:https\://jtit.pl/jtit/article/download/432/432},
  groups       = {Edge Detection},
  publisher    = {National Institute of Telecommunications},
}

@Article{Komari_Alaie_2018,
  author       = {Komari Alaie, Hamed and Farsi, Hassan},
  date         = {2018-01},
  journaltitle = {Applied Sciences},
  title        = {Passive Sonar Target Detection Using Statistical Classifier and Adaptive Threshold},
  doi          = {10.3390/app8010061},
  issn         = {2076-3417},
  number       = {1},
  pages        = {61},
  volume       = {8},
  file         = {:Komari_Alaie_2018 - Passive Sonar Target Detection Using Statistical Classifier and Adaptive Threshold.pdf:PDF:https\://www.mdpi.com/2076-3417/8/1/61/pdf?version=1514991468},
  groups       = {Thresholding},
  publisher    = {MDPI AG},
}

@InProceedings{Aridgides_1995,
  author    = {Aridgides, Tom and Antoni, Diana and Fernandez, Manuel F. and Dobeck, Gerald J.},
  booktitle = {Detection Technologies for Mines and Minelike Targets},
  date      = {1995-06},
  title     = {Adaptive filter for mine detection and classification in side-scan sonar imagery},
  doi       = {10.1117/12.211345},
  editor    = {Dubey, Abinash C. and Cindrich, Ivan and Ralston, James M. and Rigano, Kelly A.},
  pages     = {475--486},
  publisher = {SPIE},
  volume    = {2496},
  file      = {:Aridgides_1995 - Adaptive Filter for Mine Detection and Classification in Side Scan Sonar Imagery.pdf:PDF},
  groups    = {Filtering},
  issn      = {0277-786X},
}

@InProceedings{Lourey_2017,
  author    = {Lourey, Simon},
  booktitle = {Proc. Underwater Acoust. Conf. Exhib.(UACE)},
  date      = {2017},
  title     = {Adaptive filtering for enhanced detection of continuous active sonar signals},
  pages     = {145--152},
  url       = {https://www.uaconferences.org/docs/2017_papers/153_UACE2017.pdf},
  file      = {:Lourey_2017 - Adaptive Filtering for Enhanced Detection of Continuous Active Sonar Signals.pdf:PDF:https\://www.uaconferences.org/docs/2017_papers/153_UACE2017.pdf},
  groups    = {Filtering},
}

@Article{Yuan_2016,
  author       = {Yuan, Xin and Martínez, José-Fernán and Eckert, Martina and López-Santidrián, Lourdes},
  date         = {2016-07},
  journaltitle = {Sensors},
  title        = {An Improved Otsu Threshold Segmentation Method for Underwater Simultaneous Localization and Mapping-Based Navigation},
  doi          = {10.3390/s16071148},
  issn         = {1424-8220},
  number       = {7},
  pages        = {1148},
  volume       = {16},
  file         = {:Yuan_2016 - An Improved Otsu Threshold Segmentation Method for Underwater Simultaneous Localization and Mapping Based Navigation.pdf:PDF:https\://www.mdpi.com/1424-8220/16/7/1148/pdf?version=1469417331},
  groups       = {Thresholding},
  publisher    = {MDPI AG},
}

@Article{Dimitrova_Grekow_2017,
  author       = {Dimitrova-Grekow, Teodora and Salauyou, Valery and Kowalski, Karol},
  date         = {2017},
  journaltitle = {Measurement Automation Monitoring},
  title        = {Indoor Mapping Using Sonar Sensor and Otsu Method},
  issn         = {2450-2855},
  number       = {6},
  pages        = {214--216},
  url          = {https://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-c104553b-2ef9-4d60-85b1-8d7623944a7d},
  volume       = {63},
  abstract     = {In this paper we present an indoor mapping algorithm based on sonar sensor. The overall object detection and mapping experiment is based on small scale local spatial information which has been accomplished in a 2D geometrical map. Considering all drawbacks and pluses of ultrasonic sensors, we present an innovative mapping approach, applying the Otsu’s method and Hit-or-Miss for sonar-data processing. The collected data are treated as a gray-scale picture. For its binarization, we applied the well-known for vision-based systems threshold calculation. Then also the morphology effect, what rises additionally the mapping accuracy, as is shown at the end of the paper. The robot is based on the education construction set LEGO Mindstorms EV3 intelligent brick on ev3dev - a Debian Linux-based operating system and Python 2.0 have been used for programming. The results are evaluated and compared with the real space.},
  file         = {:Dimitrova_Grekow_2017 - Indoor Mapping Using Sonar Sensor and Otsu Method.pdf:PDF},
  groups       = {Thresholding},
  keywords     = {mapping,sonar,Otsu's method,education robot},
}

@Article{Ding_2001,
  author       = {Ding, Lijun and Goshtasby, Ardeshir},
  date         = {2001-03},
  journaltitle = {Pattern Recognition},
  title        = {On the Canny edge detector},
  doi          = {10.1016/s0031-3203(00)00023-6},
  issn         = {0031-3203},
  number       = {3},
  pages        = {721--725},
  volume       = {34},
  file         = {:Ding_2001 - On the Canny Edge Detector.pdf:PDF},
  groups       = {Edge Detection},
  publisher    = {Elsevier BV},
}

@Article{Redmon_2016,
  author      = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date        = {2016-06},
  title       = {You Only Look Once: Unified, Real-Time Object Detection},
  doi         = {10.1109/cvpr.2016.91},
  eprint      = {1506.02640v5},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  pages       = {779--788},
  abstract    = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  booktitle   = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  file        = {:Redmon_2016 - You Only Look Once_ Unified, Real Time Object Detection.pdf:PDF:https\://arxiv.org/pdf/1506.02640},
  groups      = {YOLO},
  keywords    = {cs.CV},
  publisher   = {IEEE},
}

@Article{Terven_2023,
  author       = {Terven, Juan and Córdova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro},
  date         = {2023-04-02},
  journaltitle = {Machine Learning and Knowledge Extraction},
  title        = {A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS},
  doi          = {10.3390/make5040083},
  eprint       = {2304.00501},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  issn         = {2504-4990},
  number       = {4},
  pages        = {1680--1716},
  volume       = {5},
  abstract     = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with Transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
  copyright    = {Creative Commons Attribution 4.0 International},
  file         = {:Terven_2023 - A Comprehensive Review of YOLO Architectures in Computer Vision_ from YOLOv1 to YOLOv8 and YOLO NAS.pdf:PDF:http\://arxiv.org/pdf/2304.00501v7},
  groups       = {YOLO},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, I.2.10},
  month        = nov,
  publisher    = {MDPI AG},
  year         = {2023},
}

@Article{Jiang_2022,
  author       = {Jiang, Peiyuan and Ergu, Daji and Liu, Fangyao and Cai, Ying and Ma, Bo},
  date         = {2022},
  journaltitle = {Procedia Computer Science},
  title        = {A Review of Yolo Algorithm Developments},
  doi          = {10.1016/j.procs.2022.01.135},
  issn         = {1877-0509},
  pages        = {1066--1073},
  volume       = {199},
  file         = {:Jiang_2022 - A Review of Yolo Algorithm Developments.pdf:PDF},
  groups       = {YOLO},
  publisher    = {Elsevier BV},
}

@Article{Diwan_2022,
  author       = {Diwan, Tausif and Anirudh, G. and Tembhurne, Jitendra V.},
  date         = {2022-08},
  journaltitle = {Multimedia Tools and Applications},
  title        = {Object detection using YOLO: challenges, architectural successors, datasets and applications},
  doi          = {10.1007/s11042-022-13644-y},
  issn         = {1573-7721},
  number       = {6},
  pages        = {9243--9275},
  volume       = {82},
  file         = {:Diwan_2022 - Object Detection Using YOLO_ Challenges, Architectural Successors, Datasets and Applications.pdf:PDF:https\://link.springer.com/content/pdf/10.1007/s11042-022-13644-y.pdf},
  groups       = {YOLO},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Chen_2023,
  author       = {Chen, Chunling and Zheng, Ziyue and Xu, Tongyu and Guo, Shuang and Feng, Shuai and Yao, Weixiang and Lan, Yubin},
  date         = {2023-03},
  journaltitle = {Drones},
  title        = {YOLO-Based UAV Technology: A Review of the Research and Its Applications},
  doi          = {10.3390/drones7030190},
  issn         = {2504-446X},
  number       = {3},
  pages        = {190},
  volume       = {7},
  file         = {:Chen_2023 - YOLO Based UAV Technology_ a Review of the Research and Its Applications.pdf:PDF},
  groups       = {YOLO},
  publisher    = {MDPI AG},
}

@Article{Wang_2023,
  author       = {Wang, Hao and Xiao, Nanfeng},
  date         = {2023-02},
  journaltitle = {Applied Sciences},
  title        = {Underwater Object Detection Method Based on Improved Faster RCNN},
  doi          = {10.3390/app13042746},
  issn         = {2076-3417},
  number       = {4},
  pages        = {2746},
  volume       = {13},
  file         = {:Wang_2023 - Underwater Object Detection Method Based on Improved Faster RCNN.pdf:PDF},
  groups       = {Faster R-CNN},
  publisher    = {MDPI AG},
}

@Article{Ren_2015,
  author      = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date        = {2015-06-04},
  title       = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  doi         = {10.48550/ARXIV.1506.01497},
  eprint      = {1506.01497},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Ren_2015 - Faster R CNN_ Towards Real Time Object Detection with Region Proposal Networks.pdf:PDF:http\://arxiv.org/pdf/1506.01497v3},
  groups      = {Faster R-CNN},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2015},
}

@Article{Zeng_2021,
  author       = {Zeng, Lingcai and Sun, Bing and Zhu, Daqi},
  date         = {2021-04},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  title        = {Underwater target detection based on Faster R-CNN and adversarial occlusion network},
  doi          = {10.1016/j.engappai.2021.104190},
  issn         = {0952-1976},
  pages        = {104190},
  volume       = {100},
  file         = {:Zeng_2021 - Underwater Target Detection Based on Faster R CNN and Adversarial Occlusion Network.pdf:PDF},
  groups       = {Faster R-CNN},
  publisher    = {Elsevier BV},
}

@InProceedings{Yulin_2020,
  author    = {Yulin, Tang and Shaohua, Jin and Gang, Bian and Yonzhou, Zhang and Fan, Li},
  booktitle = {2020 International Conference on Big Data &amp; Artificial Intelligence &amp; Software Engineering (ICBASE)},
  date      = {2020-10},
  title     = {Wreckage Target Recognition in Side-scan Sonar Images Based on an Improved Faster R-CNN Model},
  doi       = {10.1109/icbase51474.2020.00080},
  pages     = {348--354},
  publisher = {IEEE},
  file      = {:Yulin_2020 - Wreckage Target Recognition in Side Scan Sonar Images Based on an Improved Faster R CNN Model.pdf:PDF},
  groups    = {Faster R-CNN},
}

@Article{Liu_2016,
  author      = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  date        = {2015-12-08},
  title       = {SSD: Single Shot MultiBox Detector},
  doi         = {10.1007/978-3-319-46448-0_2},
  eprint      = {1512.02325},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  issn        = {1611-3349},
  pages       = {21--37},
  abstract    = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  booktitle   = {Computer Vision – ECCV 2016},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Liu_2016 - SSD_ Single Shot MultiBox Detector.pdf:PDF:http\://arxiv.org/pdf/1512.02325v5},
  groups      = {SSD},
  isbn        = {9783319464480},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  publisher   = {Springer International Publishing},
  year        = {2015},
}

@Article{Kumar_2020,
  author       = {Kumar, Ashwani and Srivastava, Sonam},
  date         = {2020},
  journaltitle = {Procedia Computer Science},
  title        = {Object Detection System Based on Convolution Neural Networks Using Single Shot Multi-Box Detector},
  doi          = {10.1016/j.procs.2020.04.283},
  issn         = {1877-0509},
  pages        = {2610--2617},
  volume       = {171},
  file         = {:Kumar_2020 - Object Detection System Based on Convolution Neural Networks Using Single Shot Multi Box Detector.pdf:PDF},
  groups       = {SSD},
  publisher    = {Elsevier BV},
}

@Article{Ma_2020,
  author       = {Ma, Wen and Wang, Xiao and Yu, Jiong},
  date         = {2020},
  journaltitle = {IEEE Access},
  title        = {A Lightweight Feature Fusion Single Shot Multibox Detector for Garbage Detection},
  doi          = {10.1109/access.2020.3031990},
  issn         = {2169-3536},
  pages        = {188577--188586},
  volume       = {8},
  file         = {:Ma_2020 - A Lightweight Feature Fusion Single Shot Multibox Detector for Garbage Detection.pdf:PDF},
  groups       = {SSD},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@InProceedings{Jiang_2020,
  author     = {Jiang, Zhongyun and Wang, Rongrong},
  booktitle  = {2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
  date       = {2020-12},
  title      = {Underwater Object Detection Based on Improved Single Shot MultiBox Detector},
  doi        = {10.1145/3446132.3446170},
  pages      = {1--7},
  publisher  = {ACM},
  series     = {ACAI 2020},
  collection = {ACAI 2020},
  file       = {:Jiang_2020 - Underwater Object Detection Based on Improved Single Shot MultiBox Detector.pdf:PDF},
  groups     = {SSD},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Data\;0\;1\;0x60bfa2ff\;\;\;;
2 StaticGroup:Datasets\;0\;0\;0x56ac92ff\;\;\;;
2 StaticGroup:Dataset Papers\;0\;1\;0xac5671ff\;\;\;;
1 StaticGroup:Deep Learning\;0\;1\;0x7260bfff\;\;\;;
2 StaticGroup:Computer Vision\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Object Detection\;0\;1\;0xbf607dff\;\;\;;
4 StaticGroup:Supervised Learning\;0\;1\;0x60b0bfff\;\;\;;
5 StaticGroup:YOLO\;0\;1\;0x569eacff\;\;\;;
5 StaticGroup:Faster R-CNN\;0\;1\;0xac6456ff\;\;\;;
5 StaticGroup:SSD\;0\;1\;0x81ac56ff\;\;\;;
3 StaticGroup:Filtering\;0\;1\;0x60bfa2ff\;\;\;;
3 StaticGroup:Thresholding\;0\;0\;0x7260bfff\;\;\;;
3 StaticGroup:Edge Detection\;0\;0\;0xadbf60ff\;\;\;;
1 StaticGroup:Specs & Formats\;0\;1\;0xbf8a60ff\;\;\;;
}
