% Encoding: UTF-8

@Article{Aubard_2024_Datasets,
  author      = {Aubard, Martin and Madureira, Ana and Teixeira, Luís and Pinto, José},
  date        = {2024-12-16},
  title       = {Sonar-based Deep Learning in Underwater Robotics: Overview, Robustness and Challenges},
  doi         = {10.48550/ARXIV.2412.11840},
  eprint      = {2412.11840},
  eprintclass = {cs.RO},
  eprinttype  = {arXiv},
  abstract    = {With the growing interest in underwater exploration and monitoring, Autonomous Underwater Vehicles (AUVs) have become essential. The recent interest in onboard Deep Learning (DL) has advanced real-time environmental interaction capabilities relying on efficient and accurate vision-based DL models. However, the predominant use of sonar in underwater environments, characterized by limited training data and inherent noise, poses challenges to model robustness. This autonomy improvement raises safety concerns for deploying such models during underwater operations, potentially leading to hazardous situations. This paper aims to provide the first comprehensive overview of sonar-based DL under the scope of robustness. It studies sonar-based DL perception task models, such as classification, object detection, segmentation, and SLAM. Furthermore, the paper systematizes sonar-based state-of-the-art datasets, simulators, and robustness methods such as neural network verification, out-of-distribution, and adversarial attacks. This paper highlights the lack of robustness in sonar-based DL research and suggests future research pathways, notably establishing a baseline sonar-based dataset and bridging the simulation-to-reality gap.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Aubard_2024_Datasets - Sonar Based Deep Learning in Underwater Robotics_ Overview, Robustness and Challenges.pdf:PDF:http\://arxiv.org/pdf/2412.11840v1},
  groups      = {Data},
  keywords    = {Robotics (cs.RO), Computer Vision and Pattern Recognition (cs.CV), Signal Processing (eess.SP), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  publisher   = {arXiv},
  year        = {2024},
}

@Article{Pessanha_Santos_2024,
  author       = {Pessanha Santos, Nuno and Moura, Ricardo and Sampaio Torgal, Gonçalo and Lobo, Victor and Neto, Miguel de Castro},
  date         = {2024-04},
  journaltitle = {Data in Brief},
  title        = {Side-scan sonar imaging data of underwater vehicles for mine detection},
  doi          = {10.1016/j.dib.2024.110132},
  issn         = {2352-3409},
  pages        = {110132},
  volume       = {53},
  file         = {:Pessanha_Santos_2024 - Side Scan Sonar Imaging Data of Underwater Vehicles for Mine Detection.pdf:PDF},
  groups       = {Dataset Papers},
  publisher    = {Elsevier BV},
}

@Misc{Dahn_2024_UXO,
  author    = {Dahn, Nikolas and Bande Firvida, Miguel and Sharma, Proneet and Mohrmann, Jochen and Geisler, Oliver and Sanghamreddy, Prithvi Kumar and Marquardt, Kevin and Christensen, Leif},
  date      = {2024},
  title     = {An Acoustic and Optical Dataset for the Perception of Underwater Unexploded Ordnance (UXO)},
  doi       = {10.5281/ZENODO.11068045},
  copyright = {BSD 3-Clause "New" or "Revised" License},
  groups    = {Datasets},
  keywords  = {Unexploded Ordnance, UXO, Environment Pollution, Imaging Sonar},
  publisher = {Zenodo},
}

@Article{Xie_2022,
  author       = {Xie, Kaibing and Yang, Jian and Qiu, Kang},
  date         = {2022-12},
  journaltitle = {Scientific Data},
  title        = {A Dataset with Multibeam Forward-Looking Sonar for Underwater Object Detection},
  doi          = {10.1038/s41597-022-01854-w},
  issn         = {2052-4463},
  number       = {1},
  volume       = {9},
  file         = {:Xie_2022 - A Dataset with Multibeam Forward Looking Sonar for Underwater Object Detection.pdf:PDF:https\://www.nature.com/articles/s41597-022-01854-w.pdf},
  groups       = {Dataset Papers},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Murray_Rust_2008,
  author       = {Murray-Rust, Peter},
  date         = {2008-01},
  journaltitle = {Nature Precedings},
  title        = {Open Data in Science},
  doi          = {10.1038/npre.2008.1526.1},
  issn         = {1756-0357},
  file         = {:Murray_Rust_2008 - Open Data in Science.pdf:PDF:https\://www.nature.com/articles/npre.2008.1526.1.pdf},
  groups       = {Data},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Aubard_2024_ROSAR,
  author      = {Aubard, Martin and Antal, László and Madureira, Ana and Teixeira, Luis F. and Ábrahám, Erika},
  date        = {2024-10-14},
  title       = {ROSAR: An Adversarial Re-Training Framework for Robust Side-Scan Sonar Object Detection},
  doi         = {10.48550/ARXIV.2410.10554},
  eprint      = {2410.10554},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {This paper introduces ROSAR, a novel framework enhancing the robustness of deep learning object detection models tailored for side-scan sonar (SSS) images, generated by autonomous underwater vehicles using sonar sensors. By extending our prior work on knowledge distillation (KD), this framework integrates KD with adversarial retraining to address the dual challenges of model efficiency and robustness against SSS noises. We introduce three novel, publicly available SSS datasets, capturing different sonar setups and noise conditions. We propose and formalize two SSS safety properties and utilize them to generate adversarial datasets for retraining. Through a comparative analysis of projected gradient descent (PGD) and patch-based adversarial attacks, ROSAR demonstrates significant improvements in model robustness and detection accuracy under SSS-specific conditions, enhancing the model's robustness by up to 1.85\%. ROSAR is available at https://github.com/remaro-network/ROSAR-framework.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Aubard_2024_ROSAR - ROSAR_ an Adversarial Re Training Framework for Robust Side Scan Sonar Object Detection.pdf:PDF:http\://arxiv.org/pdf/2410.10554v1},
  groups      = {Dataset Papers},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Robotics (cs.RO), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2024},
}

@Misc{Aubard_2024_SWDD,
  author    = {Aubard, Martin and Antal, László and Madureira, Maria and F. Teixeira, Luis and Ábrahám, Erika},
  date      = {2024},
  title     = {SWDD: Sonar Wall Detection Dataset},
  doi       = {10.5281/ZENODO.10528134},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  publisher = {Zenodo},
}

@Article{Alvarez_Tunon_2024,
  author      = {Álvarez-Tuñón, Olaya and Marnet, Luiza Ribeiro and Antal, László and Aubard, Martin and Costa, Maria and Brodskiy, Yury},
  date        = {2024-01-31},
  title       = {SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization},
  doi         = {10.48550/ARXIV.2401.17907},
  eprint      = {2401.17907},
  eprintclass = {cs.RO},
  eprinttype  = {arXiv},
  abstract    = {This paper presents SubPipe, an underwater dataset for SLAM, object detection, and image segmentation. SubPipe has been recorded using a \gls{LAUV}, operated by OceanScan MST, and carrying a sensor suite including two cameras, a side-scan sonar, and an inertial navigation system, among other sensors. The AUV has been deployed in a pipeline inspection environment with a submarine pipe partially covered by sand. The AUV's pose ground truth is estimated from the navigation sensors. The side-scan sonar and RGB images include object detection and segmentation annotations, respectively. State-of-the-art segmentation, object detection, and SLAM methods are benchmarked on SubPipe to demonstrate the dataset's challenges and opportunities for leveraging computer vision algorithms. To the authors' knowledge, this is the first annotated underwater dataset providing a real pipeline inspection scenario. The dataset and experiments are publicly available online at https://github.com/remaro-network/SubPipe-dataset},
  copyright   = {Creative Commons Attribution Share Alike 4.0 International},
  file        = {:Alvarez_Tunon_2024 - SubPipe_ a Submarine Pipeline Inspection Dataset for Segmentation and Visual Inertial Localization.pdf:PDF:http\://arxiv.org/pdf/2401.17907v2},
  groups      = {Dataset Papers},
  keywords    = {Robotics (cs.RO), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2024},
}

@Misc{Jian_2022,
  author    = {Jian, Yang and Kaibing, Xie},
  date      = {2022},
  title     = {Underwater acoustic target detection (UATD) dataset},
  doi       = {10.6084/M9.FIGSHARE.21331143.V3},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  keywords  = {91103 Ocean Engineering, FOS: Environmental engineering},
  publisher = {figshare},
}

@Article{Bourke_1998,
  author       = {Bourke, Paul},
  date         = {1998},
  journaltitle = {BMP Files. July},
  title        = {Bmp image format},
  volume       = {8},
  file         = {:Bourke_1998 - Bmp Image Format.pdf:PDF},
  groups       = {Data},
}

@Article{Everingham_2009,
  author       = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  date         = {2009-09},
  journaltitle = {International Journal of Computer Vision},
  title        = {The Pascal Visual Object Classes (VOC) Challenge},
  doi          = {10.1007/s11263-009-0275-4},
  issn         = {1573-1405},
  number       = {2},
  pages        = {303--338},
  volume       = {88},
  file         = {:Everingham_2009 - The Pascal Visual Object Classes (VOC) Challenge.pdf:PDF:https\://www.pure.ed.ac.uk/ws/files/7879113/ijcv_voc09.pdf},
  groups       = {Data},
  publisher    = {Springer Science and Business Media LLC},
}

@Misc{Pessanha_Santos_2024_SSSFMD,
  author    = {Pessanha Santos, Nuno and Moura, Ricardo},
  date      = {2024},
  title     = {Side-scan sonar imaging for Mine detection},
  doi       = {10.6084/M9.FIGSHARE.24574879},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  keywords  = {Ocean engineering, Field robotics, Autonomous vehicle systems},
  publisher = {figshare},
}

@Book{Geron_2023,
  author    = {Géron, Aurélien},
  date      = {2023},
  title     = {Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow},
  edition   = {Third edition},
  isbn      = {9781098125974},
  location  = {Beijing},
  note      = {Literaturangaben. - Index},
  pagetotal = {834},
  publisher = {O'Reilly},
  series    = {Data science / machine learning},
  subtitle  = {Concepts, tools, and techniques to build intelligent systems},
  file      = {:Geron_2023 - Hands on Machine Learning with Scikit Learn, Keras, and TensorFlow.pdf:PDF},
  groups    = {Deep Learning},
  ppn_gvk   = {1799537536},
}

@InProceedings{Dahn_2024,
  author    = {Dahn, Nikolas and Firvida, Miguel Bande and Sharma, Proneet and Christensen, Leif and Geisle, Oliver and Mohrmann, Jochen and Frey, Torsten and Kumar Sanghamreddy, Prithvi and Kirchner, Frank},
  booktitle = {OCEANS 2024 - Halifax},
  date      = {2024-09},
  title     = {An Acoustic and Optical Dataset for the Perception of Underwater Unexploded Ordnance (UXO)},
  doi       = {10.1109/oceans55160.2024.10754316},
  pages     = {1--6},
  publisher = {IEEE},
  file      = {:Dahn_2024 - An Acoustic and Optical Dataset for the Perception of Underwater Unexploded Ordnance (UXO).pdf:PDF},
  groups    = {Dataset Papers},
}

@Misc{Alvarez_Tunon_2024_SubPipe,
  author    = {Álvarez-Tuñón, Olaya and Ribeiro Marnet, Luiza and Antal, László and Aubard, Martin and Costa, Maria and Brodskiy, Yury},
  date      = {2024},
  title     = {SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization},
  doi       = {10.5281/ZENODO.10053564},
  language  = {en},
  copyright = {Creative Commons Attribution 4.0 International},
  groups    = {Datasets},
  keywords  = {underwater dataset, pipeline, RGB and grayscale camera, side-scan sonar, SLAM, object detection, semantic segmentation},
  publisher = {Zenodo},
}

@WWW{Poskanzer_2016,
  author = {Jef Poskanzer},
  date   = {2016-10-09},
  title  = {pgm - Netpbm grayscale image format},
  url    = {https://netpbm.sourceforge.net/doc/pgm.html},
  file   = {:Poskanzer_2016 - Pgm Netpbm Grayscale Image Format.pdf:PDF},
  groups = {Specs & Formats},
}

@Book{Goodfellow_2016,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date      = {2016},
  title     = {Deep learning},
  editor    = {Aaron Courville and Yoshua Bengio},
  isbn      = {9780262337373},
  location  = {Cambridge, Massachusetts},
  note      = {Includes bibliographical references and index},
  pagetotal = {1775},
  publisher = {The MIT Press},
  series    = {Adaptive computation and machine learning},
  file      = {:Goodfellow_2016 - Deep Learning.pdf:PDF},
  groups    = {Deep Learning},
  ppn_gvk   = {1789979978},
}

@Article{Torre_1986,
  author       = {Torre, Vincent and Poggio, Tomaso A.},
  date         = {1986-03},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {On Edge Detection},
  doi          = {10.1109/tpami.1986.4767769},
  issn         = {0162-8828},
  number       = {2},
  pages        = {147--163},
  volume       = {PAMI-8},
  file         = {:Torre_1986 - On Edge Detection.pdf:PDF},
  groups       = {Edge Detection},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Wang_2024,
  author       = {Wang, Wenling and Zhang, Qiaoxin and Qi, Zhisheng and Huang, Mengxing},
  date         = {2024-01},
  journaltitle = {Sensors},
  title        = {CenterNet-Saccade: Enhancing Sonar Object Detection with Lightweight Global Feature Extraction},
  doi          = {10.3390/s24020665},
  issn         = {1424-8220},
  number       = {2},
  pages        = {665},
  volume       = {24},
  file         = {:Wang_2024 - CenterNet Saccade_ Enhancing Sonar Object Detection with Lightweight Global Feature Extraction.pdf:PDF:https\://www.mdpi.com/1424-8220/24/2/665/pdf?version=1705744354},
  groups       = {Object Detection},
  publisher    = {MDPI AG},
}

@Article{Labbe_Morissette_2019,
  author      = {Labbe-Morissette, Guillaume and Gauthier, Sylvain},
  date        = {2019-09-17},
  title       = {A machine vision meta-algorithm for automated recognition of underwater objects using sidescan sonar imagery},
  doi         = {10.48550/ARXIV.1909.07763},
  eprint      = {1909.07763},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {This paper details a new method to recognize and detect underwater objects in real-time sidescan sonar data imagery streams, with case-studies of applications for underwater archeology, and ghost fishing gear retrieval. We first synthesize images from sidescan data, apply geometric and radiometric corrections, then use 2D feature detection algorithms to identify point clouds of descriptive visual microfeatures such as corners and edges in the sonar images. We then apply a clustering algorithm on the feature point clouds to group feature sets into regions of interest, reject false positives, yielding a georeferenced inventory of objects.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Labbe_Morissette_2019 - A Machine Vision Meta Algorithm for Automated Recognition of Underwater Objects Using Sidescan Sonar Imagery.pdf:PDF:http\://arxiv.org/pdf/1909.07763v1},
  groups      = {Object Detection},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  publisher   = {arXiv},
  year        = {2019},
}

@Article{Valdenegro_Toro_2019,
  author      = {Valdenegro-Toro, Matias},
  date        = {2019-07-01},
  title       = {Learning Objectness from Sonar Images for Class-Independent Object Detection},
  doi         = {10.48550/ARXIV.1907.00734},
  eprint      = {1907.00734},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Detecting novel objects without class information is not trivial, as it is difficult to generalize from a small training set. This is an interesting problem for underwater robotics, as modeling marine objects is inherently more difficult in sonar images, and training data might not be available apriori. Detection proposals algorithms can be used for this purpose but usually requires a large amount of output bounding boxes. In this paper we propose the use of a fully convolutional neural network that regresses an objectness value directly from a Forward-Looking sonar image. By ranking objectness, we can produce high recall (96 \%) with only 100 proposals per image. In comparison, EdgeBoxes requires 5000 proposals to achieve a slightly better recall of 97 \%, while Selective Search requires 2000 proposals to achieve 95 \% recall. We also show that our method outperforms a template matching baseline by a considerable margin, and is able to generalize to completely new objects. We expect that this kind of technique can be used in the field to find lost objects under the sea.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Valdenegro_Toro_2019 - Learning Objectness from Sonar Images for Class Independent Object Detection.pdf:PDF:http\://arxiv.org/pdf/1907.00734v1},
  groups      = {Object Detection},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Robotics (cs.RO), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  publisher   = {arXiv},
  year        = {2019},
}

@Article{Valdenegro_Toro_2017,
  author       = {Valdenegro-Toro, Matias},
  date         = {2017-09-08},
  journaltitle = {Proceedings of ANNPR 2016},
  title        = {Objectness Scoring and Detection Proposals in Forward-Looking Sonar Images with Convolutional Neural Networks},
  doi          = {10.1007/978-3-319-46182-3_18},
  eprint       = {1709.02600},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  issn         = {1611-3349},
  pages        = {209--219},
  abstract     = {Forward-looking sonar can capture high resolution images of underwater scenes, but their interpretation is complex. Generic object detection in such images has not been solved, specially in cases of small and unknown objects. In comparison, detection proposal algorithms have produced top performing object detectors in real-world color images. In this work we develop a Convolutional Neural Network that can reliably score objectness of image windows in forward-looking sonar images and by thresholding objectness, we generate detection proposals. In our dataset of marine garbage objects, we obtain 94\% recall, generating around 60 proposals per image. The biggest strength of our method is that it can generalize to previously unseen objects. We show this by detecting chain links, walls and a wrench without previous training in such objects. We strongly believe our method can be used for class-independent object detection, with many real-world applications such as chain following and mine detection.},
  booktitle    = {Artificial Neural Networks in Pattern Recognition},
  copyright    = {arXiv.org perpetual, non-exclusive license},
  file         = {:Valdenegro_Toro_2017 - Objectness Scoring and Detection Proposals in Forward Looking Sonar Images with Convolutional Neural Networks.pdf:PDF:http\://arxiv.org/pdf/1709.02600v1},
  groups       = {Object Detection},
  isbn         = {9783319461823},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences},
  publisher    = {Springer International Publishing},
  year         = {2017},
}

@Article{Otsu_1979,
  author       = {Otsu, Nobuyuki},
  date         = {1979-01},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  title        = {A Threshold Selection Method from Gray-Level Histograms},
  doi          = {10.1109/tsmc.1979.4310076},
  issn         = {2168-2909},
  number       = {1},
  pages        = {62--66},
  volume       = {9},
  file         = {:Otsu_1979 - A Threshold Selection Method from Gray Level Histograms.pdf:PDF},
  groups       = {Thresholding},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Priyadharsini_2019,
  author       = {Priyadharsini, R. and Sharmila, T. Sree},
  date         = {2019},
  journaltitle = {Procedia Computer Science},
  title        = {Object Detection In Underwater Acoustic Images Using Edge Based Segmentation Method},
  doi          = {10.1016/j.procs.2020.01.015},
  issn         = {1877-0509},
  pages        = {759--765},
  volume       = {165},
  file         = {:Priyadharsini_2019 - Object Detection in Underwater Acoustic Images Using Edge Based Segmentation Method.pdf:PDF},
  groups       = {Edge Detection},
  publisher    = {Elsevier BV},
}

@Article{Awalludin_2022,
  author       = {Awalludin, Ezmahamrul Afreen and Arsad, Tengku Noorfarahana T. and Yussof, Wan Nural Jawahir Hj Wan and Bachok, Zainudin and Hitam, Muhammad Suzuri},
  date         = {2022-03},
  journaltitle = {Journal of Telecommunictions and Information Technology},
  title        = {A Comparative Study of Various Edge Detection Techniques for Underwater Images},
  doi          = {10.26636/jtit.2022.155921},
  issn         = {1899-8852},
  number       = {2022},
  pages        = {23--33},
  volume       = {1},
  file         = {:Awalludin_2022 - A Comparative Study of Various Edge Detection Techniques for Underwater Images.pdf:PDF:https\://jtit.pl/jtit/article/download/432/432},
  groups       = {Edge Detection},
  publisher    = {National Institute of Telecommunications},
}

@Article{Komari_Alaie_2018,
  author       = {Komari Alaie, Hamed and Farsi, Hassan},
  date         = {2018-01},
  journaltitle = {Applied Sciences},
  title        = {Passive Sonar Target Detection Using Statistical Classifier and Adaptive Threshold},
  doi          = {10.3390/app8010061},
  issn         = {2076-3417},
  number       = {1},
  pages        = {61},
  volume       = {8},
  file         = {:Komari_Alaie_2018 - Passive Sonar Target Detection Using Statistical Classifier and Adaptive Threshold.pdf:PDF:https\://www.mdpi.com/2076-3417/8/1/61/pdf?version=1514991468},
  groups       = {Thresholding},
  publisher    = {MDPI AG},
}

@InProceedings{Aridgides_1995,
  author    = {Aridgides, Tom and Antoni, Diana and Fernandez, Manuel F. and Dobeck, Gerald J.},
  booktitle = {Detection Technologies for Mines and Minelike Targets},
  date      = {1995-06},
  title     = {Adaptive filter for mine detection and classification in side-scan sonar imagery},
  doi       = {10.1117/12.211345},
  editor    = {Dubey, Abinash C. and Cindrich, Ivan and Ralston, James M. and Rigano, Kelly A.},
  pages     = {475--486},
  publisher = {SPIE},
  volume    = {2496},
  file      = {:Aridgides_1995 - Adaptive Filter for Mine Detection and Classification in Side Scan Sonar Imagery.pdf:PDF},
  groups    = {Filtering},
  issn      = {0277-786X},
}

@InProceedings{Lourey_2017,
  author    = {Lourey, Simon},
  booktitle = {Proc. Underwater Acoust. Conf. Exhib.(UACE)},
  date      = {2017},
  title     = {Adaptive filtering for enhanced detection of continuous active sonar signals},
  pages     = {145--152},
  url       = {https://www.uaconferences.org/docs/2017_papers/153_UACE2017.pdf},
  file      = {:Lourey_2017 - Adaptive Filtering for Enhanced Detection of Continuous Active Sonar Signals.pdf:PDF:https\://www.uaconferences.org/docs/2017_papers/153_UACE2017.pdf},
  groups    = {Filtering},
}

@Article{Yuan_2016,
  author       = {Yuan, Xin and Martínez, José-Fernán and Eckert, Martina and López-Santidrián, Lourdes},
  date         = {2016-07},
  journaltitle = {Sensors},
  title        = {An Improved Otsu Threshold Segmentation Method for Underwater Simultaneous Localization and Mapping-Based Navigation},
  doi          = {10.3390/s16071148},
  issn         = {1424-8220},
  number       = {7},
  pages        = {1148},
  volume       = {16},
  file         = {:Yuan_2016 - An Improved Otsu Threshold Segmentation Method for Underwater Simultaneous Localization and Mapping Based Navigation.pdf:PDF:https\://www.mdpi.com/1424-8220/16/7/1148/pdf?version=1469417331},
  groups       = {Thresholding},
  publisher    = {MDPI AG},
}

@Article{Dimitrova_Grekow_2017,
  author       = {Dimitrova-Grekow, Teodora and Salauyou, Valery and Kowalski, Karol},
  date         = {2017},
  journaltitle = {Measurement Automation Monitoring},
  title        = {Indoor Mapping Using Sonar Sensor and Otsu Method},
  issn         = {2450-2855},
  number       = {6},
  pages        = {214--216},
  url          = {https://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-c104553b-2ef9-4d60-85b1-8d7623944a7d},
  volume       = {63},
  abstract     = {In this paper we present an indoor mapping algorithm based on sonar sensor. The overall object detection and mapping experiment is based on small scale local spatial information which has been accomplished in a 2D geometrical map. Considering all drawbacks and pluses of ultrasonic sensors, we present an innovative mapping approach, applying the Otsu’s method and Hit-or-Miss for sonar-data processing. The collected data are treated as a gray-scale picture. For its binarization, we applied the well-known for vision-based systems threshold calculation. Then also the morphology effect, what rises additionally the mapping accuracy, as is shown at the end of the paper. The robot is based on the education construction set LEGO Mindstorms EV3 intelligent brick on ev3dev - a Debian Linux-based operating system and Python 2.0 have been used for programming. The results are evaluated and compared with the real space.},
  file         = {:Dimitrova_Grekow_2017 - Indoor Mapping Using Sonar Sensor and Otsu Method.pdf:PDF},
  groups       = {Thresholding},
  keywords     = {mapping,sonar,Otsu's method,education robot},
}

@Article{Ding_2001,
  author       = {Ding, Lijun and Goshtasby, Ardeshir},
  date         = {2001-03},
  journaltitle = {Pattern Recognition},
  title        = {On the Canny edge detector},
  doi          = {10.1016/s0031-3203(00)00023-6},
  issn         = {0031-3203},
  number       = {3},
  pages        = {721--725},
  volume       = {34},
  file         = {:Ding_2001 - On the Canny Edge Detector.pdf:PDF},
  groups       = {Edge Detection},
  publisher    = {Elsevier BV},
}

@Article{Redmon_2016,
  author      = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date        = {2016-06},
  title       = {You Only Look Once: Unified, Real-Time Object Detection},
  doi         = {10.1109/cvpr.2016.91},
  eprint      = {1506.02640v5},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  pages       = {779--788},
  abstract    = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  booktitle   = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  file        = {:Redmon_2016 - You Only Look Once_ Unified, Real Time Object Detection.pdf:PDF:https\://arxiv.org/pdf/1506.02640},
  groups      = {YOLO},
  keywords    = {cs.CV},
  publisher   = {IEEE},
}

@Article{Terven_2023,
  author       = {Terven, Juan and Córdova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro},
  date         = {2023-04-02},
  journaltitle = {Machine Learning and Knowledge Extraction},
  title        = {A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS},
  doi          = {10.3390/make5040083},
  eprint       = {2304.00501},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  issn         = {2504-4990},
  number       = {4},
  pages        = {1680--1716},
  volume       = {5},
  abstract     = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with Transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
  copyright    = {Creative Commons Attribution 4.0 International},
  file         = {:Terven_2023 - A Comprehensive Review of YOLO Architectures in Computer Vision_ from YOLOv1 to YOLOv8 and YOLO NAS.pdf:PDF:http\://arxiv.org/pdf/2304.00501v7},
  groups       = {YOLO},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, I.2.10},
  month        = nov,
  publisher    = {MDPI AG},
  year         = {2023},
}

@Article{Jiang_2022,
  author       = {Jiang, Peiyuan and Ergu, Daji and Liu, Fangyao and Cai, Ying and Ma, Bo},
  date         = {2022},
  journaltitle = {Procedia Computer Science},
  title        = {A Review of Yolo Algorithm Developments},
  doi          = {10.1016/j.procs.2022.01.135},
  issn         = {1877-0509},
  pages        = {1066--1073},
  volume       = {199},
  file         = {:Jiang_2022 - A Review of Yolo Algorithm Developments.pdf:PDF},
  groups       = {YOLO},
  publisher    = {Elsevier BV},
}

@Article{Diwan_2022,
  author       = {Diwan, Tausif and Anirudh, G. and Tembhurne, Jitendra V.},
  date         = {2022-08},
  journaltitle = {Multimedia Tools and Applications},
  title        = {Object detection using YOLO: challenges, architectural successors, datasets and applications},
  doi          = {10.1007/s11042-022-13644-y},
  issn         = {1573-7721},
  number       = {6},
  pages        = {9243--9275},
  volume       = {82},
  file         = {:Diwan_2022 - Object Detection Using YOLO_ Challenges, Architectural Successors, Datasets and Applications.pdf:PDF:https\://link.springer.com/content/pdf/10.1007/s11042-022-13644-y.pdf},
  groups       = {YOLO},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Chen_2023,
  author       = {Chen, Chunling and Zheng, Ziyue and Xu, Tongyu and Guo, Shuang and Feng, Shuai and Yao, Weixiang and Lan, Yubin},
  date         = {2023-03},
  journaltitle = {Drones},
  title        = {YOLO-Based UAV Technology: A Review of the Research and Its Applications},
  doi          = {10.3390/drones7030190},
  issn         = {2504-446X},
  number       = {3},
  pages        = {190},
  volume       = {7},
  file         = {:Chen_2023 - YOLO Based UAV Technology_ a Review of the Research and Its Applications.pdf:PDF},
  groups       = {YOLO},
  publisher    = {MDPI AG},
}

@Article{Wang_2023,
  author       = {Wang, Hao and Xiao, Nanfeng},
  date         = {2023-02},
  journaltitle = {Applied Sciences},
  title        = {Underwater Object Detection Method Based on Improved Faster RCNN},
  doi          = {10.3390/app13042746},
  issn         = {2076-3417},
  number       = {4},
  pages        = {2746},
  volume       = {13},
  file         = {:Wang_2023 - Underwater Object Detection Method Based on Improved Faster RCNN.pdf:PDF},
  groups       = {Faster R-CNN},
  publisher    = {MDPI AG},
}

@Article{Ren_2015,
  author      = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date        = {2015-06-04},
  title       = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  doi         = {10.48550/ARXIV.1506.01497},
  eprint      = {1506.01497},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Ren_2015 - Faster R CNN_ Towards Real Time Object Detection with Region Proposal Networks.pdf:PDF:http\://arxiv.org/pdf/1506.01497v3},
  groups      = {Faster R-CNN},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2015},
}

@Article{Zeng_2021,
  author       = {Zeng, Lingcai and Sun, Bing and Zhu, Daqi},
  date         = {2021-04},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  title        = {Underwater target detection based on Faster R-CNN and adversarial occlusion network},
  doi          = {10.1016/j.engappai.2021.104190},
  issn         = {0952-1976},
  pages        = {104190},
  volume       = {100},
  file         = {:Zeng_2021 - Underwater Target Detection Based on Faster R CNN and Adversarial Occlusion Network.pdf:PDF},
  groups       = {Faster R-CNN},
  publisher    = {Elsevier BV},
}

@InProceedings{Yulin_2020,
  author    = {Yulin, Tang and Shaohua, Jin and Gang, Bian and Yonzhou, Zhang and Fan, Li},
  booktitle = {2020 International Conference on Big Data &amp; Artificial Intelligence &amp; Software Engineering (ICBASE)},
  date      = {2020-10},
  title     = {Wreckage Target Recognition in Side-scan Sonar Images Based on an Improved Faster R-CNN Model},
  doi       = {10.1109/icbase51474.2020.00080},
  pages     = {348--354},
  publisher = {IEEE},
  file      = {:Yulin_2020 - Wreckage Target Recognition in Side Scan Sonar Images Based on an Improved Faster R CNN Model.pdf:PDF},
  groups    = {Faster R-CNN},
}

@Article{Liu_2016,
  author      = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  date        = {2015-12-08},
  title       = {SSD: Single Shot MultiBox Detector},
  doi         = {10.1007/978-3-319-46448-0_2},
  eprint      = {1512.02325},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  issn        = {1611-3349},
  pages       = {21--37},
  abstract    = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\times 300$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\times 500$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  booktitle   = {Computer Vision – ECCV 2016},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Liu_2016 - SSD_ Single Shot MultiBox Detector.pdf:PDF:http\://arxiv.org/pdf/1512.02325v5},
  groups      = {SSD},
  isbn        = {9783319464480},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  publisher   = {Springer International Publishing},
  year        = {2015},
}

@Article{Kumar_2020,
  author       = {Kumar, Ashwani and Srivastava, Sonam},
  date         = {2020},
  journaltitle = {Procedia Computer Science},
  title        = {Object Detection System Based on Convolution Neural Networks Using Single Shot Multi-Box Detector},
  doi          = {10.1016/j.procs.2020.04.283},
  issn         = {1877-0509},
  pages        = {2610--2617},
  volume       = {171},
  file         = {:Kumar_2020 - Object Detection System Based on Convolution Neural Networks Using Single Shot Multi Box Detector.pdf:PDF},
  groups       = {SSD},
  publisher    = {Elsevier BV},
}

@Article{Ma_2020,
  author       = {Ma, Wen and Wang, Xiao and Yu, Jiong},
  date         = {2020},
  journaltitle = {IEEE Access},
  title        = {A Lightweight Feature Fusion Single Shot Multibox Detector for Garbage Detection},
  doi          = {10.1109/access.2020.3031990},
  issn         = {2169-3536},
  pages        = {188577--188586},
  volume       = {8},
  file         = {:Ma_2020 - A Lightweight Feature Fusion Single Shot Multibox Detector for Garbage Detection.pdf:PDF},
  groups       = {SSD},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@InProceedings{Jiang_2020,
  author     = {Jiang, Zhongyun and Wang, Rongrong},
  booktitle  = {2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
  date       = {2020-12},
  title      = {Underwater Object Detection Based on Improved Single Shot MultiBox Detector},
  doi        = {10.1145/3446132.3446170},
  pages      = {1--7},
  publisher  = {ACM},
  series     = {ACAI 2020},
  collection = {ACAI 2020},
  file       = {:Jiang_2020 - Underwater Object Detection Based on Improved Single Shot MultiBox Detector.pdf:PDF},
  groups     = {SSD},
}

@Article{Wang_2019,
  author      = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
  date        = {2019-07-24},
  title       = {Benchmarking TPU, GPU, and CPU Platforms for Deep Learning},
  doi         = {10.48550/ARXIV.1907.10701},
  eprint      = {1907.10701},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.},
  copyright   = {Creative Commons Attribution Share Alike 4.0 International},
  file        = {:Wang_2019 - Benchmarking TPU, GPU, and CPU Platforms for Deep Learning.pdf:PDF:http\://arxiv.org/pdf/1907.10701v4},
  groups      = {Training},
  keywords    = {Machine Learning (cs.LG), Performance (cs.PF), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2019},
}

@Article{Carranza_Garcia_2020,
  author       = {Carranza-García, Manuel and Torres-Mateo, Jesús and Lara-Benítez, Pedro and García-Gutiérrez, Jorge},
  date         = {2020-12},
  journaltitle = {Remote Sensing},
  title        = {On the Performance of One-Stage and Two-Stage Object Detectors in Autonomous Vehicles Using Camera Data},
  doi          = {10.3390/rs13010089},
  issn         = {2072-4292},
  number       = {1},
  pages        = {89},
  volume       = {13},
  file         = {:Carranza_Garcia_2020 - On the Performance of One Stage and Two Stage Object Detectors in Autonomous Vehicles Using Camera Data.pdf:PDF},
  groups       = {Object Detection},
  publisher    = {MDPI AG},
}

@Article{Girshick_2013,
  author      = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date        = {2013-11-11},
  title       = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  doi         = {10.48550/ARXIV.1311.2524},
  eprint      = {1311.2524},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Girshick_2013 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:PDF:http\://arxiv.org/pdf/1311.2524v5},
  groups      = {Object Detection},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2013},
}

@Article{Wang_2022,
  author      = {Wang, Beinan},
  date        = {2022-06-19},
  title       = {A Parallel Implementation of Computing Mean Average Precision},
  doi         = {10.48550/ARXIV.2206.09504},
  eprint      = {2206.09504},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Mean Average Precision (mAP) has been widely used for evaluating the quality of object detectors, but an efficient implementation is still absent. Current implementations can only count true positives (TP's) and false positives (FP's) for one class at a time by looping through every detection of that class sequentially. Not only are these approaches inefficient, but they are also inconvenient for reporting validation mAP during training. We propose a parallelized alternative that can process mini-batches of detected bounding boxes (DTBB's) and ground truth bounding boxes (GTBB's) as inference goes such that mAP can be instantly calculated after inference is finished. Loops and control statements in sequential implementations are replaced with extensive uses of broadcasting, masking, and indexing. All operators involved are supported by popular machine learning frameworks such as PyTorch and TensorFlow. As a result, our implementation is much faster and can easily fit into typical training routines. A PyTorch version of our implementation is available at https://github.com/bwangca/fast-map.},
  copyright   = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  file        = {:Wang_2022 - A Parallel Implementation of Computing Mean Average Precision.pdf:PDF:http\://arxiv.org/pdf/2206.09504v1},
  groups      = {Metrics},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2022},
}

@Article{Rezatofighi_2019,
  author      = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  date        = {2019-02-25},
  title       = {Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression},
  doi         = {10.48550/ARXIV.1902.09630},
  eprint      = {1902.09630},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that $IoU$ can be directly used as a regression loss. However, $IoU$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of $IoU$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized $IoU$ ($GIoU$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, $IoU$ based, and new, $GIoU$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
  copyright   = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  file        = {:Rezatofighi_2019 - Generalized Intersection Over Union_ a Metric and a Loss for Bounding Box Regression.pdf:PDF:http\://arxiv.org/pdf/1902.09630v2},
  groups      = {Metrics},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2019},
}

@Article{Lee_2018,
  author      = {Lee, Sejin and Park, Byungjae and Kim, Ayoung},
  date        = {2018-10-18},
  title       = {Deep Learning from Shallow Dives: Sonar Image Generation and Training for Underwater Object Detection},
  doi         = {10.48550/ARXIV.1810.07990},
  eprint      = {1810.07990},
  eprintclass = {cs.RO},
  eprinttype  = {arXiv},
  abstract    = {Among underwater perceptual sensors, imaging sonar has been highlighted for its perceptual robustness underwater. The major challenge of imaging sonar, however, arises from the difficulty in defining visual features despite limited resolution and high noise levels. Recent developments in deep learning provide a powerful solution for computer-vision researches using optical images. Unfortunately, deep learning-based approaches are not well established for imaging sonars, mainly due to the scant data in the training phase. Unlike the abundant publically available terrestrial images, obtaining underwater images is often costly, and securing enough underwater images for training is not straightforward. To tackle this issue, this paper presents a solution to this field's lack of data by introducing a novel end-to-end image-synthesizing method in the training image preparation phase. The proposed method present image synthesizing scheme to the images captured by an underwater simulator. Our synthetic images are based on the sonar imaging models and noisy characteristics to represent the real data obtained from the sea. We validate the proposed scheme by training using a simulator and by testing the simulated images with real underwater sonar images obtained from a water tank and the sea.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Lee_2018 - Deep Learning from Shallow Dives_ Sonar Image Generation and Training for Underwater Object Detection.pdf:PDF:http\://arxiv.org/pdf/1810.07990v1},
  groups      = {Computer Vision},
  keywords    = {Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2018},
}

@Article{Redmon_2016_YOLOv2,
  author      = {Redmon, Joseph and Farhadi, Ali},
  date        = {2016-12-25},
  title       = {YOLO9000: Better, Faster, Stronger},
  doi         = {10.48550/ARXIV.1612.08242},
  eprint      = {1612.08242},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Redmon_2016_YOLOv2 - YOLO9000_ Better, Faster, Stronger.pdf:PDF:http\://arxiv.org/pdf/1612.08242v1},
  groups      = {YOLO},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2016},
}

@Article{Redmon_2018,
  author      = {Redmon, Joseph and Farhadi, Ali},
  date        = {2018-04-08},
  title       = {YOLOv3: An Incremental Improvement},
  doi         = {10.48550/ARXIV.1804.02767},
  eprint      = {1804.02767},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Redmon_2018 - YOLOv3_ an Incremental Improvement.pdf:PDF:http\://arxiv.org/pdf/1804.02767v1},
  groups      = {YOLO},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2018},
}

@Article{Bochkovskiy_2020,
  author      = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  date        = {2020-04-23},
  title       = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  doi         = {10.48550/ARXIV.2004.10934},
  eprint      = {2004.10934},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Bochkovskiy_2020 - YOLOv4_ Optimal Speed and Accuracy of Object Detection.pdf:PDF:http\://arxiv.org/pdf/2004.10934v1},
  groups      = {YOLO},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering},
  publisher   = {arXiv},
  year        = {2020},
}

@Article{Lee_2013,
  author       = {Lee, Dong-Hyun},
  date         = {2013-07},
  journaltitle = {ICML 2013 Workshop : Challenges in Representation Learning (WREPL)},
  title        = {Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
  groups       = {Pseudo-labeling},
}

@Article{C_A_Padmanabha_Reddy_2018,
  author       = {C A Padmanabha Reddy, Y and Viswanath, P and Eswara Reddy, B},
  date         = {2018-02},
  journaltitle = {International Journal of Engineering \& Technology},
  title        = {Semi-supervised learning: a brief review},
  doi          = {10.14419/ijet.v7i1.8.9977},
  issn         = {2227-524X},
  number       = {1.8},
  pages        = {81},
  volume       = {7},
  file         = {:C_A_Padmanabha_Reddy_2018 - Semi Supervised Learning_ a Brief Review.pdf:PDF},
  groups       = {Semi-supervised Learning},
  journal      = {International Journal of Engineering \& Technology},
  publisher    = {Science Publishing Corporation},
}

@Article{Zhu_2005,
  author       = {Zhu, Xiaojin Jerry},
  date         = {2005},
  journaltitle = {CS Technical Reports},
  title        = {Semi-Supervised Learning Literature Survey},
  abstract     = {We review some of the literature on semi-supervised learning in this paper. Traditional classifiers need labeled data (feature / label pairs) to train. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice.},
  file         = {:Zhu_2005 - Semi Supervised Learning Literature Survey.pdf:PDF},
  groups       = {Semi-supervised Learning},
  publisher    = {University of Wisconsin-Madison Department of Computer Sciences},
}

@Article{van_Engelen_2019,
  author       = {van Engelen, Jesper E. and Hoos, Holger H.},
  date         = {2019-11},
  journaltitle = {Machine Learning},
  title        = {A survey on semi-supervised learning},
  doi          = {10.1007/s10994-019-05855-6},
  issn         = {1573-0565},
  number       = {2},
  pages        = {373--440},
  volume       = {109},
  file         = {:van_Engelen_2019 - A Survey on Semi Supervised Learning.pdf:PDF},
  groups       = {Semi-supervised Learning},
  publisher    = {Springer Science and Business Media LLC},
}

@InBook{Hady_2013,
  author    = {Hady, Mohamed Farouk Abdel and Schwenker, Friedhelm},
  booktitle = {Handbook on Neural Information Processing},
  date      = {2013},
  title     = {Semi-supervised Learning},
  doi       = {10.1007/978-3-642-36657-4_7},
  isbn      = {9783642366574},
  pages     = {215--239},
  publisher = {Springer Berlin Heidelberg},
  file      = {:Hady_2013 - Semi Supervised Learning.pdf:PDF},
  groups    = {Semi-supervised Learning},
  issn      = {1868-4408},
}

@Article{Fan_2022,
  author       = {Fan, Yue and Kukleva, Anna and Dai, Dengxin and Schiele, Bernt},
  date         = {2022-12},
  journaltitle = {International Journal of Computer Vision},
  title        = {Revisiting Consistency Regularization for Semi-Supervised Learning},
  doi          = {10.1007/s11263-022-01723-4},
  eprint       = {2112.05825v1},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  issn         = {1573-1405},
  number       = {3},
  pages        = {626--643},
  volume       = {131},
  abstract     = {Consistency regularization is one of the most widely-used techniques for semi-supervised learning (SSL). Generally, the aim is to train a model that is invariant to various data augmentations. In this paper, we revisit this idea and find that enforcing invariance by decreasing distances between features from differently augmented images leads to improved performance. However, encouraging equivariance instead, by increasing the feature distance, further improves performance. To this end, we propose an improved consistency regularization framework by a simple yet effective technique, FeatDistLoss, that imposes consistency and equivariance on the classifier and the feature level, respectively. Experimental results show that our model defines a new state of the art for various datasets and settings and outperforms previous work by a significant margin, particularly in low data regimes. Extensive experiments are conducted to analyze the method, and the code will be published.},
  file         = {:Fan_2022 - Revisiting Consistency Regularization for Semi Supervised Learning.pdf:PDF},
  groups       = {Consistency Regularization},
  journal      = {International Journal of Computer Vision},
  keywords     = {cs.CV},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Hall_1987,
  author       = {Hall, Peter},
  date         = {1987-12},
  journaltitle = {The Annals of Statistics},
  title        = {On Kullback-Leibler Loss and Density Estimation},
  doi          = {10.1214/aos/1176350606},
  issn         = {0090-5364},
  number       = {4},
  pages        = {1491--1519},
  url          = {http://www.jstor.org/stable/2241687},
  volume       = {15},
  file         = {:Hall_1987 - On Kullback Leibler Loss and Density Estimation.pdf:PDF},
  groups       = {Semi-supervised Learning},
  publisher    = {Institute of Mathematical Statistics},
}

@Article{Song_2021,
  author      = {Song, Zixing and Yang, Xiangli and Xu, Zenglin and King, Irwin},
  date        = {2021-02-26},
  title       = {Graph-based Semi-supervised Learning: A Comprehensive Review},
  doi         = {10.48550/ARXIV.2102.13303},
  eprint      = {2102.13303},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Semi-supervised learning (SSL) has tremendous value in practice due to its ability to utilize both labeled data and unlabelled data. An important class of SSL methods is to naturally represent data as graphs such that the label information of unlabelled samples can be inferred from the graphs, which corresponds to graph-based semi-supervised learning (GSSL) methods. GSSL methods have demonstrated their advantages in various domains due to their uniqueness of structure, the universality of applications, and their scalability to large scale data. Focusing on this class of methods, this work aims to provide both researchers and practitioners with a solid and systematic understanding of relevant advances as well as the underlying connections among them. This makes our paper distinct from recent surveys that cover an overall picture of SSL methods while neglecting fundamental understanding of GSSL methods. In particular, a major contribution of this paper lies in a new generalized taxonomy for GSSL, including graph regularization and graph embedding methods, with the most up-to-date references and useful resources such as codes, datasets, and applications. Furthermore, we present several potential research directions as future work with insights into this rapidly growing field.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Song_2021 - Graph Based Semi Supervised Learning_ a Comprehensive Review.pdf:PDF:http\://arxiv.org/pdf/2102.13303v1},
  groups      = {Semi-supervised Learning},
  keywords    = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2021},
}

@Article{Lucas_2022,
  author       = {Lucas, Thomas and Weinzaepfel, Philippe and Rogez, Gregory},
  date         = {2022-06},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Barely-Supervised Learning: Semi-supervised Learning with Very Few Labeled Images},
  doi          = {10.1609/aaai.v36i2.20082},
  issn         = {2159-5399},
  number       = {2},
  pages        = {1881--1889},
  volume       = {36},
  file         = {:Lucas_2022 - Barely Supervised Learning_ Semi Supervised Learning with Very Few Labeled Images.pdf:PDF:https\://ojs.aaai.org/index.php/AAAI/article/download/20082/19841},
  groups       = {Semi-supervised Learning},
  publisher    = {Association for the Advancement of Artificial Intelligence (AAAI)},
}

@Article{Ferreira_2023,
  author       = {Ferreira, Rafael E. P. and Lee, Yong Jae and Dórea, João R. R.},
  date         = {2023-08},
  journaltitle = {Scientific Reports},
  title        = {Using pseudo-labeling to improve performance of deep neural networks for animal identification},
  doi          = {10.1038/s41598-023-40977-x},
  issn         = {2045-2322},
  number       = {1},
  volume       = {13},
  file         = {:Ferreira_2023 - Using Pseudo Labeling to Improve Performance of Deep Neural Networks for Animal Identification.pdf:PDF:https\://www.nature.com/articles/s41598-023-40977-x.pdf},
  groups       = {Pseudo-labeling},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Kage_2024,
  author      = {Kage, Patrick and Rothenberger, Jay C. and Andreadis, Pavlos and Diochnos, Dimitrios I.},
  date        = {2024-08-13},
  title       = {A Review of Pseudo-Labeling for Computer Vision},
  doi         = {10.48550/ARXIV.2408.07221},
  eprint      = {2408.07221},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Deep neural models have achieved state of the art performance on a wide range of problems in computer science, especially in computer vision. However, deep neural networks often require large datasets of labeled samples to generalize effectively, and an important area of active research is semi-supervised learning, which attempts to instead utilize large quantities of (easily acquired) unlabeled samples. One family of methods in this space is pseudo-labeling, a class of algorithms that use model outputs to assign labels to unlabeled samples which are then used as labeled samples during training. Such assigned labels, called pseudo-labels, are most commonly associated with the field of semi-supervised learning. In this work we explore a broader interpretation of pseudo-labels within both self-supervised and unsupervised methods. By drawing the connection between these areas we identify new directions when advancements in one area would likely benefit others, such as curriculum learning and self-supervised regularization.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Kage_2024 - A Review of Pseudo Labeling for Computer Vision.pdf:PDF:http\://arxiv.org/pdf/2408.07221v1},
  groups      = {Pseudo-labeling},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.0; I.5.4; I.4.0},
  publisher   = {arXiv},
  year        = {2024},
}

@Article{Min_2022,
  author      = {Min, Zeping and Ge, Qian and Tai, Cheng},
  date        = {2022-11-18},
  title       = {Why the pseudo label based semi-supervised learning algorithm is effective?},
  doi         = {10.48550/ARXIV.2211.10039},
  eprint      = {2211.10039},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Recently, pseudo label based semi-supervised learning has achieved great success in many fields. The core idea of the pseudo label based semi-supervised learning algorithm is to use the model trained on the labeled data to generate pseudo labels on the unlabeled data, and then train a model to fit the previously generated pseudo labels. We give a theory analysis for why pseudo label based semi-supervised learning is effective in this paper. We mainly compare the generalization error of the model trained under two settings: (1) There are N labeled data. (2) There are N unlabeled data and a suitable initial model. Our analysis shows that, firstly, when the amount of unlabeled data tends to infinity, the pseudo label based semi-supervised learning algorithm can obtain model which have the same generalization error upper bound as model obtained by normally training in the condition of the amount of labeled data tends to infinity. More importantly, we prove that when the amount of unlabeled data is large enough, the generalization error upper bound of the model obtained by pseudo label based semi-supervised learning algorithm can converge to the optimal upper bound with linear convergence rate. We also give the lower bound on sampling complexity to achieve linear convergence rate. Our analysis contributes to understanding the empirical successes of pseudo label-based semi-supervised learning.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Min_2022 - Why the Pseudo Label Based Semi Supervised Learning Algorithm Is Effective_.pdf:PDF:http\://arxiv.org/pdf/2211.10039v2},
  groups      = {Pseudo-labeling},
  keywords    = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2022},
}

@Article{Cascante_Bonilla_2020,
  author      = {Cascante-Bonilla, Paola and Tan, Fuwen and Qi, Yanjun and Ordonez, Vicente},
  date        = {2020-01-16},
  title       = {Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning},
  doi         = {10.48550/ARXIV.2001.06001},
  eprint      = {2001.06001},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {In this paper we revisit the idea of pseudo-labeling in the context of semi-supervised learning where a learning algorithm has access to a small set of labeled samples and a large set of unlabeled samples. Pseudo-labeling works by applying pseudo-labels to samples in the unlabeled set by using a model trained on the combination of the labeled samples and any previously pseudo-labeled samples, and iteratively repeating this process in a self-training cycle. Current methods seem to have abandoned this approach in favor of consistency regularization methods that train models under a combination of different styles of self-supervised losses on the unlabeled samples and standard supervised losses on the labeled samples. We empirically demonstrate that pseudo-labeling can in fact be competitive with the state-of-the-art, while being more resilient to out-of-distribution samples in the unlabeled set. We identify two key factors that allow pseudo-labeling to achieve such remarkable results (1) applying curriculum learning principles and (2) avoiding concept drift by restarting model parameters before each self-training cycle. We obtain 94.91\% accuracy on CIFAR-10 using only 4,000 labeled samples, and 68.87\% top-1 accuracy on Imagenet-ILSVRC using only 10\% of the labeled samples. The code is available at https://github.com/uvavision/Curriculum-Labeling},
  copyright   = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  file        = {:Cascante_Bonilla_2020 - Curriculum Labeling_ Revisiting Pseudo Labeling for Semi Supervised Learning.pdf:PDF:http\://arxiv.org/pdf/2001.06001v2},
  groups      = {Pseudo-labeling},
  keywords    = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2020},
}

@Article{Sohn_2020,
  author      = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
  date        = {2020-01-21},
  title       = {FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
  doi         = {10.48550/arXiv.2001.07685},
  eprint      = {2001.07685v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Sohn_2020 - FixMatch_ Simplifying Semi Supervised Learning with Consistency and Confidence.pdf:PDF:http\://arxiv.org/pdf/2001.07685},
  groups      = {FixMatch & MixMatch},
  keywords    = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
}

@InProceedings{Berthelot_2019,
  author    = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A.},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2019},
  title     = {MixMatch: A Holistic Approach to Semi-Supervised Learning},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf},
  volume    = {32},
  abstract  = {Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that guesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using MixUp. MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38\% to 11\%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success. Code is attached.},
  file      = {:Berthelot_2019 - MixMatch_ a Holistic Approach to Semi Supervised Learning.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf},
  groups    = {FixMatch & MixMatch},
}

@WWW{Rosebrock_2016,
  author = {Rosebrock, Adrian},
  date   = {2016-11-07},
  title  = {Intersection over Union (IoU) for object detection},
  url    = {https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/},
  file   = {:https\://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/:},
  groups = {Afbeeldingen},
}

@WWW{http//www.freephotos.lu/_2010,
  author = {http://www.freephotos.lu/},
  date   = {2010-06-14},
  title  = {Image processing illustration, before Otsu algorithm},
  url    = {http://en.wikipedia.org/wiki/File:Image_processing_pre_otsus_algorithm.jpg},
  file   = {:http\://en.wikipedia.org/wiki/File\:Image_processing_pre_otsus_algorithm.jpg:},
  groups = {Afbeeldingen},
}

@WWW{Pikez33_2010,
  author = {Pikez33},
  date   = {2010-06-14},
  title  = {Image processing illustration, after Otsu algorithm},
  url    = {https://commons.wikimedia.org/wiki/File:Image_processing_post_otsus_algorithm.jpg},
  file   = {:https\://commons.wikimedia.org/wiki/File\:Image_processing_post_otsus_algorithm.jpg:},
  groups = {Afbeeldingen},
}

@WWW{soundmetrics.com,
  author = {soundmetrics.com},
  title  = {ARIS EXPLORER 3000},
  url    = {http://www.soundmetrics.com/products/aris-sonars/aris-explorer-3000},
  file   = {:http\://www.soundmetrics.com/products/aris-sonars/aris-explorer-3000:},
  groups = {Afbeeldingen},
}

@Article{DeVries_2017,
  author      = {DeVries, Terrance and Taylor, Graham W.},
  date        = {2017-08-15},
  title       = {Improved Regularization of Convolutional Neural Networks with Cutout},
  doi         = {10.48550/ARXIV.1708.04552},
  eprint      = {1708.04552},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:DeVries_2017 - Improved Regularization of Convolutional Neural Networks with Cutout.pdf:PDF:http\://arxiv.org/pdf/1708.04552v2},
  groups      = {FixMatch & MixMatch},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2017},
}

@Article{Zhang_2017,
  author      = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  date        = {2017-10-25},
  title       = {mixup: Beyond Empirical Risk Minimization},
  doi         = {10.48550/ARXIV.1710.09412},
  eprint      = {1710.09412},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Zhang_2017 - Mixup_ beyond Empirical Risk Minimization.pdf:PDF:http\://arxiv.org/pdf/1710.09412v2},
  groups      = {FixMatch & MixMatch},
  keywords    = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2017},
}

@Article{Gui_2024,
  author       = {Gui, Jie and Chen, Tuo and Zhang, Jing and Cao, Qiong and Sun, Zhenan and Luo, Hao and Tao, Dacheng},
  date         = {2024-12},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {A Survey on Self-Supervised Learning: Algorithms, Applications, and Future Trends},
  doi          = {10.1109/tpami.2024.3415112},
  eprint       = {2301.05712v4},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  issn         = {1939-3539},
  number       = {12},
  pages        = {9052--9071},
  volume       = {46},
  abstract     = {Deep supervised learning algorithms typically require a large volume of labeled data to achieve satisfactory performance. However, the process of collecting and labeling such data can be expensive and time-consuming. Self-supervised learning (SSL), a subset of unsupervised learning, aims to learn discriminative features from unlabeled data without relying on human-annotated labels. SSL has garnered significant attention recently, leading to the development of numerous related algorithms. However, there is a dearth of comprehensive studies that elucidate the connections and evolution of different SSL variants. This paper presents a review of diverse SSL methods, encompassing algorithmic aspects, application domains, three key trends, and open research questions. Firstly, we provide a detailed introduction to the motivations behind most SSL algorithms and compare their commonalities and differences. Secondly, we explore representative applications of SSL in domains such as image processing, computer vision, and natural language processing. Lastly, we discuss the three primary trends observed in SSL research and highlight the open questions that remain. A curated collection of valuable resources can be accessed at https://github.com/guijiejie/SSL.},
  file         = {:Gui_2024 - A Survey on Self Supervised Learning_ Algorithms, Applications, and Future Trends.pdf:PDF:http\://arxiv.org/pdf/2301.05712},
  groups       = {Self-supervised Learning},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords     = {cs.LG},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Chen_2020,
  author      = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date        = {2020-02-13},
  title       = {A Simple Framework for Contrastive Learning of Visual Representations},
  doi         = {10.48550/ARXIV.2002.05709},
  eprint      = {2002.05709v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Chen_2020 - A Simple Framework for Contrastive Learning of Visual Representations.pdf:PDF:http\://arxiv.org/pdf/2002.05709},
  groups      = {SimCLR},
  keywords    = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
}

@Article{Gupta_2022,
  author      = {Gupta, Kartik and Ajanthan, Thalaiyasingam and Hengel, Anton van den and Gould, Stephen},
  date        = {2022-12-22},
  title       = {Understanding and Improving the Role of Projection Head in Self-Supervised Learning},
  doi         = {10.48550/ARXIV.2212.11491},
  eprint      = {2212.11491},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Self-supervised learning (SSL) aims to produce useful feature representations without access to any human-labeled data annotations. Due to the success of recent SSL methods based on contrastive learning, such as SimCLR, this problem has gained popularity. Most current contrastive learning approaches append a parametrized projection head to the end of some backbone network to optimize the InfoNCE objective and then discard the learned projection head after training. This raises a fundamental question: Why is a learnable projection head required if we are to discard it after training? In this work, we first perform a systematic study on the behavior of SSL training focusing on the role of the projection head layers. By formulating the projection head as a parametric component for the InfoNCE objective rather than a part of the network, we present an alternative optimization scheme for training contrastive learning based SSL frameworks. Our experimental study on multiple image classification datasets demonstrates the effectiveness of the proposed approach over alternatives in the SSL literature.},
  copyright   = {Creative Commons Attribution 4.0 International},
  file        = {:Gupta_2022 - Understanding and Improving the Role of Projection Head in Self Supervised Learning.pdf:PDF:http\://arxiv.org/pdf/2212.11491v1},
  groups      = {SimCLR},
  keywords    = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2022},
}

@Article{He_2019,
  author      = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date        = {2019-11-13},
  title       = {Momentum Contrast for Unsupervised Visual Representation Learning},
  doi         = {10.48550/ARXIV.1911.05722},
  eprint      = {1911.05722},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:He_2019 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf:PDF:http\://arxiv.org/pdf/1911.05722v3},
  groups      = {MoCo},
  keywords    = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2019},
}

@Article{Sowe_2025,
  author       = {Sowe, Ebou A. and Bah, Yunusa A.},
  date         = {2025-01-10},
  journaltitle = {Journal of Advances in Civil and Mechanical Engineering},
  title        = {Momentum Contrast for Unsupervised Visual Representation Learning},
  doi          = {10.20944/preprints202501.0668.v1},
  abstract     = {This brief report presents a novel unsupervised learning representation learning method called momentum contrast. Momentum contrast uses a contrastive learning technique to learn representations by comparing features of related yet dissimilar images for efficient feature extraction and unsupervised representation learning. Similar images are grouped together, and dissimilar images are placed far apart. The method builds upon previous works in contrastive learning but includes a momentum optimisation step to improve representation learning performance and generate better quality representations. Experiments on various datasets demonstrate that momentum contrast is able to learn high-quality representations, allowing us to directly use them to achieve competitive performance with fewer labelled examples.},
  file         = {:Sowe_2025 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf:PDF},
  groups       = {MoCo},
  publisher    = {MDPI AG},
}

@Misc{Grill_2020,
  author    = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  date      = {2020},
  title     = {Bootstrap your own latent: A new approach to self-supervised Learning},
  doi       = {10.48550/ARXIV.2006.07733},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file      = {:Grill_2020 - Bootstrap Your Own Latent_ a New Approach to Self Supervised Learning.pdf:PDF:http\://arxiv.org/pdf/2006.07733},
  groups    = {BYOL},
  keywords  = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InProceedings{Anish_Dev_2014,
  author    = {Anish Dev, Jega},
  booktitle = {2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)},
  date      = {2014-05},
  title     = {Bitcoin mining acceleration and performance quantification},
  doi       = {10.1109/ccece.2014.6900989},
  pages     = {1--6},
  publisher = {IEEE},
  file      = {:Anish_Dev_2014 - Bitcoin Mining Acceleration and Performance Quantification.pdf:PDF},
  groups    = {Training},
}

@WWW{TechPowerUp_RTX-A5000,
  author = {TechPowerUp},
  title  = {NVIDIA RTX A5000},
  url    = {https://www.techpowerup.com/gpu-specs/rtx-a5000.c3748},
  file   = {:TechPowerUp_RTX-A5000 - NVIDIA RTX A5000.html:URL},
  groups = {Training},
}

@WWW{TechPowerUp_Tesla-T4,
  author = {TechPowerUp},
  title  = {NVIDIA Tesla T4},
  url    = {https://www.techpowerup.com/gpu-specs/tesla-t4.c3316},
  file   = {:TechPowerUp_Tesla-T4 - NVIDIA Tesla T4.html:URL},
  groups = {Training},
}

@InProceedings{Jouppi_2017,
  author     = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  booktitle  = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  date       = {2017-06},
  title      = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  doi        = {10.1145/3079856.3080246},
  pages      = {1--12},
  publisher  = {ACM},
  series     = {ISCA ’17},
  collection = {ISCA ’17},
  file       = {:Jouppi_2017 - In Datacenter Performance Analysis of a Tensor Processing Unit.pdf:PDF},
  groups     = {Training},
}

@Article{Laarhoven_2017,
  author      = {van Laarhoven, Twan},
  date        = {2017-06-16},
  title       = {L2 Regularization versus Batch and Weight Normalization},
  doi         = {10.48550/ARXIV.1706.05350},
  eprint      = {1706.05350},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Batch Normalization is a commonly used trick to improve the training of deep neural networks. These neural networks use L2 regularization, also called weight decay, ostensibly to prevent overfitting. However, we show that L2 regularization has no regularizing effect when combined with normalization. Instead, regularization has an influence on the scale of weights, and thereby on the effective learning rate. We investigate this dependence, both in theory, and experimentally. We show that popular optimization methods such as ADAM only partially eliminate the influence of normalization on the learning rate. This leads to a discussion on other ways to mitigate this issue.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Laarhoven_2017 - L2 Regularization Versus Batch and Weight Normalization.pdf:PDF:http\://arxiv.org/pdf/1706.05350v1},
  groups      = {BYOL},
  keywords    = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2017},
}

@WWW{Adaloglou_2022,
  author = {Nikolas Adaloglou},
  date   = {2022-05-12},
  title  = {BYOL tutorial: self-supervised learning on CIFAR images with code in Pytorch},
  url    = {https://theaisummer.com/byol/},
  file   = {:https\://theaisummer.com/byol/:},
  groups = {BYOL},
}

@Article{Oord_2018,
  author      = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  date        = {2018-07-10},
  title       = {Representation Learning with Contrastive Predictive Coding},
  doi         = {10.48550/ARXIV.1807.03748},
  eprint      = {1807.03748},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Oord_2018 - Representation Learning with Contrastive Predictive Coding.pdf:PDF:http\://arxiv.org/pdf/1807.03748v2},
  groups      = {Contrastive learning},
  keywords    = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2018},
}

@InBook{Chen_2017,
  author    = {Chen, Chenyi and Liu, Ming-Yu and Tuzel, Oncel and Xiao, Jianxiong},
  booktitle = {Computer Vision – ACCV 2016},
  date      = {2017},
  title     = {R-CNN for Small Object Detection},
  doi       = {10.1007/978-3-319-54193-8_14},
  isbn      = {9783319541938},
  pages     = {214--230},
  publisher = {Springer International Publishing},
  file      = {:Chen_2017 - R CNN for Small Object Detection.pdf:PDF},
  groups    = {Faster R-CNN},
  issn      = {1611-3349},
}

@Article{Ying_2019,
  author       = {Ying, Xue},
  date         = {2019-02},
  journaltitle = {Journal of Physics: Conference Series},
  title        = {An Overview of Overfitting and its Solutions},
  doi          = {10.1088/1742-6596/1168/2/022022},
  issn         = {1742-6596},
  pages        = {022022},
  volume       = {1168},
  file         = {:Ying_2019 - An Overview of Overfitting and Its Solutions.pdf:PDF},
  groups       = {Implementation},
  publisher    = {IOP Publishing},
}

@InBook{Prechelt_1998,
  author    = {Prechelt, Lutz},
  booktitle = {Neural Networks: Tricks of the Trade},
  date      = {1998},
  title     = {Early Stopping - But When?},
  doi       = {10.1007/3-540-49430-8_3},
  isbn      = {9783540494300},
  pages     = {55--69},
  publisher = {Springer Berlin Heidelberg},
  file      = {:Prechelt_1998 - Early Stopping but When_.pdf:PDF},
  groups    = {Implementation},
  issn      = {0302-9743},
}

@InBook{Imambi_2021,
  author    = {Imambi, Sagar and Prakash, Kolla Bhanu and Kanagachidambaresan, G. R.},
  booktitle = {Programming with TensorFlow},
  date      = {2021},
  title     = {PyTorch},
  doi       = {10.1007/978-3-030-57077-4_10},
  isbn      = {9783030570774},
  pages     = {87--104},
  publisher = {Springer International Publishing},
  file      = {:Imambi_2021 - PyTorch.pdf:PDF},
  groups    = {Implementation},
  issn      = {2522-8609},
}

@Article{Pang_2019,
  author       = {Pang, Bo and Nijkamp, Erik and Wu, Ying Nian},
  date         = {2019-09},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  title        = {Deep Learning With TensorFlow: A Review},
  doi          = {10.3102/1076998619872761},
  issn         = {1935-1054},
  number       = {2},
  pages        = {227--248},
  volume       = {45},
  file         = {:Pang_2019 - Deep Learning with TensorFlow_ a Review.pdf:PDF},
  groups       = {Implementation},
  publisher    = {American Educational Research Association (AERA)},
}

@Article{Micikevicius_2017,
  author      = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  date        = {2017-10-10},
  title       = {Mixed Precision Training},
  doi         = {10.48550/ARXIV.1710.03740},
  eprint      = {1710.03740},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  abstract    = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Micikevicius_2017 - Mixed Precision Training.pdf:PDF:http\://arxiv.org/pdf/1710.03740v3},
  groups      = {Training},
  keywords    = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
  year        = {2017},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Data\;0\;1\;0x60bfa2ff\;\;\;;
2 StaticGroup:Datasets\;0\;0\;0x56ac92ff\;\;\;;
2 StaticGroup:Dataset Papers\;0\;1\;0xac5671ff\;\;\;;
1 StaticGroup:Deep Learning\;0\;1\;0x7260bfff\;\;\;;
2 StaticGroup:Computer Vision\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Object Detection\;0\;1\;0xbf607dff\;\;\;;
4 StaticGroup:Supervised Learning\;0\;1\;0x60b0bfff\;\;\;;
5 StaticGroup:YOLO\;0\;1\;0x569eacff\;\;\;;
5 StaticGroup:Faster R-CNN\;0\;1\;0xac6456ff\;\;\;;
5 StaticGroup:SSD\;0\;1\;0x81ac56ff\;\;\;;
4 StaticGroup:Semi-supervised Learning\;0\;1\;0xbf6f60ff\;\;\;;
5 StaticGroup:Pseudo-labeling\;0\;1\;0xac6456ff\;\;\;;
5 StaticGroup:Consistency Regularization\;0\;1\;0x8e56acff\;\;\;;
5 StaticGroup:FixMatch & MixMatch\;0\;1\;0x569eacff\;\;\;;
4 StaticGroup:Self-supervised Learning\;0\;1\;0x8fbf60ff\;\;\;;
5 StaticGroup:SimCLR\;0\;1\;0x81ac56ff\;\;\;;
5 StaticGroup:MoCo\;0\;1\;0x8156acff\;\;\;;
5 StaticGroup:BYOL\;0\;1\;0x56acacff\;\;\;;
5 StaticGroup:Contrastive learning\;0\;1\;0xac5656ff\;\;\;;
4 StaticGroup:Metrics\;0\;1\;0x9060bfff\;\;\;;
3 StaticGroup:Filtering\;0\;1\;0x60bfa2ff\;\;\;;
3 StaticGroup:Thresholding\;0\;0\;0x7260bfff\;\;\;;
3 StaticGroup:Edge Detection\;0\;0\;0xadbf60ff\;\;\;;
2 StaticGroup:Training\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Implementation\;0\;1\;0x6095bfff\;\;\;;
1 StaticGroup:Specs & Formats\;0\;1\;0xbf8a60ff\;\;\;;
1 StaticGroup:Afbeeldingen\;0\;1\;0xbf8a60ff\;\;\;;
}
