% === Acroniemen ===
\newacronym{auv}{AUV}{Autonomous Underwater Vehicle}
\newacronym{mfls}{MFLS}{Multibeam Forward-Looking Sonar}
\newacronym{cnn}{CNN}{Convolutioneel Neuraal Netwerk}
\newacronym{fcn}{FCN}{Fully Convolutional Network}
\newacronym{rcnn}{R-CNN}{Region-based Convolutional Neural Network}
\newacronym{yolo}{YOLO}{You Only Look Once}
\newacronym{ssd}{SSD}{Single Shot Multibox Detector}
\newacronym{ssl}{SSL}{Semi-Supervised Learning}
\newacronym{self-sl}{Self-SL}{Self-Supervised Learning}
\newacronym{iou}{IoU}{Intersection over Union}
\newacronym{map}{mAP}{Mean Average Precision}
\newacronym{simclr}{SimCLR}{Simple Framework for Contrastive Learning of Visual Representations}
\newacronym{byol}{BYOL}{Bootstrap Your Own Latent}
\newacronym{sss}{SSS}{Side-Scan Sonar}
\newacronym[plural={MILCO's}]{milco}{MILCO}{MIne-Like COntact}
\newacronym[plural={NOMBO's}]{nombo}{NOMBO}{NOn-Mine-like BOttom Object}
\newacronym{dms3}{DMS 3}{Destacamento de Mergulhadores Sapadores}
\newacronym[plural={ROV's}]{rov}{ROV}{Remotely Operated Vehicle}
\newacronym{coco}{COCO}{Common Objects in Context}
\newacronym{uxo}{UXO}{Unexploded Ordnance}
\newacronym{ptu}{PTU}{Pan-Tilt Unit}
\newacronym{dfki}{DFKI}{Deutsches Forschungszentrum für Künstliche Intelligenz}
\newacronym{cas}{CAS}{Continuous Active Sonar}
\newacronym{ar}{AR}{Augmented Reality}
\newacronym[description={\gls{non_max_suppression}}]{nms}{NMS}{Non-Maximum Suppression}
\newacronym{voc}{VOC}{Visual Object Classes}
\newacronym{mse}{MSE}{Mean Squared Error}
\newacronym{kl}{KL}{Kullback-Leibler}
\newacronym{nlp}{NLP}{Natural Language Processing}
\newacronym{vae}{VAE}{Variational Autoencoder}
\newacronym{gan}{GAN}{Generative Adversarial Networks}
\newacronym{nt-xent}{NT-Xent}{Normalized Temperature-scaled Cross Entropy}
\newacronym{moco}{MoCo}{Momentum Contrast}
\newacronym{ema}{EMA}{Exponential Moving Average}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{tpu}{TPU}{Tensor Processing Unit}
\newacronym{asic}{ASIC}{Application Specific Integrated Circuit}
\newacronym{fair}{FAIR}{Facebook AI Research}
\newacronym{rpn}{RPN}{Region Proposal Network}
\newacronym{roi}{RoI}{Region of Interest}
\newacronym{amp}{AMP}{Automatic Mixed Precision}
\newacronym{dino}{DINO}{Self-Distillation with No Labels}
\newacronym{uda}{UDA}{Unsupervised Data Augmentation}
\newacronym{sgd}{SGD}{Stochastic Gradient Descent}

% === Termen ===
\newglossaryentry{blindganger}
{
    name={Blindganger},
    text={blindganger},
    description={Explosief dat niet is afgegaan},
    plural={blindgangers},
    descriptionplural={Explosieven die niet zijn afgegaan.}
}

\newglossaryentry{bounding_box}
{
    name={Bounding box},
    text={bounding box},
    description={Rechthoek om een object op een afbeelding te identificeren en te lokaliseren},
    plural={bounding boxes},
    descriptionplural={Rechthoeken om objecten op een afbeelding te identificeren en te lokaliseren}
}

\newglossaryentry{batch}
{
    name={Batch},
    text={batch},
    description={Groep van samples die gebruikt worden in één trainingsstap. \autocite{Geron_2023}},
    plural={batches},
    descriptionplural={Groepen van samples die gebruikt worden in één trainingsstap. \autocite{Geron_2023}}    
}

\newglossaryentry{batch_size}
{
    name={Batch size},
    text={batch size},
    description={Het aantal samples die gebruikt worden in één trainingsstap. \autocite{Geron_2023}}
}

\newglossaryentry{mini_batch}
{
    name={Mini-batch},
    text={mini-batch},
    description={Kleine, meer behapbare subset van een batch om het geheugengebruik tijdens training te optimaliseren. \autocite{Geron_2023}},
    plural={mini-batches},
    descriptionplural={Kleine, meer behapbare subsets van een batch om het geheugengebruik tijdens training te optimaliseren. \autocite{Geron_2023}}    
}

\newglossaryentry{learning_rate}
{
    name={Learning rate},
    text={learning rate},
    description={Hyperparameter die bepaalt hoe snel het model convergeert richting het minimum van de \gls{loss_functie}. \autocite{Geron_2023}},
}

\newglossaryentry{precision}
{
    name={Precision},
    text={precision},
    description={Een metriek om de accuraatheid van de positieve voorspellingen van het model te meten. Een hoge precision betekent dat het model een lage hoeveelheid \emph{false-positives} voorspelt. Wanneer dit model iets als positief voorspelt, is de kans groot dat dit correct is. 
        $$
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        $$
        \autocite{Geron_2023}}
}

\newglossaryentry{recall}
{
    name={Recall},
    text={recall},
    description={Een metriek om te meten hoe goed het model alle relevante positieve datapunten kan voorspellen. Een hoge recall betekent dat het model heel goed is in het correct voorspellen van alle positieve datapunten, zelfs als dit betekent dat het meer \emph{false-positives} voorspelt.
        $$
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        $$
        \autocite{Geron_2023}}
}

\newglossaryentry{portaalkraan}
{
    name={Portaalkraan},
    text={portaalkraan},
    description={Een (meestal verrijdbaar) hijswerktuig, opgebouwd uit een portaal waarop een kraan gemonteerd is die verplaatsbaar is langs de horizontale draagbalk. Meestal wordt deze kraan gebruikt op kaden om o.a. containers uit te laden.}
}

\newglossaryentry{overfitting}
{
    name={Overfitting},
    text={overfitting},
    description={Een fenomeen in machine learning en deep learning waarbij het model de trainingsdata te goed leert -- inclusief ruis en outliers -- in plaats van te generaliseren. Dit zorgt ervoor dat het model excellent presteert op de trainingsdata, maar zeer slecht op ongeziene data.}
}

\newglossaryentry{non_max_suppression}
{
    name={Non-Maximum Supression},
    description={Een techniek in computervisie die wordt gebruikt om overlappende detecties te filteren en alleen de meest waarschijnlijke objectlocaties te behouden. Het selecteert de detectie met de hoogste score en onderdrukt omliggende detecties met een hoge overlap (op basis van de \gls{iou}-score), waardoor dubbele detecties worden verminderd en de nauwkeurigheid van objectdetectie-algoritmes wordt verbeterd. \autocite{Geron_2023}}
}

\newglossaryentry{loss_functie}
{
    name={Loss-functie},
    text={loss-functie},
    description={Wiskundige functie die de afwijking meet tussen de voorspellingen van een model en de werkelijke waarden. Het doel is om deze afwijking te minimaliseren tijdens het trainen van het model, zodat de nauwkeurigheid van de voorspellingen verbetert. Hoe lager de waarde van de loss-functie, hoe beter het model presteert. \autocite{Geron_2023}}
}

\newglossaryentry{thermocline}
{
    name={Thermocline},
    text={thermocline},
    description={De overgang tussen twee lagen water (of lucht) met verschillende temperatuur en dichtheid, in bijvoorbeeld meren of oceanen.},
    plural={thermoclines},
    descriptionplural={De overgangen tussen twee lagen water (of lucht) met verschillende temperatuur en dichtheid, in bijvoorbeeld meren of oceanen.}
}

\newglossaryentry{confidence_score}
{
    name={Confidence score},
    text={confidence score},
    description={De zekerheid van het model dat de voorspelling juist is. \autocite{Geron_2023}},
    plural={confidence scores},
    descriptionplural={De zekerheid van het model dat de voorspellingen juist zijn. \autocite{Geron_2023}}    
}

\newglossaryentry{batch_normalisatie}
{
    name={Batch normalisatie},
    text={batch normalisatie},
    description={Techniek in deep learning die de invoer van elke laag normaliseert (per batch) waardoor de training minder gevoelig wordt voor initiële waarden van de gewichten en \gls{learning_rate}. \autocite{Geron_2023}}
}

\newglossaryentry{backpropagation}
{
    name={Backpropagation},
    text={backpropagation},
    description={Algoritme om parameters in neurale netwerken te updaten en het model zo te trainen. Het werkt door de fout tussen de voorspelde en werkelijke output terug te propageren door het netwerk, waarbij de gradiënt van de fout ten opzichte van de gewichten wordt berekend. Deze informatie wordt vervolgens gebruikt om de gewichten an te passen, zodat het netwerk beter presteert bij toekomstige voorspellingen. \autocite{Geron_2023}}
}

\newglossaryentry{gradient_descent}
{
    name={Gradient descent},
    text={gradient descent},
    description={Wiskundig optimalisatie-algoritme dat stap voor stap het minimum van een functie zoekt door telkens in de richting van de grootste daling (de negatieve gradiënt) te bewegen. Dit algoritme gebruikt de gradiënten van de \gls{loss_functie} die door het \gls{backpropagation}-algoritme berekend zijn om de daadwerkelijke parameters aan te passen, zodat de fout afneemt. \autocite{Geron_2023}}
}

\newglossaryentry{buffer_underflow}
{
    name={Buffer underflow},
    text={buffer underflow},
    description={Numerieke fout die ontstaat wanneer een getal (bv. een gradiënt) zo klein is dat het niet nauwkeurig kan worden opgeslagen in \texttt{float16}-formaat en dus afgerond wordt naar nul. Dit heeft als gevolg een verlies van precisie en een slecht of instabiel leerproces. \autocite{Micikevicius_2017}}
}