{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOneUnLkYqud6d9g55Anogf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MGFs2WIrneDg"},"outputs":[],"source":["import keras\n","import tensorflow as tf"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbGyV1CxplAn","executionInfo":{"status":"ok","timestamp":1744287829962,"user_tz":-120,"elapsed":1239,"user":{"displayName":"Yoran Gyselen","userId":"15423074994451913636"}},"outputId":"4d418f5f-5733-4c16-9967-ebef4fe886c4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","drive  sample_data\n"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/BAP/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vy2SCS5XqFCL","executionInfo":{"status":"ok","timestamp":1744287909054,"user_tz":-120,"elapsed":128,"user":{"displayName":"Yoran Gyselen","userId":"15423074994451913636"}},"outputId":"b5ec8739-0664-463d-dbae-6db026e13a59"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["BYOL.ipynb  UATD_TF_Datasets\n"]}]},{"cell_type":"code","source":["# For EMA updates\n","class EMA:\n","  def __init__(self, model, decay=0.99):\n","    self.model = model\n","    self.decay = decay\n","\n","    self.ema_model = keras.models.clone_model(model)\n","    self.ema_model.set_weights(model.get_weights())\n","\n","  def update(self):\n","    for (ema_w, w) in zip(self.ema_model.weights, self.model.weights):\n","      ema_w.assign(self.decay * ema_w + (1 - self.decay) * w)\n","\n","  def get_ema_model(self):\n","    return self.ema_model"],"metadata":{"id":"4xLEh9ayn3d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define encoder network\n","def build_encoder():\n","  base_model = keras.applications.ResNet50(\n","      include_top=False,\n","      weights=None,\n","      input_shape=(224, 224, 3),\n","      pooling=\"avg\"\n","  )\n","  return keras.Model(base_model.input, base_model.output, name=\"encoder\")"],"metadata":{"id":"9q4Zxhpxt5Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the Projection Head\n","def build_projection_head(input_dim, output_dim=256):\n","  model = keras.Sequential(name=\"projection_head\")\n","  model.add(keras.layers.Dense(units=4096, activation=\"relu\"))\n","  model.add(keras.layers.Dense(units=output_dim, activation=None))\n","  return model"],"metadata":{"id":"up_21FeQun3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the Prediction Head\n","def build_prediction_head(input_dim, output_dim=256):\n","  model = keras.Sequential(name=\"prediction_head\")\n","  model.add(keras.layers.Dense(units=4096, activation=\"relu\"))\n","  model.add(keras.layers.Dense(units=output_dim, activation=None))\n","  return model"],"metadata":{"id":"q-br5nROvW1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BYOL(keras.Model):\n","\n","  def __init__(self, input_shape=(224, 224, 3)):\n","    super(BYOL, self).__init__()\n","\n","    self.encoder = build_encoder()\n","    self.projection_head = build_projection_head(input_dim=2048)\n","    self.prediction_head = build_prediction_head(input_dim=256)\n","\n","    self.ema = EMA(self.encoder, decay=0.99) # Exponential moving average\n","    self.ema_projection = EMA(self.projection_head, decay=0.99)\n","\n","  def call(self, inputs, training=True):\n","    view1, view2 = inputs # Two augmented views\n","\n","    # Online network\n","    z1 = self.projection_head(self.encoder(view1, training=training))\n","    z2 = self.projection_head(self.encoder(view2, training=training))\n","\n","    p1 = self.prediction_head(z1)\n","    p2 = self.prediction_head(z2)\n","\n","    # Target network (EMA updates)\n","    with tf.stop_gradient():\n","      target_encoder = self.ema.get_ema_model()\n","      target_projection = self.ema_projection.get_ema_model()\n","      z1_target = target_projection(target_encoder(view1, training=False))\n","      z2_target = target_projection(target_encoder(view2, training=False))\n","\n","    return p1, p2, z1_target, z2_target"],"metadata":{"id":"tReddWYVvjkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cosine_similarity_loss(x, y):\n","    x = tf.math.l2_normalize(x, axis=1)\n","    y = tf.math.l2_normalize(y, axis=1)\n","    return -tf.reduce_mean(tf.reduce_sum(x * y, axis=1))  # Negative cosine similarity"],"metadata":{"id":"J7bfE66Jz_PG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BYOLLoss(keras.losses.Loss):\n","    def call(self, y_true, y_pred):\n","        p1, p2, z1_target, z2_target = y_pred\n","        return (cosine_similarity_loss(p1, z2_target) + cosine_similarity_loss(p2, z1_target)) / 2"],"metadata":{"id":"ItweEC5Yl-lN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataset of augmented image pairs\n","def get_dataset(batch_size=32):\n","    (x_train, _), (_, _) = keras.datasets.cifar10.load_data()\n","    x_train = tf.image.resize(x_train, (224, 224)) / 255.0  # Normalize\n","    dataset = tf.data.Dataset.from_tensor_slices(x_train)\n","\n","    def augment(image):\n","        return tf.image.random_flip_left_right(tf.image.random_crop(image, (224, 224, 3)))\n","\n","    dataset = dataset.map(lambda x: (augment(x), augment(x)))\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    return dataset\n","\n","byol = BYOL()\n","optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","loss_fn = BYOLLoss()\n","\n","@tf.function\n","def train_step(images):\n","    with tf.GradientTape() as tape:\n","        predictions = byol(images, training=True)\n","        loss = loss_fn(None, predictions)\n","\n","    grads = tape.gradient(loss, byol.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, byol.trainable_variables))\n","\n","    # Update EMA model\n","    byol.ema.update()\n","    byol.ema_projection.update()\n","\n","    return loss\n","\n","dataset = get_dataset()\n","epochs = 10\n","\n","for epoch in range(epochs):\n","    for batch in dataset:\n","        loss = train_step(batch)\n","    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n"],"metadata":{"id":"CEeln3I6rBiJ"},"execution_count":null,"outputs":[]}]}