2025-05-22 15:44:09,606 - Dataset fraction: 0.05
2025-05-22 15:44:09,606 - Seed: 42
2025-05-22 15:44:09,606 - Batch size: 8
2025-05-22 15:44:09,606 - Epochs: 100
2025-05-22 15:44:09,941 - Initially frozen backbone layers: ['layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight']
2025-05-22 15:44:28,956 - Epoch 1/100 | Train Loss: 0.8274, LR: 0.000204, mAP@0.5: (skipped — frozen)
2025-05-22 15:44:47,128 - Epoch 2/100 | Train Loss: 0.2744, LR: 0.000402, mAP@0.5: (skipped — frozen)
2025-05-22 15:45:05,471 - Epoch 3/100 | Train Loss: 0.2194, LR: 0.000599, mAP@0.5: (skipped — frozen)
2025-05-22 15:45:05,471 - Unfroze backbone layers at epoch 3: ['layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight']
2025-05-22 15:45:45,521 - Epoch 4/100 | Train Loss: 0.2260, LR: 0.000798, mAP@0.5: 0.0212
2025-05-22 15:46:26,116 - Epoch 5/100 | Train Loss: 0.2309, LR: 0.000996, mAP@0.5: 0.0365
2025-05-22 15:47:06,661 - Epoch 6/100 | Train Loss: 0.2853, LR: 0.000996, mAP@0.5: 0.0320
2025-05-22 15:47:47,634 - Epoch 7/100 | Train Loss: 0.3131, LR: 0.000996, mAP@0.5: 0.0581
2025-05-22 15:48:29,162 - Epoch 8/100 | Train Loss: 0.3451, LR: 0.000996, mAP@0.5: 0.1106
2025-05-22 15:49:10,626 - Epoch 9/100 | Train Loss: 0.3284, LR: 0.000996, mAP@0.5: 0.1408
2025-05-22 15:49:52,306 - Epoch 10/100 | Train Loss: 0.3307, LR: 0.000996, mAP@0.5: 0.1983
2025-05-22 15:50:34,128 - Epoch 11/100 | Train Loss: 0.3238, LR: 0.000996, mAP@0.5: 0.1998
2025-05-22 15:51:15,968 - Epoch 12/100 | Train Loss: 0.3152, LR: 0.000996, mAP@0.5: 0.2181
2025-05-22 15:51:57,692 - Epoch 13/100 | Train Loss: 0.3102, LR: 0.000996, mAP@0.5: 0.2195
2025-05-22 15:52:39,884 - Epoch 14/100 | Train Loss: 0.2955, LR: 0.000996, mAP@0.5: 0.2332
2025-05-22 15:53:21,957 - Epoch 15/100 | Train Loss: 0.2947, LR: 0.000996, mAP@0.5: 0.2610
2025-05-22 15:54:03,710 - Epoch 16/100 | Train Loss: 0.2915, LR: 0.000996, mAP@0.5: 0.3099
2025-05-22 15:54:45,457 - Epoch 17/100 | Train Loss: 0.2850, LR: 0.000996, mAP@0.5: 0.3450
2025-05-22 15:55:27,270 - Epoch 18/100 | Train Loss: 0.2839, LR: 0.000996, mAP@0.5: 0.3559
2025-05-22 15:56:09,157 - Epoch 19/100 | Train Loss: 0.2690, LR: 0.000996, mAP@0.5: 0.3609
2025-05-22 15:56:50,832 - Epoch 20/100 | Train Loss: 0.2660, LR: 0.000996, mAP@0.5: 0.3967
2025-05-22 15:57:32,655 - Epoch 21/100 | Train Loss: 0.2724, LR: 0.000996, mAP@0.5: 0.3416
2025-05-22 15:58:14,631 - Epoch 22/100 | Train Loss: 0.2681, LR: 0.000996, mAP@0.5: 0.3515
2025-05-22 15:58:56,390 - Epoch 23/100 | Train Loss: 0.2650, LR: 0.000996, mAP@0.5: 0.3555
2025-05-22 15:59:38,069 - Epoch 24/100 | Train Loss: 0.2591, LR: 0.000996, mAP@0.5: 0.4228
2025-05-22 16:00:20,011 - Epoch 25/100 | Train Loss: 0.2568, LR: 0.000996, mAP@0.5: 0.4004
2025-05-22 16:01:01,982 - Epoch 26/100 | Train Loss: 0.2522, LR: 0.000996, mAP@0.5: 0.3643
2025-05-22 16:01:43,867 - Epoch 27/100 | Train Loss: 0.2579, LR: 0.000996, mAP@0.5: 0.4052
2025-05-22 16:02:25,771 - Epoch 28/100 | Train Loss: 0.2443, LR: 0.000996, mAP@0.5: 0.4594
2025-05-22 16:03:07,523 - Epoch 29/100 | Train Loss: 0.2417, LR: 0.000996, mAP@0.5: 0.4420
2025-05-22 16:03:49,284 - Epoch 30/100 | Train Loss: 0.2374, LR: 0.000996, mAP@0.5: 0.4500
2025-05-22 16:04:31,048 - Epoch 31/100 | Train Loss: 0.2361, LR: 0.000996, mAP@0.5: 0.5018
2025-05-22 16:05:12,808 - Epoch 32/100 | Train Loss: 0.2336, LR: 0.000996, mAP@0.5: 0.4862
2025-05-22 16:05:54,485 - Epoch 33/100 | Train Loss: 0.2288, LR: 0.000996, mAP@0.5: 0.5516
2025-05-22 16:06:36,180 - Epoch 34/100 | Train Loss: 0.2223, LR: 0.000996, mAP@0.5: 0.5418
2025-05-22 16:07:17,956 - Epoch 35/100 | Train Loss: 0.2197, LR: 0.000996, mAP@0.5: 0.5110
2025-05-22 16:07:59,631 - Epoch 36/100 | Train Loss: 0.2170, LR: 0.000996, mAP@0.5: 0.5074
2025-05-22 16:08:41,317 - Epoch 37/100 | Train Loss: 0.2229, LR: 0.000996, mAP@0.5: 0.5088
2025-05-22 16:09:22,882 - Epoch 38/100 | Train Loss: 0.2115, LR: 0.000996, mAP@0.5: 0.5417
2025-05-22 16:10:04,512 - Epoch 39/100 | Train Loss: 0.2047, LR: 0.000996, mAP@0.5: 0.5728
2025-05-22 16:10:46,233 - Epoch 40/100 | Train Loss: 0.2080, LR: 0.000996, mAP@0.5: 0.5419
2025-05-22 16:11:27,837 - Epoch 41/100 | Train Loss: 0.2020, LR: 0.000996, mAP@0.5: 0.6137
2025-05-22 16:12:09,538 - Epoch 42/100 | Train Loss: 0.2041, LR: 0.000996, mAP@0.5: 0.6188
2025-05-22 16:12:51,124 - Epoch 43/100 | Train Loss: 0.1907, LR: 0.000996, mAP@0.5: 0.6175
2025-05-22 16:13:32,744 - Epoch 44/100 | Train Loss: 0.1913, LR: 0.000996, mAP@0.5: 0.6018
2025-05-22 16:14:14,291 - Epoch 45/100 | Train Loss: 0.1895, LR: 0.000996, mAP@0.5: 0.5711
2025-05-22 16:14:55,939 - Epoch 46/100 | Train Loss: 0.1846, LR: 0.000996, mAP@0.5: 0.6270
2025-05-22 16:15:37,535 - Epoch 47/100 | Train Loss: 0.1820, LR: 0.000996, mAP@0.5: 0.6199
2025-05-22 16:16:19,226 - Epoch 48/100 | Train Loss: 0.1811, LR: 0.000996, mAP@0.5: 0.6148
2025-05-22 16:17:00,961 - Epoch 49/100 | Train Loss: 0.1880, LR: 0.000996, mAP@0.5: 0.6098
2025-05-22 16:17:42,539 - Epoch 50/100 | Train Loss: 0.1790, LR: 0.000996, mAP@0.5: 0.6164
2025-05-22 16:18:24,204 - Epoch 51/100 | Train Loss: 0.1730, LR: 0.000996, mAP@0.5: 0.5718
2025-05-22 16:19:05,871 - Epoch 52/100 | Train Loss: 0.1698, LR: 0.000996, mAP@0.5: 0.6234
2025-05-22 16:19:47,387 - Epoch 53/100 | Train Loss: 0.1691, LR: 0.000996, mAP@0.5: 0.6567
2025-05-22 16:20:28,933 - Epoch 54/100 | Train Loss: 0.1711, LR: 0.000996, mAP@0.5: 0.6356
2025-05-22 16:21:10,431 - Epoch 55/100 | Train Loss: 0.1650, LR: 0.000996, mAP@0.5: 0.6331
2025-05-22 16:21:51,999 - Epoch 56/100 | Train Loss: 0.1601, LR: 0.000996, mAP@0.5: 0.6221
2025-05-22 16:22:33,472 - Epoch 57/100 | Train Loss: 0.1647, LR: 0.000996, mAP@0.5: 0.6706
2025-05-22 16:23:14,973 - Epoch 58/100 | Train Loss: 0.1629, LR: 0.000996, mAP@0.5: 0.6384
2025-05-22 16:23:56,667 - Epoch 59/100 | Train Loss: 0.1635, LR: 0.000996, mAP@0.5: 0.6457
2025-05-22 16:24:38,226 - Epoch 60/100 | Train Loss: 0.1561, LR: 0.000996, mAP@0.5: 0.6283
2025-05-22 16:25:19,830 - Epoch 61/100 | Train Loss: 0.1552, LR: 0.000996, mAP@0.5: 0.6124
2025-05-22 16:26:01,416 - Epoch 62/100 | Train Loss: 0.1478, LR: 0.000996, mAP@0.5: 0.6690
2025-05-22 16:26:43,045 - Epoch 63/100 | Train Loss: 0.1567, LR: 0.000996, mAP@0.5: 0.6430
2025-05-22 16:27:24,688 - Epoch 64/100 | Train Loss: 0.1501, LR: 0.000996, mAP@0.5: 0.6397
2025-05-22 16:28:06,247 - Epoch 65/100 | Train Loss: 0.1502, LR: 0.000996, mAP@0.5: 0.5904
2025-05-22 16:28:47,841 - Epoch 66/100 | Train Loss: 0.1510, LR: 0.000996, mAP@0.5: 0.6453
2025-05-22 16:29:29,500 - Epoch 67/100 | Train Loss: 0.1469, LR: 0.000996, mAP@0.5: 0.6475
2025-05-22 16:29:29,501 - Early stopping at epoch 67
2025-05-22 16:29:29,586 - Model saved at 'models/faster_rcnn_5_byol/faster_rcnn_5_byol_20250522_154409.pth'
2025-05-22 16:29:50,317 - {'map': tensor(0.6452), 'map_50': tensor(0.6452), 'map_75': tensor(-1.), 'map_small': tensor(0.5904), 'map_medium': tensor(0.7169), 'map_large': tensor(-1.), 'mar_1': tensor(0.8208), 'mar_10': tensor(0.8865), 'mar_100': tensor(0.8865), 'mar_small': tensor(0.8720), 'mar_medium': tensor(0.8591), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int32)}
2025-05-22 16:29:50,322 - Total Duration: 0:45:40.715789
