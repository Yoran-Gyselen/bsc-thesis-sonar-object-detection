2025-05-22 14:31:04,893 - Dataset fraction: 0.1
2025-05-22 14:31:04,893 - Seed: 42
2025-05-22 14:31:04,893 - Batch size: 16
2025-05-22 14:31:04,893 - Epochs: 100
2025-05-22 14:31:05,226 - Initially frozen backbone layers: ['layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight']
2025-05-22 14:31:31,287 - Epoch 1 | Train Loss: 0.6302, LR: 0.000132, mAP@0.5: (skipped — frozen)
2025-05-22 14:31:55,811 - Epoch 2 | Train Loss: 0.2737, LR: 0.000374, mAP@0.5: (skipped — frozen)
2025-05-22 14:32:20,596 - Epoch 3 | Train Loss: 0.2195, LR: 0.000672, mAP@0.5: (skipped — frozen)
2025-05-22 14:32:20,596 - Unfroze layers at epoch 3: ['layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight']
2025-05-22 14:33:08,807 - Epoch 4 | Train Loss: 0.2096, LR: 0.000065, mAP@0.5: 0.0227
2025-05-22 14:33:56,045 - Epoch 5 | Train Loss: 0.2099, LR: 0.000138, mAP@0.5: 0.0310
2025-05-22 14:34:43,379 - Epoch 6 | Train Loss: 0.2228, LR: 0.000250, mAP@0.5: 0.0382
2025-05-22 14:35:31,859 - Epoch 7 | Train Loss: 0.2588, LR: 0.000391, mAP@0.5: 0.0705
2025-05-22 14:36:20,096 - Epoch 8 | Train Loss: 0.2413, LR: 0.000545, mAP@0.5: 0.0814
2025-05-22 14:37:08,810 - Epoch 9 | Train Loss: 0.3002, LR: 0.000697, mAP@0.5: 0.0962
2025-05-22 14:37:58,038 - Epoch 10 | Train Loss: 0.3300, LR: 0.000830, mAP@0.5: 0.1153
2025-05-22 14:38:46,998 - Epoch 11 | Train Loss: 0.3500, LR: 0.000930, mAP@0.5: 0.1769
2025-05-22 14:39:35,399 - Epoch 12 | Train Loss: 0.3268, LR: 0.000988, mAP@0.5: 0.1426
2025-05-22 14:40:24,512 - Epoch 13 | Train Loss: 0.3376, LR: 0.001000, mAP@0.5: 0.2348
2025-05-22 14:41:13,805 - Epoch 14 | Train Loss: 0.3328, LR: 0.000999, mAP@0.5: 0.2437
2025-05-22 14:42:03,273 - Epoch 15 | Train Loss: 0.3417, LR: 0.000998, mAP@0.5: 0.2372
2025-05-22 14:42:52,778 - Epoch 16 | Train Loss: 0.3228, LR: 0.000996, mAP@0.5: 0.3132
2025-05-22 14:43:42,216 - Epoch 17 | Train Loss: 0.3121, LR: 0.000994, mAP@0.5: 0.2792
2025-05-22 14:44:31,836 - Epoch 18 | Train Loss: 0.3115, LR: 0.000991, mAP@0.5: 0.3926
2025-05-22 14:45:21,513 - Epoch 19 | Train Loss: 0.3152, LR: 0.000987, mAP@0.5: 0.3936
2025-05-22 14:46:11,294 - Epoch 20 | Train Loss: 0.3024, LR: 0.000983, mAP@0.5: 0.4030
2025-05-22 14:47:04,262 - Epoch 21 | Train Loss: 0.2998, LR: 0.000978, mAP@0.5: 0.3716
2025-05-22 14:47:54,092 - Epoch 22 | Train Loss: 0.2929, LR: 0.000972, mAP@0.5: 0.4181
2025-05-22 14:48:43,796 - Epoch 23 | Train Loss: 0.2918, LR: 0.000966, mAP@0.5: 0.4730
2025-05-22 14:49:33,864 - Epoch 24 | Train Loss: 0.2883, LR: 0.000959, mAP@0.5: 0.4242
2025-05-22 14:50:23,652 - Epoch 25 | Train Loss: 0.2863, LR: 0.000952, mAP@0.5: 0.4427
2025-05-22 14:51:13,576 - Epoch 26 | Train Loss: 0.2854, LR: 0.000944, mAP@0.5: 0.4337
2025-05-22 14:52:03,354 - Epoch 27 | Train Loss: 0.2794, LR: 0.000935, mAP@0.5: 0.4715
2025-05-22 14:52:53,287 - Epoch 28 | Train Loss: 0.2739, LR: 0.000926, mAP@0.5: 0.4692
2025-05-22 14:53:43,216 - Epoch 29 | Train Loss: 0.2737, LR: 0.000917, mAP@0.5: 0.4793
2025-05-22 14:54:33,034 - Epoch 30 | Train Loss: 0.2696, LR: 0.000906, mAP@0.5: 0.4886
2025-05-22 14:55:22,812 - Epoch 31 | Train Loss: 0.2707, LR: 0.000896, mAP@0.5: 0.4959
2025-05-22 14:56:12,477 - Epoch 32 | Train Loss: 0.2630, LR: 0.000884, mAP@0.5: 0.5084
2025-05-22 14:57:02,296 - Epoch 33 | Train Loss: 0.2686, LR: 0.000873, mAP@0.5: 0.4984
2025-05-22 14:57:51,960 - Epoch 34 | Train Loss: 0.2625, LR: 0.000860, mAP@0.5: 0.5307
2025-05-22 14:58:41,650 - Epoch 35 | Train Loss: 0.2564, LR: 0.000848, mAP@0.5: 0.5509
2025-05-22 14:59:31,487 - Epoch 36 | Train Loss: 0.2576, LR: 0.000835, mAP@0.5: 0.5227
2025-05-22 15:00:21,291 - Epoch 37 | Train Loss: 0.2540, LR: 0.000821, mAP@0.5: 0.5666
2025-05-22 15:01:11,155 - Epoch 38 | Train Loss: 0.2507, LR: 0.000807, mAP@0.5: 0.5042
2025-05-22 15:02:00,823 - Epoch 39 | Train Loss: 0.2498, LR: 0.000793, mAP@0.5: 0.5799
2025-05-22 15:02:50,567 - Epoch 40 | Train Loss: 0.2684, LR: 0.000778, mAP@0.5: 0.5013
2025-05-22 15:03:40,318 - Epoch 41 | Train Loss: 0.2445, LR: 0.000763, mAP@0.5: 0.5417
2025-05-22 15:04:30,092 - Epoch 42 | Train Loss: 0.2423, LR: 0.000748, mAP@0.5: 0.5589
2025-05-22 15:05:19,828 - Epoch 43 | Train Loss: 0.2427, LR: 0.000732, mAP@0.5: 0.5541
2025-05-22 15:06:09,559 - Epoch 44 | Train Loss: 0.2371, LR: 0.000716, mAP@0.5: 0.6036
2025-05-22 15:06:59,400 - Epoch 45 | Train Loss: 0.2338, LR: 0.000699, mAP@0.5: 0.5788
2025-05-22 15:07:49,147 - Epoch 46 | Train Loss: 0.2377, LR: 0.000683, mAP@0.5: 0.6038
2025-05-22 15:08:38,948 - Epoch 47 | Train Loss: 0.2271, LR: 0.000666, mAP@0.5: 0.6053
2025-05-22 15:09:28,741 - Epoch 48 | Train Loss: 0.2288, LR: 0.000649, mAP@0.5: 0.5841
2025-05-22 15:10:18,438 - Epoch 49 | Train Loss: 0.2243, LR: 0.000632, mAP@0.5: 0.5942
2025-05-22 15:11:08,171 - Epoch 50 | Train Loss: 0.2208, LR: 0.000614, mAP@0.5: 0.6282
2025-05-22 15:11:57,941 - Epoch 51 | Train Loss: 0.2211, LR: 0.000597, mAP@0.5: 0.5930
2025-05-22 15:12:47,653 - Epoch 52 | Train Loss: 0.2170, LR: 0.000579, mAP@0.5: 0.6159
2025-05-22 15:13:37,439 - Epoch 53 | Train Loss: 0.2161, LR: 0.000562, mAP@0.5: 0.5799
2025-05-22 15:14:27,143 - Epoch 54 | Train Loss: 0.2124, LR: 0.000544, mAP@0.5: 0.5938
2025-05-22 15:15:16,933 - Epoch 55 | Train Loss: 0.2123, LR: 0.000526, mAP@0.5: 0.5923
2025-05-22 15:16:06,623 - Epoch 56 | Train Loss: 0.2109, LR: 0.000508, mAP@0.5: 0.6102
2025-05-22 15:16:56,323 - Epoch 57 | Train Loss: 0.2057, LR: 0.000490, mAP@0.5: 0.6312
2025-05-22 15:17:45,977 - Epoch 58 | Train Loss: 0.2030, LR: 0.000472, mAP@0.5: 0.6321
2025-05-22 15:18:35,717 - Epoch 59 | Train Loss: 0.2024, LR: 0.000454, mAP@0.5: 0.6437
2025-05-22 15:19:25,404 - Epoch 60 | Train Loss: 0.2008, LR: 0.000436, mAP@0.5: 0.6120
2025-05-22 15:20:15,017 - Epoch 61 | Train Loss: 0.2002, LR: 0.000419, mAP@0.5: 0.6326
2025-05-22 15:21:04,638 - Epoch 62 | Train Loss: 0.1965, LR: 0.000401, mAP@0.5: 0.6484
2025-05-22 15:21:54,327 - Epoch 63 | Train Loss: 0.1968, LR: 0.000384, mAP@0.5: 0.6086
2025-05-22 15:22:44,014 - Epoch 64 | Train Loss: 0.1941, LR: 0.000366, mAP@0.5: 0.6273
2025-05-22 15:23:33,586 - Epoch 65 | Train Loss: 0.1917, LR: 0.000349, mAP@0.5: 0.6547
2025-05-22 15:24:23,148 - Epoch 66 | Train Loss: 0.1888, LR: 0.000332, mAP@0.5: 0.6506
2025-05-22 15:25:12,715 - Epoch 67 | Train Loss: 0.1856, LR: 0.000315, mAP@0.5: 0.6407
2025-05-22 15:26:02,375 - Epoch 68 | Train Loss: 0.1879, LR: 0.000299, mAP@0.5: 0.6641
2025-05-22 15:26:51,869 - Epoch 69 | Train Loss: 0.1859, LR: 0.000283, mAP@0.5: 0.6504
2025-05-22 15:27:41,428 - Epoch 70 | Train Loss: 0.1806, LR: 0.000267, mAP@0.5: 0.6472
2025-05-22 15:28:30,937 - Epoch 71 | Train Loss: 0.1816, LR: 0.000251, mAP@0.5: 0.6611
2025-05-22 15:29:20,525 - Epoch 72 | Train Loss: 0.1786, LR: 0.000236, mAP@0.5: 0.6500
2025-05-22 15:30:10,142 - Epoch 73 | Train Loss: 0.1797, LR: 0.000221, mAP@0.5: 0.6533
2025-05-22 15:30:59,665 - Epoch 74 | Train Loss: 0.1814, LR: 0.000206, mAP@0.5: 0.6452
2025-05-22 15:31:49,168 - Epoch 75 | Train Loss: 0.1759, LR: 0.000192, mAP@0.5: 0.6440
2025-05-22 15:32:38,765 - Epoch 76 | Train Loss: 0.1725, LR: 0.000178, mAP@0.5: 0.6587
2025-05-22 15:33:28,343 - Epoch 77 | Train Loss: 0.1721, LR: 0.000165, mAP@0.5: 0.6566
2025-05-22 15:34:17,814 - Epoch 78 | Train Loss: 0.1694, LR: 0.000152, mAP@0.5: 0.6659
2025-05-22 15:35:07,285 - Epoch 79 | Train Loss: 0.1695, LR: 0.000139, mAP@0.5: 0.6492
2025-05-22 15:35:56,779 - Epoch 80 | Train Loss: 0.1686, LR: 0.000127, mAP@0.5: 0.6411
2025-05-22 15:36:46,314 - Epoch 81 | Train Loss: 0.1681, LR: 0.000116, mAP@0.5: 0.6437
2025-05-22 15:37:35,739 - Epoch 82 | Train Loss: 0.1648, LR: 0.000105, mAP@0.5: 0.6600
2025-05-22 15:38:25,174 - Epoch 83 | Train Loss: 0.1630, LR: 0.000094, mAP@0.5: 0.6588
2025-05-22 15:39:14,625 - Epoch 84 | Train Loss: 0.1640, LR: 0.000084, mAP@0.5: 0.6582
2025-05-22 15:40:04,150 - Epoch 85 | Train Loss: 0.1614, LR: 0.000075, mAP@0.5: 0.6632
2025-05-22 15:40:53,613 - Epoch 86 | Train Loss: 0.1613, LR: 0.000066, mAP@0.5: 0.6580
2025-05-22 15:41:43,076 - Epoch 87 | Train Loss: 0.1577, LR: 0.000057, mAP@0.5: 0.6545
2025-05-22 15:42:32,575 - Epoch 88 | Train Loss: 0.1584, LR: 0.000050, mAP@0.5: 0.6612
2025-05-22 15:42:32,575 - Early stopping at epoch 88
2025-05-22 15:42:32,663 - Model saved at 'models/faster_rcnn_10_byol/faster_rcnn_10_byol_20250522_143104.pth'
2025-05-22 15:42:54,906 - {'map': tensor(0.7096), 'map_50': tensor(0.7096), 'map_75': tensor(-1.), 'map_small': tensor(0.7009), 'map_medium': tensor(0.7048), 'map_large': tensor(-1.), 'mar_1': tensor(0.8526), 'mar_10': tensor(0.9020), 'mar_100': tensor(0.9020), 'mar_small': tensor(0.8913), 'mar_medium': tensor(0.9240), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int32)}
2025-05-22 15:42:54,919 - Total Duration: 1:11:50.025657
