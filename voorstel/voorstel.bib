% Encoding: UTF-8

@InProceedings{He_2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2016-06},
  title     = {Deep Residual Learning for Image Recognition},
  doi       = {10.1109/cvpr.2016.90},
  publisher = {IEEE},
  month     = jun,
  type      = {techreport},
  year      = {2016},
}

@InProceedings{Tompson_2015,
  author    = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  date      = {2015-06},
  title     = {Efficient Object Localization Using Convolutional Networks},
  doi       = {10.1109/cvpr.2015.7298664},
  publisher = {IEEE},
  file      = {:Tompson_2015 - Efficient Object Localization Using Convolutional Networks.pdf:PDF:http\://arxiv.org/pdf/1411.4280},
  year      = {2015},
}

@Article{Karimanzira_2020,
  author       = {Karimanzira, Divas and Renkewitz, Helge and Shea, David and Albiez, Jan},
  date         = {2020-07},
  journaltitle = {Electronics},
  title        = {Object Detection in Sonar Images},
  doi          = {10.3390/electronics9071180},
  issn         = {2079-9292},
  number       = {7},
  pages        = {1180},
  volume       = {9},
  file         = {:Karimanzira_2020 - Object Detection in Sonar Images.pdf:PDF:https\://www.mdpi.com/2079-9292/9/7/1180/pdf?version=1595401000},
  journal      = {Electronics},
  month        = jul,
  publisher    = {MDPI AG},
  year         = {2020},
}

@Article{Ritter_1997,
  author       = {Ritter, N. and Ruth, M.},
  date         = {1997-05},
  journaltitle = {International Journal of Remote Sensing},
  title        = {The GeoTiff data interchange standard for raster geographic images},
  doi          = {10.1080/014311697218340},
  issn         = {1366-5901},
  number       = {7},
  pages        = {1637--1647},
  volume       = {18},
  journal      = {International Journal of Remote Sensing},
  month        = may,
  publisher    = {Informa UK Limited},
  year         = {1997},
}

@Misc{Grill_2020,
  author    = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  date      = {2020},
  title     = {Bootstrap your own latent: A new approach to self-supervised Learning},
  doi       = {10.48550/ARXIV.2006.07733},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file      = {:Grill_2020 - Bootstrap Your Own Latent_ a New Approach to Self Supervised Learning.pdf:PDF:http\://arxiv.org/pdf/2006.07733},
  keywords  = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  year      = {2020},
}

@Article{Long_2015,
  author      = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  date        = {2015-06},
  title       = {Fully Convolutional Networks for Semantic Segmentation},
  doi         = {10.1109/cvpr.2015.7298965},
  eprint      = {1411.4038v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT\_Flow, while inference takes one third of a second for a typical image.},
  booktitle   = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  file        = {:Long_2015 - Fully Convolutional Networks for Semantic Segmentation.pdf:PDF:http\://arxiv.org/pdf/1411.4038},
  keywords    = {cs.CV},
  publisher   = {IEEE},
}

@Article{Redmon_2016,
  author      = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date        = {2016-06},
  title       = {You Only Look Once: Unified, Real-Time Object Detection},
  doi         = {10.1109/cvpr.2016.91},
  eprint      = {1506.02640v5},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  pages       = {779--788},
  abstract    = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  booktitle   = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  file        = {:Redmon_2016 - You Only Look Once_ Unified, Real Time Object Detection.pdf:PDF:https\://arxiv.org/pdf/1506.02640},
  keywords    = {cs.CV},
  publisher   = {IEEE},
}

@Article{C_A_Padmanabha_Reddy_2018,
  author       = {C A Padmanabha Reddy, Y and Viswanath, P and Eswara Reddy, B},
  date         = {2018-02},
  journaltitle = {International Journal of Engineering &amp; Technology},
  title        = {Semi-supervised learning: a brief review},
  doi          = {10.14419/ijet.v7i1.8.9977},
  issn         = {2227-524X},
  number       = {1.8},
  pages        = {81},
  volume       = {7},
  journal      = {International Journal of Engineering \& Technology},
  month        = feb,
  publisher    = {Science Publishing Corporation},
  year         = {2018},
}

@Article{Lee_2013,
  author  = {Lee, Dong-Hyun},
  title   = {Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
  journal = {ICML 2013 Workshop : Challenges in Representation Learning (WREPL)},
  month   = {07},
  year    = {2013},
}

@Article{Sohn_2020,
  author      = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
  date        = {2020-01-21},
  title       = {FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
  doi         = {10.48550/arXiv.2001.07685},
  eprint      = {2001.07685v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Sohn_2020 - FixMatch_ Simplifying Semi Supervised Learning with Consistency and Confidence.pdf:PDF:http\://arxiv.org/pdf/2001.07685},
  keywords    = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
}

@Article{Fan_2022,
  author       = {Fan, Yue and Kukleva, Anna and Dai, Dengxin and Schiele, Bernt},
  date         = {2022-12},
  journaltitle = {International Journal of Computer Vision},
  title        = {Revisiting Consistency Regularization for Semi-Supervised Learning},
  doi          = {10.1007/s11263-022-01723-4},
  eprint       = {2112.05825v1},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  issn         = {1573-1405},
  number       = {3},
  pages        = {626--643},
  volume       = {131},
  abstract     = {Consistency regularization is one of the most widely-used techniques for semi-supervised learning (SSL). Generally, the aim is to train a model that is invariant to various data augmentations. In this paper, we revisit this idea and find that enforcing invariance by decreasing distances between features from differently augmented images leads to improved performance. However, encouraging equivariance instead, by increasing the feature distance, further improves performance. To this end, we propose an improved consistency regularization framework by a simple yet effective technique, FeatDistLoss, that imposes consistency and equivariance on the classifier and the feature level, respectively. Experimental results show that our model defines a new state of the art for various datasets and settings and outperforms previous work by a significant margin, particularly in low data regimes. Extensive experiments are conducted to analyze the method, and the code will be published.},
  file         = {:Fan_2022 - Revisiting Consistency Regularization for Semi Supervised Learning.pdf:PDF:https\://link.springer.com/content/pdf/10.1007/s11263-022-01723-4.pdf},
  journal      = {International Journal of Computer Vision},
  keywords     = {cs.CV},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Gui_2024,
  author       = {Gui, Jie and Chen, Tuo and Zhang, Jing and Cao, Qiong and Sun, Zhenan and Luo, Hao and Tao, Dacheng},
  date         = {2024-12},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {A Survey on Self-Supervised Learning: Algorithms, Applications, and Future Trends},
  doi          = {10.1109/tpami.2024.3415112},
  eprint       = {2301.05712v4},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  issn         = {1939-3539},
  number       = {12},
  pages        = {9052--9071},
  volume       = {46},
  abstract     = {Deep supervised learning algorithms typically require a large volume of labeled data to achieve satisfactory performance. However, the process of collecting and labeling such data can be expensive and time-consuming. Self-supervised learning (SSL), a subset of unsupervised learning, aims to learn discriminative features from unlabeled data without relying on human-annotated labels. SSL has garnered significant attention recently, leading to the development of numerous related algorithms. However, there is a dearth of comprehensive studies that elucidate the connections and evolution of different SSL variants. This paper presents a review of diverse SSL methods, encompassing algorithmic aspects, application domains, three key trends, and open research questions. Firstly, we provide a detailed introduction to the motivations behind most SSL algorithms and compare their commonalities and differences. Secondly, we explore representative applications of SSL in domains such as image processing, computer vision, and natural language processing. Lastly, we discuss the three primary trends observed in SSL research and highlight the open questions that remain. A curated collection of valuable resources can be accessed at https://github.com/guijiejie/SSL.},
  file         = {:Gui_2024 - A Survey on Self Supervised Learning_ Algorithms, Applications, and Future Trends.pdf:PDF:http\://arxiv.org/pdf/2301.05712},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords     = {cs.LG},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Chen_2020,
  author      = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date        = {2020-02-13},
  title       = {A Simple Framework for Contrastive Learning of Visual Representations},
  doi         = {10.48550/ARXIV.2002.05709},
  eprint      = {2002.05709v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  copyright   = {arXiv.org perpetual, non-exclusive license},
  file        = {:Chen_2020 - A Simple Framework for Contrastive Learning of Visual Representations.pdf:PDF:http\://arxiv.org/pdf/2002.05709},
  keywords    = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher   = {arXiv},
}

@Comment{jabref-meta: databaseType:biblatex;}
